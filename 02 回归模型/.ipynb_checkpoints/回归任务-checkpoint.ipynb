{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入相关科学计算包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use(\"ggplot\")      \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 使用sklearn构建完整的机器学习项目流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，一个完整的机器学习项目分为以下步骤：\n",
    "   - 明确项目任务：回归/分类\n",
    "   - 收集数据集并选择合适的特征。\n",
    "   - 选择度量模型性能的指标。\n",
    "   - 选择具体的模型并进行训练以优化模型。\n",
    "   - 评估模型的性能并调参。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 使用sklearn构建完整的回归项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 收集数据集并选择合适的特征：        \n",
    "在数据集上我们使用我们比较熟悉的Boston房价数据集，原因是：        \n",
    "   - 第一个，我们通过这些简单的数据集快速让我们上手sklearn，以及掌握sklearn的相关操作。\n",
    "   - 第二个，我们用简单的数据集能更加清晰地介绍机器学习的相关模型，避免在处理数据上花费较大的精力。\n",
    "\n",
    "如果您对具体的项目感兴趣，我们会在第六章给出三个大型的案例让大家体验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  Price  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()     # 返回一个类似于字典的类\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "boston_data = pd.DataFrame(X,columns=features)\n",
    "boston_data[\"Price\"] = y\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个特征的相关解释：\n",
    "   - CRIM：各城镇的人均犯罪率\n",
    "   - ZN：规划地段超过25,000平方英尺的住宅用地比例\n",
    "   - INDUS：城镇非零售商业用地比例\n",
    "   - CHAS：是否在查尔斯河边(=1是)\n",
    "   - NOX：一氧化氮浓度(/千万分之一)\n",
    "   - RM：每个住宅的平均房间数\n",
    "   - AGE：1940年以前建造的自住房屋的比例\n",
    "   - DIS：到波士顿五个就业中心的加权距离\n",
    "   - RAD：放射状公路的可达性指数\n",
    "   - TAX：全部价值的房产税率(每1万美元)\n",
    "   - PTRATIO：按城镇分配的学生与教师比例\n",
    "   - B：1000(Bk - 0.63)^2其中Bk是每个城镇的黑人比例\n",
    "   - LSTAT：较低地位人口\n",
    "   - Price：房价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 选择度量模型性能的指标：                      \n",
    "   - MSE均方误差：$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2.$\n",
    "   - MAE平均绝对误差:$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|$\n",
    "   - $R^2$决定系数：$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "   - 解释方差得分:$explained\\_{}variance(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}$\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "![jupyter](./1.3.png)              \n",
    "在这个案例中，我们使用MSE均方误差为模型的性能度量指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 选择具体的模型并进行训练\n",
    "   - **线性回归模型**         \n",
    "   回归这个概念是19世纪80年代由英国统计学家郎西斯.高尔顿在研究父子身高关系提出来的，他发现：在同一族群中，子代的平均身高介于父代的身高以及族群的平均身高之间。具体而言，高个子父亲的儿子的身高有低于其父亲身高的趋势，而矮个子父亲的儿子身高则有高于父亲的身高的趋势。也就是说，子代的身高有向族群平均身高\"平均\"的趋势，这就是统计学上\"回归\"的最初含义。回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（特征）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。通常使用曲线/线来拟合数据点，目标是使曲线到数据点的距离差异最小。而线性回归就是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数w ：                                                \n",
    "   假设：数据集$D = \\{(x_1,y_1),...,(x_N,y_N) \\}$，$x_i \\in R^p,y_i \\in R,i = 1,2,...,N$，$X = (x_1,x_2,...,x_N)^T,Y=(y_1,y_2,...,y_N)^T$                        \n",
    "   假设X和Y之间存在线性关系，模型的具体形式为$\\hat{y}=f(w) =w^Tx$           \n",
    "   ![jupyter](./1.4.png)      \n",
    "   (a) 最小二乘估计：                 \n",
    "   我们需要衡量真实值$y_i$与线性回归模型的预测值$w^Tx_i$之间的差距，在这里我们和使用二范数的平方和L(w)来描述这种差距，即：                      \n",
    "   $$\n",
    "   L(w) = \\sum\\limits_{i=1}^{N}||w^Tx_i-y_i||_2^2=\\sum\\limits_{i=1}^{N}(w^Tx_i-y_i)^2 = (w^TX^T-Y^T)(w^TX^T-Y^T)^T = w^TX^TXw - 2w^TX^TY+YY^T\\\\\n",
    "   因此，我们需要找到使得L(w)最小时对应的参数w，即：\\\\\n",
    "   \\hat{w} = argmin\\;L(w)\\\\\n",
    "   为了达到求解最小化L(w)问题，我们应用高等数学的知识，使用求导来解决这个问题： \\\\\n",
    "   \\frac{\\partial L(w)}{\\partial w} = 2X^TXw-2X^TY = 0,因此： \\\\\n",
    "   \\hat{w} = (X^TX)^{-1}X^TY\n",
    "   $$                   \n",
    "   (b) 几何解释：                \n",
    "   在线性代数中，我们知道两个向量a和b相互垂直可以得出：$<a,b> = a.b = a^Tb = 0$,而平面X的法向量为Y-Xw，与平面X互相垂直，因此：$X^T(Y-Xw) = 0$，即：$w = (X^TX)^{-1}X^TY$                             \n",
    "   ![jupyter](./1.5.png)               \n",
    "   (c) 概率视角：       \n",
    "   假设噪声$\\epsilon \\backsim N(0,\\sigma^2),y=f(w)+\\epsilon=w^Tx+\\epsilon$，因此：$y|x_i,w ~ N(w^Tx,\\sigma^2)$          \n",
    "   我们使用极大似然估计MLE对参数w进行估计：       \n",
    "   $$\n",
    "   L(w) = log\\;P(Y|X;w) = log\\;\\prod_{i=1}^N P(y_i|x_i;w) = \\sum\\limits_{i=1}^{N} log\\; P(y_i|x_i;w)\\\\\n",
    "    = \\sum\\limits_{i=1}^{N}log(\\frac{1}{\\sqrt{2\\pi \\sigma}}exp(-\\frac{(y_i-w^Tx_i)^2}{2\\sigma^2})) = \\sum\\limits_{i=1}^{N}[log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\frac{1}{2\\sigma^2}(y_i-w^Tx_i)^2] \\\\\n",
    "    argmax_w L(w) = argmin_w[l(w) = \\sum\\limits_{i = 1}^{N}(y_i-w^Tx_i)^2]\\\\\n",
    "    因此：线性回归的最小二乘估计<==>噪声\\epsilon\\backsim N(0,\\sigma^2)的极大似然估计\n",
    "   $$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们使用sklearn的线性回归实例来演示：                   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型系数： [-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n",
      "模型得分： 0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model      # 引入线性回归方法\n",
    "lin_reg = linear_model.LinearRegression()       # 创建线性回归的类\n",
    "lin_reg.fit(X,y)        # 输入特征X和因变量y进行训练\n",
    "print(\"模型系数：\",lin_reg.coef_)             # 输出模型的系数\n",
    "print(\"模型得分：\",lin_reg.score(X,y))    # 输出模型的决定系数R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **线性回归的推广**                     \n",
    "   在线性回归中，我们假设因变量与特征之间的关系是线性关系，这样的假设使得模型很简单，但是缺点也是显然的，那就是当数据存在非线性关系时，我们使用线性回归模型进行预测会导致预测性能极其低下，因为模型的形式本身是线性的，无法表达数据中的非线性关系。我们一个很自然的想法就是去推广线性回归模型，使得推广后的模型更能表达非线性的关系。                     \n",
    "   (a) 多项式回归：        \n",
    "   为了体现因变量和特征的非线性关系，一个很自然而然的想法就是将标准的线性回归模型：             \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_i + \\epsilon_i\n",
    "   $$               \n",
    "   换成一个多项式函数：          \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_i + w_2x_i^2 + ...+w_dx_i^d + \\epsilon\n",
    "   $$                        \n",
    "   对于多项式的阶数d不能取过大，一般不大于3或者4，因为d越大，多项式曲线就会越光滑，在X的边界处有异常的波动。（图中的边界处的4阶多项式拟合曲线的置信区间(虚线表示置信区间)明显增大，预测效果的稳定性下降。）   \n",
    "\n",
    "   ![jupyter](./1.6.1.png) ![jupyter](./1.6.2.png)                             \n",
    "   \n",
    "   (b) 广义可加模型(GAM)：         \n",
    "   广义可加模型GAM实际上是线性模型推广至非线性模型的一个框架，在这个框架中，每一个变量都用一个非线性函数来代替，但是模型本身保持整体可加性。GAM模型不仅仅可以用在线性回归的推广，还可以将线性分类模型进行推广。具体的推广形式是：          \n",
    "   标准的线性回归模型：           \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_{i1} +...+w_px_{ip} + \\epsilon_i \n",
    "   $$                 \n",
    "   GAM模型框架：                  \n",
    "   $$\n",
    "   y_i = w_0 + \\sum\\limits_{j=1}^{p}f_{j}(x_{ij}) + \\epsilon_i\n",
    "   $$                  \n",
    "   GAM模型的优点与不足：             \n",
    "      - 优点：简单容易操作，能够很自然地推广线性回归模型至非线性模型，使得模型的预测精度有所上升；由于模型本身是可加的，因此GAM还是能像线性回归模型一样把其他因素控制不变的情况下单独对某个变量进行推断，极大地保留了线性回归的易于推断的性质。\n",
    "      - 缺点：GAM模型会经常忽略一些有意义的交互作用，比如某两个特征共同影响因变量，不过GAM还是能像线性回归一样加入交互项$x^{(i)} \\times x^{(j)}$的形式进行建模；但是GAM模型本质上还是一个可加模型，如果我们能摆脱可加性模型形式，可能还会提升模型预测精度，详情请看后面的算法。\n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 多项式回归实例介绍：                    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=poly#sklearn.preprocessing.PolynomialFeatures                              \n",
    "sklearn.preprocessing.PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C'):               \n",
    "   - 参数：         \n",
    "   degree：特征转换的阶数。                       \n",
    "   interaction_onlyboolean：是否只包含交互项，默认False 。              \n",
    "   include_bias：是否包含截距项，默认True。           \n",
    "   order：str in {‘C’, ‘F’}, default ‘C’，输出数组的顺序。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始X为：\n",
      " [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "2次转化X：\n",
      " [[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n",
      "2次转化X：\n",
      " [[ 1.  0.  1.  0.]\n",
      " [ 1.  2.  3.  6.]\n",
      " [ 1.  4.  5. 20.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X_arr = np.arange(6).reshape(3, 2)\n",
    "print(\"原始X为：\\n\",X_arr)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "print(\"2次转化X：\\n\",poly.fit_transform(X_arr))\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "print(\"2次转化X：\\n\",poly.fit_transform(X_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) GAM模型实例介绍：          \n",
    "安装pygam：pip install pygam               \n",
    "https://github.com/dswah/pyGAM/blob/master/doc/source/notebooks/quick_start.ipynb                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearGAM                                                                                                 \n",
      "=============================================== ==========================================================\n",
      "Distribution:                        NormalDist Effective DoF:                                    103.2423\n",
      "Link Function:                     IdentityLink Log Likelihood:                                 -1589.7653\n",
      "Number of Samples:                          506 AIC:                                             3388.0152\n",
      "                                                AICc:                                            3442.7649\n",
      "                                                GCV:                                               13.7683\n",
      "                                                Scale:                                              8.8269\n",
      "                                                Pseudo R-Squared:                                   0.9168\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "s(0)                              [0.6]                20           11.1         2.20e-11     ***         \n",
      "s(1)                              [0.6]                20           12.8         8.15e-02     .           \n",
      "s(2)                              [0.6]                20           13.4         2.59e-03     **          \n",
      "s(3)                              [0.6]                20           3.6          2.76e-01                 \n",
      "s(4)                              [0.6]                20           11.3         1.11e-16     ***         \n",
      "s(5)                              [0.6]                20           10.2         1.11e-16     ***         \n",
      "s(6)                              [0.6]                20           10.4         8.22e-01                 \n",
      "s(7)                              [0.6]                20           8.5          4.44e-16     ***         \n",
      "s(8)                              [0.6]                20           3.5          5.96e-03     **          \n",
      "s(9)                              [0.6]                20           3.5          1.33e-09     ***         \n",
      "s(10)                             [0.6]                20           1.8          3.26e-03     **          \n",
      "s(11)                             [0.6]                20           6.4          6.25e-02     .           \n",
      "s(12)                             [0.6]                20           6.6          1.11e-16     ***         \n",
      "intercept                                              1            0.0          2.23e-13     ***         \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n",
      " \n",
      "Please do not make inferences based on these values! \n",
      "\n",
      "Collaborate on a solution, and stay up to date at: \n",
      "github.com/dswah/pyGAM/issues/163 \n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from pygam import LinearGAM\n",
    "gam = LinearGAM().fit(boston_data[boston.feature_names], y)\n",
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **回归树：**                                   \n",
    "   基于树的回归方法主要是依据分层和分割的方式将特征空间划分为一系列简单的区域。对某个给定的待预测的自变量，用他所属区域中训练集的平均数或者众数对其进行预测。由于划分特征空间的分裂规则可以用树的形式进行概括，因此这类方法称为决策树方法。决策树由结点(node)和有向边(diredcted edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类别或者某个值。区域$R_1,R_2$等称为叶节点，将特征空间分开的点为内部节点。                           \n",
    "   ![jupyter](./1.7.png)                     \n",
    "   建立回归树的过程大致可以分为以下两步：          \n",
    "      - 将自变量的特征空间(即$x^{(1)},x^{(2)},x^{(3)},...,x^{(p)}$)的可能取值构成的集合分割成J个互不重叠的区域$R_1,R_2,...,R_j$。        \n",
    "      - 对落入区域$R_j$的每个观测值作相同的预测，预测值等于$R_j$上训练集的因变量的简单算术平均。              \n",
    "   具体来说，就是：             \n",
    "      a.  选择最优切分特征j以及该特征上的最优点s：                \n",
    "      遍历特征j以及固定j后遍历切分点s，选择使得下式最小的(j,s)  $min_{j,s}[min_{c_1}\\sum\\limits_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\\sum\\limits_{x_i\\in R_2(j,s)}(y_i-c_2)^2 ]$                           \n",
    "       b. 按照(j,s)分裂特征空间：$R_1(j,s) = \\{x|x^{j} \\le s \\}和R_2(j,s) = \\{x|x^{j} > s \\},\\hat{c}_m = \\frac{1}{N_m}\\sum\\limits_{x \\in R_m(j,s)}y_i,\\;m=1,2$                           \n",
    "       c. 继续调用步骤1，2直到满足停止条件，就是每个区域的样本数小于等于5。        \n",
    "       d. 将特征空间划分为J个不同的区域，生成回归树：$f(x) = \\sum\\limits_{m=1}^{J}\\hat{c}_mI(x \\in R_m)$                \n",
    "       如以下生成的关于运动员在棒球大联盟数据的回归树：             \n",
    "       ![jupyter](./1.8.png)                   \n",
    "    回归树与线性模型的比较：              \n",
    "    线性模型的模型形式与树模型的模型形式有着本质的区别，具体而言，线性回归对模型形式做了如下假定：$f(x) = w_0 + \\sum\\limits_{j=1}^{p}w_jx^{(j)}$，而回归树则是$f(x) = \\sum\\limits_{m=1}^{J}\\hat{c}_mI(x \\in R_m)$。那问题来了，哪种模型更优呢？这个要视具体情况而言，如果特征变量与因变量的关系能很好的用线性关系来表达，那么线性回归通常有着不错的预测效果，拟合效果则优于不能揭示线性结构的回归树。反之，如果特征变量与因变量的关系呈现高度复杂的非线性，那么树方法比传统方法更优。                     \n",
    "    ![jupyter](./1.9.1.png)                        \n",
    "    树模型的优缺点：                 \n",
    "    - 树模型的解释性强，在解释性方面可能比线性回归还要方便。\n",
    "    - 树模型更接近人的决策方式。\n",
    "    - 树模型可以用图来表示，非专业人士也可以轻松解读。\n",
    "    - 树模型可以直接做定性的特征而不需要像线性回归一样哑元化。\n",
    "    - 树模型能很好处理缺失值和异常值，对异常值不敏感，但是这个对线性模型来说却是致命的。\n",
    "    - 树模型的预测准确性一般无法达到其他回归模型的水平，但是改进的方法很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn使用回归树的实例：                \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=tree#sklearn.tree.DecisionTreeRegressor                         \n",
    "sklearn.tree.DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0）                                                 \n",
    "   - 参数：(列举几个重要的，常用的，详情请看上面的官网)                 \n",
    "   criterion：{“ mse”，“ friedman_mse”，“ mae”}，默认=“ mse”。衡量分割标准的函数 。                      \n",
    "   splitter：{“best”, “random”}, default=”best”。分割方式。                    \n",
    "   max_depth：树的最大深度。               \n",
    "   min_samples_split：拆分内部节点所需的最少样本数，默认是2。                      \n",
    "   min_samples_leaf：在叶节点处需要的最小样本数。默认是1。                    \n",
    "   min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。默认是0。                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9376307599929274"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor    \n",
    "reg_tree = DecisionTreeRegressor(criterion = \"mse\",min_samples_leaf = 5)\n",
    "reg_tree.fit(X,y)\n",
    "reg_tree.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 支持向量机回归(SVR)                                        \n",
    "   在介绍支持向量回归SVR之前，我们先来了解下约束优化的相关知识：          \n",
    "      - 约束优化问题(P)：                   \n",
    "      $$\n",
    "      min f(x)  \\\\\n",
    "      s.t.\\;\\;\\;g_i(x) \\le 0,\\; i=1,2,...,m\\\\\n",
    "      \\;\\;\\;\\;\\; h_j(x) = 0,\\; j=1,2,...,l\n",
    "      $$                         \n",
    "   我们假设$x^*$为满足以上条件的局部最优解，$p^* = f(x^*)$，我们的目的就是要找到$x^*$与$p^*$，满足不等式和等式约束的x集合成为可行域，记作S。\n",
    "   \n",
    "    - KKT条件(最优解的一阶必要条件)          \n",
    "    因为KKT条件是最优化的相关内容，在本次开源学习中并不是重点，因此在这里我用一个更加简单的例子说明KKT条件，严格的证明请参见凸优化相关书籍。                       \n",
    "    在这个例子中，我们考虑：($x^*$为我们的最优解)               \n",
    "    $$\n",
    "    minf(x)\\\\\n",
    "    s.t.\\;g_1(x) \\le 0,\\;x \\in R^n\\\\\n",
    "    \\;\\;\\;g_2(x) \\le 0\\\\\n",
    "    \\;\\;\\;g_3(x) \\le 0\n",
    "    $$\n",
    "    ![jupyter](./1.11.png)\n",
    "      \n",
    "      我们可以看到：$-\\nabla f(x^*)$可以由$\\nabla g_1(x^*)$与$\\nabla g_2(x^*)$线性表出，因此有：$-\\nabla f(x^*) = \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*)$，其中$\\lambda_1,\\lambda_2 \\ge 0$，即：                          \n",
    "      $$\n",
    "      \\nabla f(x^*) + \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*) = 0,\\;\\;\\;其中\\lambda_1,\\lambda_2 \\ge 0\n",
    "      $$                       \n",
    "      我们把没有起作用的约束$g_3(x)$也放到式子里面去，目的也就是为了书写方便，即要求：                  \n",
    "      $$\n",
    "      \\nabla f(x^*) + \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*) + \\lambda_3 \\nabla g_3(x^*)= 0,\\;\\;\\;其中\\lambda_1,\\lambda_2 \\ge 0,\\lambda_3 = 0\n",
    "      $$                      \n",
    "      由于点$x^*$位于方程$g_1(x)=0$与$g_2(x)=0$上，因此：$\\lambda_1 g_1(x^*)  = 0,\\lambda_2  g_2(x^*) = 0 , \\lambda_3  g_3(x^*)= 0$                        \n",
    "      \n",
    "      因此，KKT条件就是：假设$x^*$为最优化问题(P)的局部最优解，且$x^*$ 在某个适当的条件下 ,有：                             \n",
    "      $$\n",
    "     \\nabla f(x^*) + \\sum\\limits_{i=1}^{m}\\lambda_i \\nabla g(x^*) + \\sum\\limits_{j=1}^{l}\\mu_j \\nabla h_j(x^*) = 0(对偶条件)\\\\     \n",
    "      \\lambda_i \\ge 0,\\;i = 1,2,...,m(对偶条件)\\\\\n",
    "      g_i(x^*) \\le 0(原问题条件)\\\\\n",
    "      h_j(x^*) = 0(原问题条件)\\\\\n",
    "      \\lambda_i g(x^*) = 0(互补松弛定理)\n",
    "      $$                              \n",
    " - 对偶理论：               \n",
    "   为什么要引入对偶问题呢？是因为原问题与对偶问题就像是一个问题两个角度去看，如利润最大与成本最低等。有时侯原问题上难以解决，但是在对偶问题上就会变得很简单。再者，任何一个原问题在变成对偶问题后都会变成一个凸优化的问题，这点我们后面会有介绍。下面我们来引入对偶问题：                           \n",
    "   首先，我们的原问题(P)是：\n",
    "      $$\n",
    "      min f(x)  \\\\\n",
    "      s.t.\\;\\;\\;g_i(x) \\le 0,\\; i=1,2,...,m\\\\\n",
    "      \\;\\;\\;\\;\\; h_j(x) = 0,\\; j=1,2,...,l\n",
    "      $$                             \n",
    "      引入拉格朗日函数：$L(x,\\lambda,\\mu) = f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x)$                        \n",
    "      拉格朗日对偶函数：                                  \n",
    "      $$\n",
    "      d(\\lambda,\\mu)  =  min_{x\\in X}\\{ f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x)\\} ,其中X为满足条件的x变量\\\\\n",
    "      \\le min_{x\\in S}\\{ f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x) \\},由于g_i(x) \\le 0,h_j(x) = 0,\\lambda_i \\ge 0 ,其中S为可行域\\\\\n",
    "      \\le min_{x\\in S}\\{f(x) \\}\n",
    "      $$                                   \n",
    "      因此：拉格朗日对偶函数$d(\\lambda,\\mu)$是原问题最优解的函数值$p^*$的下界，即每个不同的$\\lambda$与$\\mu$确定的$d(\\lambda,\\mu)$都是$p^*$的下界，但是我们希望下界越大越好，因为越大就更能接近真实的$p^*$。因此：                               \n",
    "      拉格朗日对偶问题(D)转化为：                 \n",
    "      $$\n",
    "      max_{\\lambda,\\mu}d(\\lambda,\\mu)\\\\\n",
    "      s.t. \\lambda_i \\ge 0,i = 1,2,...,m\\\\\n",
    "      也就是：\\\\\n",
    "      max_{\\lambda \\ge 0,\\mu}\\;min_{x \\in S} L(x,\\lambda,\\mu)\n",
    "      $$                   \n",
    "      我们可以观察到，对偶问题是关于$\\lambda$和$\\mu$的线性函数，因此对偶问题是一个凸优化问题，凸优化问题在最优化理论较为简单。\n",
    "      弱对偶定理：对偶问题(D)的最优解$D^*$一定小于原问题最优解$P^*$，这点在刚刚的讨论得到了充分的证明，一定成立。                   \n",
    "      强对偶定理：对偶问题(D)的最优解$D^*$在一定的条件下等于原问题最优解$P^*$，条件非常多样化且不是唯一的，也就是说这是个开放性的问题，在这里我给出一个最简单的条件，即：$f(x)$与$g_i(x)$为凸函数，$h_j(x)$为线性函数，X是凸集，$x^*$满足KKT条件，那么$D^* = P^*$。 \n",
    "      \n",
    "   \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 支持向量回归SVR                    \n",
    "   在介绍完了相关的优化知识以后，我们开始正式学习支持向量回归SVR。                    \n",
    "   ![jupyter](./1.10.png)                         \n",
    "   在线性回归的理论中，每个样本点都要计算平方损失，但是SVR却是不一样的。SVR认为：落在$f(x)$的$\\epsilon$邻域空间中的样本点不需要计算损失，这些都是预测正确的，其余的落在$\\epsilon$邻域空间以外的样本才需要计算损失，因此：              \n",
    "   ![jupyter](./1.12.png)                                      \n",
    "   $$\n",
    "   min_{w,b,\\xi_i,\\hat{\\xi}_i} \\frac{1}{2}||w||^2 +C \\sum\\limits_{i=1}^{N}(\\xi_i,\\hat{\\xi}_i)\\\\\n",
    "   s.t.\\;\\;\\; f(x_i) - y_i \\le \\epsilon + \\xi_i\\\\\n",
    "   \\;\\;\\;\\;\\;y_i - f(x_i) \\le  \\epsilon +\\hat{\\xi}_i\\\\\n",
    "   \\;\\;\\;\\;\\; \\xi_i,\\hat{\\xi}_i \\le 0,i = 1,2,...,N\n",
    "   $$                                 \n",
    "   引入拉格朗日函数：                  \n",
    "   $$\n",
    "   \\begin{array}{l}\n",
    "L(w, b, \\alpha, \\hat{\\alpha}, \\xi, \\xi, \\mu, \\hat{\\mu}) \\\\\n",
    "\\quad=\\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N}\\left(\\xi_{i}+\\widehat{\\xi}_{i}\\right)-\\sum_{i=1}^{N} \\xi_{i} \\mu_{i}-\\sum_{i=1}^{N} \\widehat{\\xi}_{i} \\widehat{\\mu}_{i} \\\\\n",
    "\\quad+\\sum_{i=1}^{N} \\alpha_{i}\\left(f\\left(x_{i}\\right)-y_{i}-\\epsilon-\\xi_{i}\\right)+\\sum_{i=1}^{N} \\widehat{\\alpha}_{i}\\left(y_{i}-f\\left(x_{i}\\right)-\\epsilon-\\widehat{\\xi}_{i}\\right)\n",
    "\\end{array}\n",
    "   $$                       \n",
    "   再令$L(w, b, \\alpha, \\hat{\\alpha}, \\xi, \\xi, \\mu, \\hat{\\mu})$对$w,b,\\xi,\\hat{\\xi}$求偏导等于0，得： $w=\\sum_{i=1}^{N}\\left(\\widehat{\\alpha}_{i}-\\alpha_{i}\\right) x_{i}$。                             \n",
    "   上述过程中需满足KKT条件，即要求：                 \n",
    "   $$\n",
    "   \\left\\{\\begin{array}{c}\n",
    "\\alpha_{i}\\left(f\\left(x_{i}\\right)-y_{i}-\\epsilon-\\xi_{i}\\right)=0 \\\\\n",
    "\\hat{\\alpha_{i}}\\left(y_{i}-f\\left(x_{i}\\right)-\\epsilon-\\hat{\\xi}_{i}\\right)=0 \\\\\n",
    "\\alpha_{i} \\widehat{\\alpha}_{i}=0, \\xi_{i} \\hat{\\xi}_{i}=0 \\\\\n",
    "\\left(C-\\alpha_{i}\\right) \\xi_{i}=0,\\left(C-\\widehat{\\alpha}_{i}\\right) \\hat{\\xi}_{i}=0\n",
    "\\end{array}\\right.\n",
    "   $$                         \n",
    "   SVR的解形如:$f(x)=\\sum_{i=1}^{N}\\left(\\widehat{\\alpha}_{i}-\\alpha_{i}\\right) x_{i}^{T} x+b$                                                  \n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn中使用SVR实例：                        \n",
    "sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)            \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svr#sklearn.svm.SVR                        \n",
    "   - 参数：                   \n",
    "   kernel：核函数，{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, 默认=’rbf’。(后面会详细介绍)              \n",
    "   degree：多项式核函数的阶数。默认 = 3。                          \n",
    "   C：正则化参数，默认=1.0。(后面会详细介绍)                            \n",
    "   epsilon：SVR模型允许的不计算误差的邻域大小。默认0.1。              \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7024525421955277"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler     # 标准化数据\n",
    "from sklearn.pipeline import make_pipeline   # 使用管道，把预处理和模型形成一个流程\n",
    "\n",
    "reg_svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "reg_svr.fit(X, y)\n",
    "reg_svr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) 优化基础模型                   \n",
    "在刚刚的回归问题的基本算法中，我们使用数据集去估计模型的参数，如线性回归模型中的参数w，那么这个数据集我们称为训练数据集，简称训练集。我们在回归问题中使用训练集估计模型的参数的原则一般都是使得我们的损失函数在训练集达到最小值，其实在实际问题中我们是可以让损失函数在训练集最小化为0，如：在线性回归中，我加入非常多的高次项，使得我们模型在训练集的每一个数据点都恰好位于曲线上，那这时候模型在训练集的损失值也就是误差为0。                \n",
    "![jupyter](./1.13.png)                            \n",
    "既然能做到这件事，是不是代表我们的建模完事大吉呢？换句话说我们的模型可以预测任意情况呢？答案是显然否定的。我们建立机器学习的目的并不是为了在已有的数据集，也就是训练集上效果表现非常优异，我们希望建立的机器学习模型在未知且情况复杂的测试数据上表现优异，我们称这样的未出现在训练集的未知数据集成为测试数据集，简称测试集。我们希望模型在测试集上表现优异！因为假如我们根据股票市场前六个月的数据拟合一个预测模型，我们的目的不是为了预测以前这六个月越准越好，而是预测明天乃至未来的股价变化。                                  \n",
    "   - (a) 训练均方误差与测试均方误差：                   \n",
    "   在回归中，我们最常用的评价指标为均方误差，即：$MSE = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(y_i -\\hat{ f}(x_i))^2$，其中$\\hat{ f}(x_i)$是样本$x_i$应用建立的模型$\\hat{f}$预测的结果。如果我们所用的数据是训练集上的数据，那么这个误差为训练均方误差，如果我们使用测试集的数据计算的均方误差，我们称为测试均方误差。一般而言，我们并不关心模型在训练集上的训练均方误差，我们关心的是模型面对未知的样本集，即测试集上的测试误差，我们的目标是使得我们建立的模型在测试集上的测试误差最小。那我们如何选择一个测试误差最小的模型呢？这是个棘手的问题，因为在模型建立阶段，我们是不能得到测试数据的，比如：我们在模型未上线之前是不能拿到未知且真实的测试数据来验证我们的模型的。在这种情况下，为了简便起见，一些观点认为通过训练误差最小化来选择模型也是可行的。这种观点表面看上去是可行的，但是存在一个致命的缺点，那就是：一个模型的训练均方误差最小时，不能保证测试均方误差同时也很小。对于这种想法构造的模型，一般在训练误差达到最小时，测试均方误差一般很大！如图：                       \n",
    "   ![jupyter](./1.14.png)                       \n",
    "   ![jupyter](./1.15.png)                    \n",
    "   可以看到：当我们的模型的训练均方误差达到很小时，测试均方误差反而很大，但是我们寻找的最优的模型是测试均方误差达到最小时对应的模型，因此基于训练均方误差达到最小选择模型本质上是行不同的。正如上右图所示：模型在训练误差很小，但是测试均方误差很大时，我们称这种情况叫模型的**过拟合**。                            \n",
    "   - (b) 偏差-方差的权衡：                         \n",
    "   从上图的测试均方误差曲线可以看到：测试均方误差曲线呈现U型曲线，这表明了在测试误差曲线中有两种力量在互相博弈。可以证明：           \n",
    "   $$\n",
    "   E\\left(y_{0}-\\hat{f}\\left(x_{0}\\right)\\right)^{2}=\\operatorname{Var}\\left(\\hat{f}\\left(x_{0}\\right)\\right)+\\left[\\operatorname{Bias}\\left(\\hat{f}\\left(x_{0}\\right)\\right)\\right]^{2}+\\operatorname{Var}(\\varepsilon)\n",
    "   $$                  \n",
    "   也就是说，我们的测试均方误差的期望值可以分解为$\\hat{f}(x_0)$的方差、$\\hat{f}(x_0)$的偏差平方和误差项$\\epsilon$的方差。为了使得模型的测试均方误差达到最小值，也就是同时最小化偏差的平方和方差。由于我们知道偏差平方和方差本身是非负的，因此测试均方误差的期望不可能会低于误差的方差，因此我们称$\\operatorname{Var}(\\varepsilon)$为建模任务的难度，这个量在我们的任务确定后是无法改变的，也叫做不可约误差。那么模型的方差和偏差的平方和究竟是什么呢？所谓模型的方差就是：用不同的数据集去估计$f$时，估计函数的改变量。举个例子：我们想要建立一个线性回归模型，可以通过输入中国人身高去预测我们的体重。但是显然我们没有办法把全中国13亿人做一次人口普查，拿到13亿人的身高体重去建立模型。我们能做的就是从13亿中抽1000个样本进行建模，我们对这个抽样的过程重复100遍，就会得到100个1000人的样本集。我们使用线性回归模型估计参数就能得到100个线性回归模型。由于样本抽取具有随机性，我们得到的100个模型不可能参数完全一样，那么这100个模型之间的差异就叫做方差。显然，我们希望得到一个稳定的模型，也就是在不同的样本集估计的模型都不会相差太大，即要求f的方差越小越好。**一般来说，模型的复杂度越高，f的方差就会越大。** 如加入二次项的模型的方差比线性回归模型的方差要大。                              \n",
    "   ![jupyter](./1.16.png)                                \n",
    "   另一方面，模型的偏差是指：为了选择一个简单的模型去估计真实函数所带入的误差。假如真实的数据X与Y的关系是二次关系，但是我们选择了线性模型进行建模，那由于模型的复杂度引起的这种误差我们称为偏差，它的构成时复杂的。偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。偏差度量的是单个模型的学习能力，而方差度量的是同一个模型在不同数据集上的稳定性。“偏差-方差分解”说明：泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。                          \n",
    "   ![jupyter](./1.17.png)                    \n",
    "   一般而言，增加模型的复杂度，会增加模型的方差，但是会减少模型的偏差，我们要找到一个方差--偏差的权衡，使得测试均方误差最。                        \n",
    "   ![jupyter](./1.18.png)                              \n",
    "   (c) 特征提取：              \n",
    "   在前面的讨论中，我们已经明确一个目标，就是：我们要选择一个测试误差达到最小的模型。但是实际上我们很难对实际的测试误差做精确的计算，因此我们要对测试误差进行估计，估计的方式有两种：训练误差修正与交叉验证。                         \n",
    "      - 训练误差修正：      \n",
    "      前面的讨论我们已经知道，模型越复杂，训练误差越小，测试误差先减后增。因此，我们先构造一个特征较多的模型使其过拟合，此时训练误差很小而测试误差很大，那这时我们加入关于特征个数的惩罚。因此，当我们的训练误差随着特征个数的增加而减少时，惩罚项因为特征数量的增加而增大，抑制了训练误差随着特征个数的增加而无休止地减小。具体的数学量如下：             \n",
    "      $C_p = \\frac{1}{N}(RSS  +  2d\\hat{\\sigma}^2)$，其中d为模型特征个数，$RSS = \\sum\\limits_{i=1}^{N}(y_i-\\hat{f}(x_i))^2$，$\\hat{\\sigma}^2$为模型预测误差的方差的估计值，即残差的方差。                       \n",
    "      AIC赤池信息量准则：$AIC = \\frac{1}{d\\hat{\\sigma}^2}(RSS  +  2d\\hat{\\sigma}^2)$                              \n",
    "      BIC贝叶斯信息量准则：$BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)$                     \n",
    "      - 交叉验证：                   \n",
    "      前面讨论的对训练误差修正得到测试误差的估计是间接方法，这种方法的桥梁是训练误差，而交叉验证则是对测试误差的直接估计。交叉验证比训练误差修正的优势在于：能够给出测试误差的一个直接估计。在这里只介绍K折交叉验证：我们把训练样本分成K等分，然后用K-1个样本集当做训练集，剩下的一份样本集为验证集去估计由K-1个样本集得到的模型的精度，这个过程重复K次取平均值得到测试误差的一个估计$CV_{(K)} = \\frac{1}{K}\\sum\\limits_{i=1}^{K}MSE_i$。5折交叉验证如下图：（蓝色的是训练集，黄色的是验证集）                     \n",
    "      ![jupyter](./1.19.png)                   \n",
    "   \n",
    "   在测试误差能够被合理的估计出来以后，我们做特征选择的目标就是：从p个特征中选择m个特征，使得对应的模型的测试误差的估计最小。对应的方法有：\n",
    "      - 最优子集选择：                  \n",
    "      (i) 记不含任何特征的模型为$M_0$，计算这个$M_0$的测试误差。                                              \n",
    "      (ii) 在$M_0$基础上增加一个变量，计算p个模型的RSS，选择RSS最小的模型记作$M_1$，并计算该模型$M_1$的测试误差。                                          \n",
    "      (iii) 再增加变量，计算p-1个模型的RSS，并选择RSS最小的模型记作$M_2$，并计算该模型$M_2$的测试误差。                                                \n",
    "      (iv) 重复以上过程知道拟合的模型有p个特征为止，并选择p+1个模型$\\{M_0,M_1,...,M_p \\}$中测试误差最小的模型作为最优模型。                           \n",
    "      - 向前逐步选择：               \n",
    "      最优子集选择虽然在原理上很直观，但是随着数据特征维度p的增加，子集的数量为$2^p$，计算效率非常低下且需要的计算内存也很高，在大数据的背景下显然不适用。因此，我们需要把最优子集选择的运算效率提高，因此向前逐步选择算法的过程如下：                            \n",
    "      (i) 记不含任何特征的模型为$M_0$，计算这个$M_0$的测试误差。                    \n",
    "      (ii) 在$M_0$基础上增加一个变量，计算p个模型的RSS，选择RSS最小的模型记作$M_1$，并计算该模型$M_1$的测试误差。                  \n",
    "      (iii) 在最小的RSS模型下继续增加一个变量，选择RSS最小的模型记作$M_2$，并计算该模型$M_2$的测试误差。                  \n",
    "      (iv) 以此类推，重复以上过程知道拟合的模型有p个特征为止，并选择p+1个模型$\\{M_0,M_1,...,M_p \\}$中测试误差最小的模型作为最优模型。                      \n",
    "  \n",
    "  (d) 压缩估计(正则化)：                         \n",
    "  除了刚刚讨论的直接对特征自身进行选择以外，我们还可以对回归的系数进行约束或者加罚的技巧对p个特征的模型进行拟合，显著降低模型方差，这样也会提高模型的拟合效果。具体来说，就是将回归系数往零的方向压缩，这也就是为什么叫压缩估计的原因了。                          \n",
    "     - 岭回归(L2正则化的例子)：                 \n",
    "     在线性回归中，我们的损失函数为$J(w) = \\sum\\limits_{i=1}^{N}(y_i-w_0-\\sum\\limits_{j=1}^{p}w_jx_{ij})^2$，我们在线性回归的损失函数的基础上添加对系数的约束或者惩罚，即：                           \n",
    "     $$\n",
    "     J(w) = \\sum\\limits_{i=1}^{N}(y_i-w_0-\\sum\\limits_{j=1}^{p}w_jx_{ij})^2 + \\lambda\\sum\\limits_{j=1}^{p}w_j^2,\\;\\;其中，\\lambda \\ge 0\\\\\n",
    "     \\hat{w} = (X^TX + \\lambda I)^{-1}X^TY\n",
    "     $$                        \n",
    "   调节参数$\\lambda$的大小是影响压缩估计的关键，$\\lambda$越大，惩罚的力度越大，系数则越趋近于0，反之，选择合适的$\\lambda$对模型精度来说十分重要。岭回归通过牺牲线性回归的无偏性降低方差，有可能使得模型整体的测试误差较小，提高模型的泛化能力。                                 \n",
    "     - Lasso回归(L1正则化的例子)：             \n",
    "     岭回归的一个很显著的特点是：将模型的系数往零的方向压缩，但是岭回归的系数只能呢个趋于0但无法等于0，换句话说，就是无法做特征选择。能否使用压缩估计的思想做到像特征最优子集选择那样提取出重要的特征呢？答案是肯定的！我们只需要对岭回归的优化函数做小小的调整就行了，我们使用系数向量的L1范数替换岭回归中的L2范数：                     \n",
    "     $$\n",
    "     J(w) = \\sum\\limits_{i=1}^{N}(y_i-w_0-\\sum\\limits_{j=1}^{p}w_jx_{ij})^2 + \\lambda\\sum\\limits_{j=1}^{p}|w_j|,\\;\\;其中，\\lambda \\ge 0\n",
    "     $$                  \n",
    "   为什么Losso能做到特征选择而岭回归却不能呢个做到呢？(如图：左边为lasso，右边为岭回归)                \n",
    "   ![jupyter](./1.20.png)                       \n",
    "   椭圆形曲线为RSS等高线，菱形和圆形区域分别代表了L1和L2约束，Lsaao回归和岭回归都是在约束下的回归，因此最优的参数为椭圆形曲线与菱形和圆形区域相切的点。但是Lasso回归的约束在每个坐标轴上都有拐角，因此当RSS曲线与坐标轴相交时恰好回归系数中的某一个为0，这样就实现了特征提取。反观岭回归的约束是一个圆域，没有尖点，因此与RSS曲线相交的地方一般不会出现在坐标轴上，因此无法让某个特征的系数为0，因此无法做到特征提取。                                \n",
    "   \n",
    "   (e) 降维：                \n",
    "   到目前为止，我们所讨论的方法对方差的控制有两种方式：一种是使用原始变量的子集，另一种是将变量系数压缩至零。但是这些方法都是基于原始特征$x_1,...,x_p$得到的，现在我们探讨一类新的方法：将原始的特征空间投影到一个低维的空间实现变量的数量变少，如：将二维的平面投影至一维空间。机器学习领域中所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x->y，其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。目前大部分降维算法处理向量表达的数据，也有一些降维算法处理高阶张量表达的数据。之所以使用降维后的数据表示是因为在原始的高维空间中，包含有冗余信息以及噪音信息，在实际应用例如图像识别中造成了误差，降低了准确率；而通过降维,我们希望减少 冗余信息 所造成的误差,提高识别（或其他应用）的精度。又或者希望通过降维算法来寻找数据内部的本质结构特征。在很多算法中，降维算法成为了数据预处理的一部分，如PCA。事实上，有一些算法如果没有降维预处理，其实是很难得到很好的效果的。 (摘自：rosenor1博客)                                \n",
    "   主成分分析(PCA)：                     \n",
    "   主成分分析的思想：通过**最大投影方差** 将原始空间进行重构，即由特征相关重构为无关，即落在某个方向上的点(投影)的方差最大。在进行下一步推导之前，我们先把样本均值和样本协方差矩阵推广至矩阵形式：                     \n",
    "   样本均值Mean:$\\bar{x} = \\frac{1}{N}\\sum\\limits_{i=1}^{N}x_i =  \\frac{1}{N}X^T1_N,\\;\\;\\;其中1_N = (1,1,...,1)_{N}^T$                         \n",
    "   样本协方差矩阵$S^2 = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(x_i-\\bar{x})(x_i-\\bar{x})^T = \\frac{1}{N}X^THX,\\;\\;\\;其中，H = I_N - \\frac{1}{N}1_N1_N^T$                             \n",
    "   最大投影方差的步骤：                \n",
    "   (i) 中心化：$x_i - \\bar{x}$                       \n",
    "   (ii) 计算每个点$x_1,...,x_N$至$\\vec{u}_1$方向上的投影：$(x_i-\\bar{x})\\vec{u}_1,\\;\\;\\;||\\vec{u}_1|| = 1$                             \n",
    "   (iii) 计算投影方差：$J = \\frac{1}{N}\\sum\\limits_{i=1}^{N}[(x_i-\\bar{x})^T\\vec{u}_1]^2,\\;\\;\\;||\\vec{u}_1|| = 1$                           \n",
    "   (iv) 最大化投影方差求$\\vec{u}_1$：                       \n",
    "   $$\n",
    "   \\bar{u}_1 = argmax_{u_1}\\;\\;\\frac{1}{N}\\sum\\limits_{i=1}^{N}[(x_i-\\bar{x})^T\\vec{u}_1]^2 \\\\\n",
    "   \\;\\;\\;s.t. \\vec{u}_1^T\\vec{u}_1 = 1 (\\vec{u}_1往后不带向量符号)\n",
    "   $$                         \n",
    "   得到：               \n",
    "   $$\n",
    "   J = \\frac{1}{N}\\sum\\limits_{i=1}^{N}[(x_i-\\bar{x})^T\\vec{u}_1]^2  = \\frac{1}{N}\\sum\\limits_{i=1}^{N}[u_1^T(x_i-\\bar{x})(x_i-\\bar{x})^Tu_1]\\\\\n",
    "   \\; = u_1^T[\\frac{1}{N}\\sum\\limits_{i=1}^{N}(x_i-\\bar{x})(x_i - \\bar{x})^T]u_1 = u_1^TS^2u_1\n",
    "   $$                    \n",
    "   即：           \n",
    "   $$\n",
    "   \\hat{u}_1 = argmax_{u_1}u_1^TS^2u_1,\\;\\;\\;s.t.u_1^Tu_1 = 1\\\\\n",
    "   L(u_1,\\lambda) = u_1^TS^2u_1 + \\lambda (1-u_1^Tu_1)\\\\\n",
    "   \\frac{\\partial L}{\\partial u_1} = 2S^2u_1-2\\lambda u_1 = 0\\\\\n",
    "   即：S^2u_1 = \\lambda u_1\n",
    "   $$                        \n",
    "   可以看到：$\\lambda$为$S^2$的特征值，$u_1$为$S^2$的特征向量。因此我们只需要对中心化后的协方差矩阵进行特征值分解，得到的特征向量即为投影方向。如果需要进行降维，那么只需要取p的前M个特征向量即可。\n",
    "   \n",
    "    \n",
    "   \n",
    "      \n",
    "      \n",
    "   \n",
    "\n",
    "\n",
    "     \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征提取的实例：向前逐步回归                      \n",
    "案例来源：https://blog.csdn.net/weixin_44835596/article/details/89763300                            \n",
    "根据AIC准则定义向前逐步回归进行变量筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义向前逐步回归函数\n",
    "def forward_select(data,target):\n",
    "    variate=set(data.columns)  #将字段名转换成字典类型\n",
    "    variate.remove(target)  #去掉因变量的字段名\n",
    "    selected=[]\n",
    "    current_score,best_new_score=float('inf'),float('inf')  #目前的分数和最好分数初始值都为无穷大（因为AIC越小越好）\n",
    "    #循环筛选变量\n",
    "    while variate:\n",
    "        aic_with_variate=[]\n",
    "        for candidate in variate:  #逐个遍历自变量\n",
    "            formula=\"{}~{}\".format(target,\"+\".join(selected+[candidate]))  #将自变量名连接起来\n",
    "            aic=ols(formula=formula,data=data).fit().aic  #利用ols训练模型得出aic值\n",
    "            aic_with_variate.append((aic,candidate))  #将第每一次的aic值放进空列表\n",
    "        aic_with_variate.sort(reverse=True)  #降序排序aic值\n",
    "        best_new_score,best_candidate=aic_with_variate.pop()  #最好的aic值等于删除列表的最后一个值，以及最好的自变量等于列表最后一个自变量\n",
    "        if current_score>best_new_score:  #如果目前的aic值大于最好的aic值\n",
    "            variate.remove(best_candidate)  #移除加进来的变量名，即第二次循环时，不考虑此自变量了\n",
    "            selected.append(best_candidate)  #将此自变量作为加进模型中的自变量\n",
    "            current_score=best_new_score  #最新的分数等于最好的分数\n",
    "            print(\"aic is {},continuing!\".format(current_score))  #输出最小的aic值\n",
    "        else:\n",
    "            print(\"for selection over!\")\n",
    "            break\n",
    "    formula=\"{}~{}\".format(target,\"+\".join(selected))  #最终的模型式子\n",
    "    print(\"final formula is {}\".format(formula))\n",
    "    model=ols(formula=formula,data=data).fit()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aic is 3286.974956900157,continuing!\n",
      "aic is 3171.5423142992013,continuing!\n",
      "aic is 3114.0972674193326,continuing!\n",
      "aic is 3097.359044862759,continuing!\n",
      "aic is 3069.438633167217,continuing!\n",
      "aic is 3057.9390497191152,continuing!\n",
      "aic is 3048.438382711162,continuing!\n",
      "aic is 3042.274993098419,continuing!\n",
      "aic is 3040.154562175143,continuing!\n",
      "aic is 3032.0687017003256,continuing!\n",
      "aic is 3021.726387825062,continuing!\n",
      "for selection over!\n",
      "final formula is Price~LSTAT+RM+PTRATIO+DIS+NOX+CHAS+B+ZN+CRIM+RAD+TAX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7fa523c32c10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm #最小二乘\n",
    "from statsmodels.formula.api import ols #加载ols模型\n",
    "forward_select(data=boston_data,target=\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Price</td>      <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.735</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   128.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 12 Dec 2020</td> <th>  Prob (F-statistic):</th> <td>5.54e-137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:57:30</td>     <th>  Log-Likelihood:    </th> <td> -1498.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3022.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   494</td>      <th>  BIC:               </th> <td>   3072.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   36.3411</td> <td>    5.067</td> <td>    7.171</td> <td> 0.000</td> <td>   26.385</td> <td>   46.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>     <td>   -0.5226</td> <td>    0.047</td> <td>  -11.019</td> <td> 0.000</td> <td>   -0.616</td> <td>   -0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>        <td>    3.8016</td> <td>    0.406</td> <td>    9.356</td> <td> 0.000</td> <td>    3.003</td> <td>    4.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th>   <td>   -0.9465</td> <td>    0.129</td> <td>   -7.334</td> <td> 0.000</td> <td>   -1.200</td> <td>   -0.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>       <td>   -1.4927</td> <td>    0.186</td> <td>   -8.037</td> <td> 0.000</td> <td>   -1.858</td> <td>   -1.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>       <td>  -17.3760</td> <td>    3.535</td> <td>   -4.915</td> <td> 0.000</td> <td>  -24.322</td> <td>  -10.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>      <td>    2.7187</td> <td>    0.854</td> <td>    3.183</td> <td> 0.002</td> <td>    1.040</td> <td>    4.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>         <td>    0.0093</td> <td>    0.003</td> <td>    3.475</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>        <td>    0.0458</td> <td>    0.014</td> <td>    3.390</td> <td> 0.001</td> <td>    0.019</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>      <td>   -0.1084</td> <td>    0.033</td> <td>   -3.307</td> <td> 0.001</td> <td>   -0.173</td> <td>   -0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>       <td>    0.2996</td> <td>    0.063</td> <td>    4.726</td> <td> 0.000</td> <td>    0.175</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>       <td>   -0.0118</td> <td>    0.003</td> <td>   -3.493</td> <td> 0.001</td> <td>   -0.018</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.430</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 787.785</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.523</td>  <th>  Prob(JB):          </th> <td>8.60e-172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.300</td>  <th>  Cond. No.          </th> <td>1.47e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.47e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Price   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.735\n",
       "Method:                 Least Squares   F-statistic:                     128.2\n",
       "Date:                Sat, 12 Dec 2020   Prob (F-statistic):          5.54e-137\n",
       "Time:                        17:57:30   Log-Likelihood:                -1498.9\n",
       "No. Observations:                 506   AIC:                             3022.\n",
       "Df Residuals:                     494   BIC:                             3072.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     36.3411      5.067      7.171      0.000      26.385      46.298\n",
       "LSTAT         -0.5226      0.047    -11.019      0.000      -0.616      -0.429\n",
       "RM             3.8016      0.406      9.356      0.000       3.003       4.600\n",
       "PTRATIO       -0.9465      0.129     -7.334      0.000      -1.200      -0.693\n",
       "DIS           -1.4927      0.186     -8.037      0.000      -1.858      -1.128\n",
       "NOX          -17.3760      3.535     -4.915      0.000     -24.322     -10.430\n",
       "CHAS           2.7187      0.854      3.183      0.002       1.040       4.397\n",
       "B              0.0093      0.003      3.475      0.001       0.004       0.015\n",
       "ZN             0.0458      0.014      3.390      0.001       0.019       0.072\n",
       "CRIM          -0.1084      0.033     -3.307      0.001      -0.173      -0.044\n",
       "RAD            0.2996      0.063      4.726      0.000       0.175       0.424\n",
       "TAX           -0.0118      0.003     -3.493      0.001      -0.018      -0.005\n",
       "==============================================================================\n",
       "Omnibus:                      178.430   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              787.785\n",
       "Skew:                           1.523   Prob(JB):                    8.60e-172\n",
       "Kurtosis:                       8.300   Cond. No.                     1.47e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.47e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm=ols(\"Price~LSTAT+RM+PTRATIO+DIS+NOX+CHAS+B+ZN+CRIM+RAD+TAX\",data=boston_data).fit()\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "岭回归实例分享：                   \n",
    "sklearn.linear_model.ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.001, verbose=0, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)                          \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge_regression.html?highlight=rid#sklearn.linear_model.ridge_regression                      \n",
    "   - 参数：                     \n",
    "   alpha：较大的值表示更强的正则化。浮点数                   \n",
    "   sample_weight：样本权重，默认无。                      \n",
    "   solver：求解方法，{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}, 默认=’auto’。“ svd”使用X的奇异值分解来计算Ridge系数。'cholesky'使用标准的scipy.linalg.solve函数通过dot（XT，X）的Cholesky分解获得封闭形式的解。'sparse_cg'使用scipy.sparse.linalg.cg中的共轭梯度求解器。作为一种迭代算法，对于大规模数据（可能设置tol和max_iter），此求解器比“ Cholesky”更合适。 lsqr”使用专用的正则化最小二乘例程scipy.sparse.linalg.lsqr。它是最快的，并且使用迭代过程。“ sag”使用随机平均梯度下降，“ saga”使用其改进的无偏版本SAGA。两种方法都使用迭代过程，并且当n_samples和n_features都很大时，通常比其他求解器更快。请注意，只有在比例大致相同的要素上才能确保“ sag”和“ saga”快速收敛。您可以使用sklearn.preprocessing中的缩放器对数据进行预处理。最后五个求解器均支持密集和稀疏数据。但是，当fit_intercept为True时，仅'sag'和'sparse_cg'支持稀疏输入。                             \n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.739957023371629"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg_rid = linear_model.Ridge(alpha=.5)\n",
    "reg_rid.fit(X,y)\n",
    "reg_rid.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso实例分享：                  \n",
    "class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')                           \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?highlight=lasso#sklearn.linear_model.Lasso                                \n",
    "   - 参数：                \n",
    "   alpha：正则化强度，1.0代表标准最小二乘。                \n",
    "   fit_intercept：是否计算模型截距。默认true。                         \n",
    "   normalize：是否标准化，默认false。                   \n",
    "   positive：是否强制系数为正，默认false。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7140164719858566"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg_lasso = linear_model.Lasso(alpha = 0.5)\n",
    "reg_lasso.fit(X,y)\n",
    "reg_lasso.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) 对模型超参数进行调优(调参)：                             \n",
    "在刚刚的讨论中，我们似乎对模型的优化都是对模型算法本身的改进，比如：岭回归对线性回归的优化在于在线性回归的损失函数中加入L2正则化项从而牺牲无偏性降低方差。但是，大家是否想过这样的问题：在L2正则化中参数$\\lambda$应该选择多少？是0.01、0.1、还是1？到目前为止，我们只能凭经验或者瞎猜，能不能找到一种方法找到最优的参数$\\lambda$？事实上，找到最佳参数的问题本质上属于最优化的内容，因为从一个参数集合中找到最佳的值本身就是最优化的任务之一，我们脑海中浮现出来的算法无非就是：梯度下降法、牛顿法等无约束优化算法或者约束优化算法，但是在具体验证这个想法是否可行之前，我们必须先认识两个最本质概念的区别。                                     \n",
    "   - 参数与超参数：                            \n",
    "   我们很自然的问题就是岭回归中的参数$\\lambda$和参数w之间有什么不一样？事实上，参数w是我们通过设定某一个具体的$\\lambda$后使用类似于最小二乘法、梯度下降法等方式优化出来的，我们总是设定了$\\lambda$是多少后才优化出来的参数w。因此，类似于参数w一样，使用最小二乘法或者梯度下降法等最优化算法优化出来的数我们称为参数，类似于$\\lambda$一样，我们无法使用最小二乘法或者梯度下降法等最优化算法优化出来的数我们称为超参数。                                       \n",
    "   模型参数是模型内部的配置变量，其值可以根据数据进行估计。                       \n",
    "      - 进行预测时需要参数。                     \n",
    "      - 它参数定义了可使用的模型。                        \n",
    "      - 参数是从数据估计或获悉的。                       \n",
    "      - 参数通常不由编程者手动设置。                     \n",
    "      - 参数通常被保存为学习模型的一部分。                      \n",
    "      - 参数是机器学习算法的关键，它们通常由过去的训练数据中总结得出 。                          \n",
    "   模型超参数是模型外部的配置，其值无法从数据中估计。\n",
    "      - 超参数通常用于帮助估计模型参数。\n",
    "      - 超参数通常由人工指定。\n",
    "      - 超参数通常可以使用启发式设置。\n",
    "      - 超参数经常被调整为给定的预测建模问题。                            \n",
    "   我们前面(4)部分的优化都是基于模型本身的具体形式的优化，那本次(5)调整的内容是超参数，也就是取不同的超参数的值对于模型的性能有不同的影响。                             \n",
    "   - 网格搜索GridSearchCV()：                \n",
    "   网格搜索：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV                         \n",
    "   网格搜索结合管道：https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html?highlight=gridsearchcv                              \n",
    "   网格搜索的思想非常简单，比如你有2个超参数需要去选择，那你就把所有的超参数选择列出来分别做排列组合。举个例子：$\\lambda = 0.01,0.1,1.0$和$\\alpha = 0.01,0.1,1.0$,你可以做一个排列组合，即：{[0.01,0.01],[0.01,0.1],[0.01,1],[0.1,0.01],[0.1,0.1],[0.1,1.0],[1,0.01],[1,0.1],[1,1]}  ，然后针对每组超参数分别建立一个模型，然后选择测试误差最小的那组超参数。换句话说，我们需要从超参数空间中寻找最优的超参数，很像一个网格中找到一个最优的节点，因此叫网格搜索。                         \n",
    "   - 随机搜索 RandomizedSearchCV() ：               \n",
    "   https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV                           \n",
    "   网格搜索相当于暴力地从参数空间中每个都尝试一遍，然后选择最优的那组参数，这样的方法显然是不够高效的，因为随着参数类别个数的增加，需要尝试的次数呈指数级增长。有没有一种更加高效的调优方式呢？那就是使用随机搜索的方式，这种方式不仅仅高校，而且实验证明，随机搜索法结果比稀疏化网格法稍好(有时候也会极差，需要权衡)。参数的随机搜索中的每个参数都是从可能的参数值的分布中采样的。与网格搜索相比，这有两个主要优点：        \n",
    "      - 可以独立于参数数量和可能的值来选择计算成本。                 \n",
    "      - 添加不影响性能的参数不会降低效率。                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们使用SVR的例子，结合管道来进行调优："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.187 +/- 0.649\n"
     ]
    }
   ],
   "source": [
    "# 我们先来对未调参的SVR进行评价： \n",
    "from sklearn.svm import SVR     # 引入SVR类\n",
    "from sklearn.pipeline import make_pipeline   # 引入管道简化学习流程\n",
    "from sklearn.preprocessing import StandardScaler # 由于SVR基于距离计算，引入对数据进行标准化的类\n",
    "from sklearn.model_selection import GridSearchCV  # 引入网格搜索调优\n",
    "from sklearn.model_selection import cross_val_score # 引入K折交叉验证\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "boston = datasets.load_boston()     # 返回一个类似于字典的类\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "pipe_SVR = make_pipeline(StandardScaler(),\n",
    "                                                         SVR())\n",
    "score1 = cross_val_score(estimator=pipe_SVR,\n",
    "                                                     X = X,\n",
    "                                                     y = y,\n",
    "                                                     scoring = 'r2',\n",
    "                                                      cv = 10)       # 10折交叉验证\n",
    "print(\"CV accuracy: %.3f +/- %.3f\" % ((np.mean(score1)),np.std(score1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网格搜索最优得分： 0.6081303070817233\n",
      "网格搜索最优参数组合：\n",
      " {'svr__C': 1000.0, 'svr__gamma': 0.001, 'svr__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# 下面我们使用网格搜索来对SVR调参：\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_svr = Pipeline([(\"StandardScaler\",StandardScaler()),\n",
    "                                                         (\"svr\",SVR())])\n",
    "param_range = [0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0]\n",
    "param_grid = [{\"svr__C\":param_range,\"svr__kernel\":[\"linear\"]},  # 注意__是指两个下划线，一个下划线会报错的\n",
    "                            {\"svr__C\":param_range,\"svr__gamma\":param_range,\"svr__kernel\":[\"rbf\"]}]\n",
    "gs = GridSearchCV(estimator=pipe_svr,\n",
    "                                                     param_grid = param_grid,\n",
    "                                                     scoring = 'r2',\n",
    "                                                      cv = 10)       # 10折交叉验证\n",
    "gs = gs.fit(X,y)\n",
    "print(\"网格搜索最优得分：\",gs.best_score_)\n",
    "print(\"网格搜索最优参数组合：\\n\",gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机搜索最优得分： 0.30021249798866756\n",
      "随机搜索最优参数组合：\n",
      " {'svr__C': 1.4195029566223933, 'svr__gamma': 1.8683733769303625, 'svr__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# 下面我们使用随机搜索来对SVR调参：\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform  # 引入均匀分布设置参数\n",
    "pipe_svr = Pipeline([(\"StandardScaler\",StandardScaler()),\n",
    "                                                         (\"svr\",SVR())])\n",
    "distributions = dict(svr__C=uniform(loc=1.0, scale=4),    # 构建连续参数的分布\n",
    "                     svr__kernel=[\"linear\",\"rbf\"],                                   # 离散参数的集合\n",
    "                    svr__gamma=uniform(loc=0, scale=4))\n",
    "\n",
    "rs = RandomizedSearchCV(estimator=pipe_svr,\n",
    "                                                     param_distributions = distributions,\n",
    "                                                     scoring = 'r2',\n",
    "                                                      cv = 10)       # 10折交叉验证\n",
    "rs = rs.fit(X,y)\n",
    "print(\"随机搜索最优得分：\",rs.best_score_)\n",
    "print(\"随机搜索最优参数组合：\\n\",rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过我们不懈的努力，从收集数据集并选择合适的特征、选择度量模型性能的指标、选择具体的模型并进行训练以优化模型到评估模型的性能并调参，我们认识到了如何使用sklearn构建简单回归模型。在本章的最后，我们会给出一个具体的案例，整合回归的内容。下面我们来看看机器学习另外一类大问题：分类。与回归一样，分类问题在机器学习的地位非常重要，甚至有的地方用的比回归问题还要多，因此分类问题是十分重要的！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用sklearn构建完整的分类项目                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 收集数据集并选择合适的特征：在数据集上我们使用我们比较熟悉的IRIS鸢尾花数据集。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature = iris.feature_names\n",
    "data = pd.DataFrame(X,columns=feature)\n",
    "data['target'] = y\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个特征的相关解释：\n",
    "   - sepal length (cm)：花萼长度(厘米)\n",
    "   - sepal width (cm)：花萼宽度(厘米)\n",
    "   - petal length (cm)：花瓣长度(厘米)\n",
    "   - petal width (cm)：花瓣宽度(厘米)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 选择度量模型性能的指标：                                    \n",
    "度量分类模型的指标和回归的指标有很大的差异，首先是因为分类问题本身的因变量是离散变量，因此像定义回归的指标那样，单单衡量预测值和因变量的相似度可能行不通。其次，在分类任务中，我们对于每个类别犯错的代价不尽相同，例如：我们将癌症患者错误预测为无癌症和无癌症患者错误预测为癌症患者，在医院和个人的代价都是不同的，前者会使得患者无法得到及时的救治而耽搁了最佳治疗时间甚至付出生命的代价，而后者只需要在后续的治疗过程中继续取证就好了，因此我们很不希望出现前者，当我们发生了前者这样的错误的时候会认为建立的模型是很差的。为了解决这些问题，我们必须将各种情况分开讨论，然后给出评价指标。             \n",
    "   - 真阳性TP：预测值和真实值都为正例；                        \n",
    "   - 真阴性TN：预测值与真实值都为正例；                     \n",
    "   - 假阳性FP：预测值为正，实际值为负；\n",
    "   - 假阴性FN：预测值为负，实际值为正；                      \n",
    "   ![jupyter](./1.22.png)                                       \n",
    "分类模型的指标：                    \n",
    "   - 准确率：分类正确的样本数占总样本的比例，即：$ACC = \\frac{TP+TN}{FP+FN+TP+TN}$.                                \n",
    "   - 精度：预测为正且分类正确的样本占预测值为正的比例，即：$PRE = \\frac{TP}{TP+FP}$.                     \n",
    "   - 召回率：预测为正且分类正确的样本占类别为正的比例，即：$REC =  \\frac{TP}{TP+FN}$.                     \n",
    "   - F1值：综合衡量精度和召回率，即：$F1 = 2\\frac{PRE\\times REC}{PRE + REC}$.                                     \n",
    "   - ROC曲线：以假阳率为横轴，真阳率为纵轴画出来的曲线，曲线下方面积越大越好。                                                          \n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics                           \n",
    "![jupyter](./1.21.png)                          \n",
    "在本次小案例中，我们使用ROC曲线作为最终评价指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 选择具体的模型并进行训练                              \n",
    "   - **逻辑回归logistic regression：**                      \n",
    "   说到分类问题与回归问题的区别，在于回归问题与分类问题需要预测的因变量不一样。在回归问题中，因变量是连续性变量，我们需要预测$E(Y|X)$是一个连续的实数，但是在分类问题中，我们往往是通过已知X的信息预测Y的类别，往往是一个离散集合中的某个元素。如：是否患癌症，图片是猫还是狗等。一个很自然的想法是能否用线性回归去处理分类问题，答案是可以但不好！先来看看线性回归处理分类问题会出现什么弊端，我们仔细来看这个线性回归的例子，${default = \\beta_0 + \\beta_1 Balance + \\beta_2 Income}$，只要输入Balance 和 Income 以及default的数据就能用最小二乘法估计出${\\beta_0,\\beta_1}$,设定预测的default>0.5就是违约反之不违约，感觉很完美的样子，但事实真的是这样吗？假设我们需要用某个人的债务(Balance)和收入(Income)去预测是否会信用卡违约(default)：       \n",
    "      - 我们假设有一个穷人Lisa,他的Balance和Income都很小，那么有可能会导致default的值为负数，那么这个负数代表什么意义呢？显然是没有任何意义的。                \n",
    "      ![jupyter](./1.23.png)                            \n",
    "      - 当我们的分类变量是多类的时候，以0.5为界限划分分类就不可用了，那么我们应该怎么找到一个界限衡量多分类呢？                              \n",
    "   基于以上问题，现在大家是否还觉得线性回归模型作为一个分类模型是否足够优秀呢？其实，为了解决以上的问题（1）我们来想想能不能将线性回归的结果default转化为区间[0:1]上，让default转变成一个违约的概率呢？下面我们来解决这个问题吧。                              \n",
    "   在推导逻辑回归之前，我们先来认识下一组函数，这组函数具有神奇的作用，可以将是实数轴上的数转换为[0:1]区间上的概率。\n",
    "  首先，我们假设我们的线性回归模型为 ${Y=\\beta_0+\\beta_1 X}$，那么这个函数是如何将线性回归的结果转化为概率呢？这个函数就是logistic 函数，具体的形式为   ${p(X) = \\dfrac{e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}}}$，他的函数图像如下图：（左边是线性回归，右边是逻辑函数）                             \n",
    "  ![jupyter](./1.24.png)                                   \n",
    "  因此，我们假设逻辑回归模型为：$p(y = 1|x) = \\frac{1}{1+e^{-w^Tx}}$ .                              \n",
    "  下面我们来具体推导下逻辑回归模型：                          \n",
    "  假设数据Data$\\{(x_i,y_i) \\},\\;\\;i = 1,2,...,N,\\;\\;x_i \\in R^p,y_i \\in \\{0,1 \\}$，设$p_1 = p(y=1|x) = \\sigma(w^T) = \\frac{1}{1+e^{-w^Tx}}$。因为y只可能取0或者1，因此假设数据服从0-1分布，也叫伯努力分布，即：当y=1时，$p(y|x)=p_1$，当y=0时，$p(y|x)=1-p_1$，可以写成$p(y|x) = p_1^y(1-p_1)^{1-y}$，可以带入y=0和y=1进去验证，结果和前面的结论一模一样。                    \n",
    "  我们使用极大似然估计MLE，即：                         \n",
    "  $$\n",
    "  \\hat{w} = argmax_w\\;\\;log\\;P(Y|X) = argmax_x\\;\\;log\\;\\prod_{i=1}^N P(y_i|x_i) = argmax_w \\sum\\limits_{i=1}^{N} log\\;P(y_i|x_i)\\\\\n",
    "  \\;\\;\\; = argmax_w \\sum\\limits_{i=1}^{N}(y_ilog\\;p_1 + (1-y_i)log(1-p_1)) \\\\ \n",
    "  记：L(w) = \\sum\\limits_{i=1}^{N}(y_ilog\\;p_1 + (1-y_i)log(1-p_1))\\\\\n",
    " \\;\\;\\; \\frac{\\partial L}{\\partial w_k} = \\sum\\limits_{i=1}^{N} y_i\\frac{1}{p_1}\\frac{\\partial p_1}{\\partial z}\\frac{\\partial z}{\\partial w_k} + (1-y_i)\\frac{1}{1-p_1}(-\\frac{\\partial p_1}{\\partial z}\\frac{\\partial z}{\\partial w_k})\\\\\n",
    "  \\;\\;\\;=\\sum\\limits_{i=1}^{N}y_i\\frac{1}{\\sigma(z)}(\\sigma(z_i)-\\sigma(z_i)^2)x_i + (1-y_i)\\frac{1}{1-\\sigma(z_i)}[-(\\sigma(z_i)-\\sigma(z_i)^2)x_i]\\\\\n",
    "  \\;\\;\\; =\\sum\\limits_{i=1}^{N}[(y_i-y_i\\sigma(z_i))x_i + (1-y_i)(-\\sigma(z_i))x_i]\\\\\n",
    "  \\;\\;\\; = \\sum\\limits_{i=1}^{N}y_ix_i-\\sigma(z_i)x_i = \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i\n",
    "  $$                 \n",
    "  因此，$\\frac{\\partial L}{\\partial w_k} = \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i$，由于这里涉及的函数不像线性回归一样能简单求出解析解，因此我们使用迭代的优化算法：梯度下降法，即：                       \n",
    "  $w_k^{(t+1)}\\leftarrow w_k^{(t)} - \\eta \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i^{(k)},\\;\\;\\;其中，x_i^{(k)}为第i个样本第k个特征$                                 \n",
    "  如果想了解关于梯度下降法等无约束算法的具体细节，可以参照笔者写的另外两篇知乎博客：                          \n",
    "  最优化理论之无约束优化基本结构及其python应用：https://zhuanlan.zhihu.com/p/163405865                                                   \n",
    "  最优化理论之负梯度方法与Newton型方法：https://zhuanlan.zhihu.com/p/165914126                                              \n",
    "  对于问题(2),我们值得注意的是，逻辑回归在实际中不太用于多分类问题，因为实际效果不是很好，所以我们可以借助其他模型来解决这个问题，那让我们来解决这个遗留下来的问题吧。                                \n",
    " \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 基于概率的分类模型：                               \n",
    "   (1) 线性判别分析：                                              \n",
    "   线性判别分析是一个比较久远的算法，我将会从两个方向去描述这个算法，因为我觉得每位读者都有自己喜欢的那个理解的方向，分别是基于贝叶斯公式和降维分类的思想。                        \n",
    "      - 基于贝叶斯公式对线性判别分析的理解：                       \n",
    "   在讨论如何解决多分类问题之前，我们先来说说贝叶斯的那些事吧。在概率统计的领域里有一条神奇的公式叫贝叶斯定理，具体的形式是：${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$ ，我们 先不要被公式的符号吓到，我们先来看看符号具体代表什么意思。我们假设观测有${K}$类，${\\pi_k}$为随机选择的观测来自第${k}$类的 __先验概率__，也就是样本里面第${k}$类的样本个数除以总样本的个数：${\\pi_k = \\dfrac{n_k}{n}}$。再来 ${f_k(x) =P(X=x|Y=k)}$，表示第${k}$类观测的X的密度函数，说的直白一点就是在${Y=k}$的样本里${X=x}$的样本个数，即${f_k(x) = P(X=x|Y=k) = \\dfrac{n_{(X=x,Y=k)}}{n_{(Y=k)}}}$，最后，${\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}=P(X=x)=\\dfrac{n_{(X=x)}}{n}$，也就是样本中${X=x}$的概率。\n",
    "      在讨论贝叶斯定理后，我们回到分类问题，这个定理跟我们的分类问题有什么关联呢？没错，这个公式${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$给出了给定样本条件下，${Y=k}$这个类别下的概率，这给分类问题提供了一条思路，那就是计算这个${P(Y=k|X=x)}$，而且我们的逻辑回归就是这么干的，但是在${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$这个公式中，分母${{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)} = P(X=x)}$当样本给定的时候是一个与分类${k}$无关的常数,所以我们的问题可以简化为只需要计算分子${{\\pi}_kf_k(x)}$,进而比较哪个类别的概率最大就知道属于哪个类别了，因此我们的分类思路就出来啦，这个思路不同于逻辑回归，逻辑回归需要计算具体的${P(Y=k|X=x)}$概率值，而我们现在的思路是通过贝叶斯定理计算贝叶斯定理的分子，比较分子最大的那个类别为最终类别。                 \n",
    "      在我们推导复杂算法之前，我们先推导下简单的当自变量个数只有一个的模型，即${p=1}$的简单模型。我们记${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$ 的分子为${g_k(x) = {\\pi}_kf_k(x)}$。在这里，我们做个模型假设：假设${f_k(x) }$服从正态分布，即${f_k(x) \\sim N(\\mu,\\sigma_k^2)}$，而且每个${\\sigma_k^2 = \\sigma^2}$，同方差假设。因此${f_k(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\dfrac{1}{2\\sigma^2}(x-\\mu_k)^2}}$，最终我们的${g_k(x) = \\pi_k\\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\dfrac{1}{2\\sigma^2}(x-\\mu_k)^2}}$,终于算出来啦。这个式子不是很好计算，我们对${g_k(x)}$取个对数，令${\\delta_k(x) = ln(g_k(x))=ln\\pi_k+\\dfrac{\\mu}{\\sigma^2}x-\\dfrac{\\mu^2}{2\\sigma^2}}$，到这里我们的模型建立模型，我们只需要把位置的${\\mu_k}$与${\\sigma^2}$估计出来就好了。${\\hat{\\mu}_k =\\dfrac{1}{n_k}\\sum\\limits_{i:y_i=k}x_i}$，也就是当${y=k}$这一类中${x}$的平均值；${\\hat{\\sigma}^2 =\\dfrac{1}{n-K}\\sum\\limits_{k=1}^K\\sum\\limits_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 }$，说白了就是计算每一类的方差，再求平均值。总结下上面的公式就是：                                    \n",
    "${\\begin{cases}\\delta_k(x) = ln(g_k(x))=ln\\pi_k+\\dfrac{\\mu}{\\sigma^2}x-\\dfrac{\\mu^2}{2\\sigma^2}\\\\{\\hat{\\mu}_k =\\dfrac{1}{n_k}\\sum\\limits_{i:y_i=k}x_i}\\\\{\\hat{\\sigma}^2 =\\dfrac{1}{n-K}\\sum\\limits_{k=1}^K\\sum\\limits_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2}\\end{cases}}$                              \n",
    "      至此，我们的模型就建立完成了，我们只需要代入数据求出${\\delta_k(x)}$，哪个${k}$对应的${\\delta_k(x)}$大，就是哪一类。                                   \n",
    "   （下图虚线是线性判别分析的决策边界，正态曲线哪边高样本就是哪一类）                  \n",
    "      ![jupyter](./1.25.png)                            \n",
    "      我们推到出了一个自变量的简单模型，就要泛化为多个自变量的线性判别分析了，即${p>1}$。其实原理一样的，只是将一元正态分布扩展为多元正态分布：\n",
    "      ${f_k(x)=\\dfrac{1}{(2\\pi)^{\\tfrac{p}{2}}|\\Sigma|^\\tfrac{1}{2}}e^{[-\\tfrac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)]}}$                           \n",
    "      ${\\hat{\\mu_k}=(\\mu_{k1},\\mu_{k2},......,\\mu_{kp})   ,   \\hat{\\Sigma}=\\dfrac{1}{p-1}\\sum\\limits_{j=1}^p(x_j-\\overline{x})(x_j-\\overline{x})^T}$                               \n",
    "      ${\\delta_k(x) = ln(\\pi_kf_k(x))=ln(\\pi_k)-(\\dfrac{p}{2}ln(2\\pi)+\\dfrac{1}{2}ln(|\\Sigma|))-\\dfrac{1}{2}(x-\\mu_k)^T\\Sigma^-1(x-\\mu_k)=x^T\\hat{\\Sigma}\\hat{\\mu}_k-\\dfrac{1}                                                       {2}\\hat{\\mu}_k^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k+ln\\hat{\\pi}_k}$                            \n",
    "      - 降维分类的思想理解线性判别分析：                   \n",
    "      基于数据进行分类时，一个很自然的想法是：将高维的数据降维至一维，然后使用某个阈值将各个类别分开。下面用图的形式展示：                   \n",
    "      ![jupyter](./1.26.png)                        \n",
    "      图中，数据的维度是二维的，我们的想法是把数据降维至一维，然后用阈值就能分类。这个似乎是一个很好的想法，我们总是希望降维后的数据同一个类别自身内部方差小，不同类别之间的方差要尽可能大。这也是合理的，因为同一个类别的数据应该更加相似，因此方差小；不同类别的数据之间应该很不相似，这样才能更容易对数据进行分类，我们简称为：**类内方差小，类间方差大**，在计算机语言叫“松耦合，高内聚”。在做具体的推导之前，我们对数据的形式和一些基本统计量做一些描述：                            \n",
    "      特征$X = (x_1,x_2,...,x_N)^T$，因变量$Y = (y_1,y_2,...,y_N)^T,\\;\\;其中，y_i \\in \\{+1,-1 \\}$，类别c1的特征$X_{c_1} = \\{x_i|y_i=+1 \\}$，同理，类别c2的特征$X_{c_2} = \\{x_i|y_i=-1 \\}$，属于c1类别的数据个数为$N_1$，属于类别c2的数据个数为$N_2$，其中，$N_1+N_2 = N$。                         \n",
    "      特征X投影在w方向至一维：$z_i = w^Tx_i,\\;\\;||w|| = 1$                            \n",
    "      全样本投影的均值$\\bar{z} = \\frac{1}{N}\\sum\\limits_{i=1}^{N}z_i = \\frac{1}{N}\\sum\\limits_{i=1}^{N}w^Tx_i$                    \n",
    "      全样本投影的协方差$S_z = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(z_i-\\bar{z})(z_i-\\bar{z})^T = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(w^Tx_i-\\bar{z})(w^Tx_i-\\bar{z})^T$                   \n",
    "      c1样本投影的均值$\\bar{z_1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}z_i = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}w^Tx_i$                    \n",
    "      c1样本投影的协方差$S_{z_1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(z_i-\\bar{z_1})(z_i-\\bar{z_1})^T = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(w^Tx_i-\\bar{z_1})(w^Tx_i-\\bar{z_1})^T$                       \n",
    "      c2样本投影的均值 $\\bar{z_2} = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}z_i = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}w^Tx_i$                     \n",
    "      c2样本投影的协方差$S_{z_2} = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(z_i-\\bar{z_2})(z_i-\\bar{z_2})^T = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(w^Tx_i-\\bar{z_2})(w^Tx_i-\\bar{z_2})^T$                      \n",
    "      类间差距：$(\\bar{z}_1-\\bar{z}_2)^2$                      \n",
    "      类内方差：$S_1 + S_2$                          \n",
    "      由于线性判别分析的目标是同一类别内方差小，不同类别之间距离大，因此损失函数定义为：   \n",
    "                            \n",
    "   $$\n",
    "      J(w) = \\frac{(\\bar{z}_1-\\bar{z}_2)^2}{s_1+s_2} = \\frac{w^T(\\bar{x}_{c_1}-\\bar{x}_{c_2})(\\bar{x}_{c_1}-\\bar{x}_{c_2})^Tw}{w^T(s_{c_1}+s_{c_2})w}\\\\\n",
    "      \\;\\;\\; \\hat{w} = argmax_w\\;J(w)\n",
    "   $$                             \n",
    "   记：$S_b = (\\bar{x}_{c_1}-\\bar{x}_{c_2})(\\bar{x}_{c_1}-\\bar{x}_{c_2})^T,\\;S_w = (s_{c_1}+s_{c_2})$，因此$J(w) = \\frac{w^TS_bw}{w^TS_ww}$                   \n",
    "   让J(w)对w求导等于0，求出：$w = S_w^{-1}(\\bar{x}_{c_1}-\\bar{x}_{c_2})$                       \n",
    "   (2) 朴素贝叶斯：                                        \n",
    "   在线性判别分析中，我们假设每种分类类别下的特征遵循同一个协方差矩阵，每两个特征之间是存在协方差的，因此在线性判别分析中各种特征是不是独立的。但是，朴素贝叶斯算法对线性判别分析作进一步的模型简化，它将线性判别分析中的协方差矩阵中的协方差全部变成0，只保留各自特征的方差，也就是朴素贝叶斯假设各个特征之间是不相关的。在之前所看到的偏差-方差理论中，我们知道模型的简化可以带来方差的减少但是增加偏差，因此朴素贝叶斯也不例外，它比线性判别分析模型的方差小，偏差大。虽然简化了模型，实际中使用朴素贝叶斯的案例非常多，甚至多于线性判别分析，例如鼎鼎大名的新闻分类，垃圾邮件分类等。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  逻辑回归\n",
    "'''\n",
    "penalty       {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’正则化方式\n",
    "dual      bool, default=False   是否使用对偶形式，当n_samples> n_features时，默认dual = False。   \n",
    "C        float, default=1.0      \n",
    "solver       {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’     \n",
    "l1_ratio         float, default=None           \n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_iris = LogisticRegression()\n",
    "log_iris.fit(X,y)\n",
    "log_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性判别分析\n",
    "'''\n",
    "参数：\n",
    "solver:{'svd'，'lsqr'，'eigen'}，默认='svd'\n",
    "solver的使用，可能的值：\n",
    "'svd'：奇异值分解（默认）。不计算协方差矩阵，因此建议将此求解器用于具有大量特征的数据。\n",
    "\n",
    "'lsqr'：最小二乘解，可以与收缩结合使用。\n",
    "\n",
    "'eigen'：特征值分解，可以与收缩结合使用。\n",
    "'''\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda_iris = LinearDiscriminantAnalysis()\n",
    "lda_iris.fit(X,y)\n",
    "lda_iris.score(X,y)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 朴素贝叶斯             \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_iris = GaussianNB()\n",
    "NB_iris.fit(X, y)\n",
    "NB_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 决策树 ：                     \n",
    "   与前面内容所讲的决策树回归大致是一样的，只是在回归问题中，选择分割点的标准是均方误差，但是在分类问题中，由于因变量是类别变量而不是连续变量，因此用均方误差显然不合适。那问题是用什么作为选择分割点的标准呢？我们先来分析具体的问题：                         \n",
    "   在回归树中，对一个给定的观测值，因变量的预测值取它所属的终端结点内训练集的平均因变量。与之相对应，对于分类树来说，给定一个观测值，因变量的预测值为它所属的终端结点内训练集的**最常出现的类**。分类树的构造过程与回归树也很类似，与回归树一样，分类树也是采用递归二叉分裂。但是在分类树中，均方误差无法作为确定分裂节点的准则，一个很自然的替代指标是分类错误率。分类错误率就是：此区域内的训练集中非常见类所占的类别，即：                                   \n",
    "   $$\n",
    "   E = 1-max_k(\\hat{p}_{mk})\n",
    "   $$                       \n",
    "   上式中的$\\hat{p}_{mk}$代表第m个区域的训练集中第k类所占的比例。但是在大量的事实证明：分类错误率在构建决策树时不够敏感，一般在实际中用如下两个指标代替：             \n",
    "   (1) 基尼系数：                   \n",
    "   $$\n",
    "   G = \\sum\\limits_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})\n",
    "   $$             \n",
    "   在基尼系数的定义中，我们发现这个指标衡量的是K个类别的总方差。不难发现，如果所有的$\\hat{p}_{mk}$的取值都接近0或者1，基尼系数会很小。因此基尼系数被视为衡量结点纯度的指标----如果他的取值小，那就意味着某个节点包含的观测值几乎来自同一个类别。                         \n",
    "   由基尼系数作为指标得到的分类树叫做：CART。                        \n",
    "   (2) 交叉熵：                       \n",
    "   可以替代基尼系数的指标是交叉熵，定义如下：                           \n",
    "   $$\n",
    "   D = -\\sum\\limits_{k=1}^{K} \\hat{p}_{mk}log\\;\\hat{p}_{mk}\n",
    "   $$                     \n",
    "   显然，如果所有的$\\hat{p}_{mk}$都接近于0或者1，那么交叉熵就会接近0。因此，和基尼系数一样，如果第m个结点的纯度越高，则交叉熵越小。事实证明，基尼系数和交叉熵在数值上时很接近的。                   \n",
    "   \n",
    "   ![jupyter](./1.27.png)                                            \n",
    "   决策树分类算法的完整步骤：                          \n",
    "      a.  选择最优切分特征j以及该特征上的最优点s：                \n",
    "      遍历特征j以及固定j后遍历切分点s，选择使得基尼系数或者交叉熵最小的(j,s)                                                   \n",
    "       b. 按照(j,s)分裂特征空间，每个区域内的类别为该区域内样本比例最多的类别。                           \n",
    "       c. 继续调用步骤1，2直到满足停止条件，就是每个区域的样本数小于等于5。        \n",
    "       d. 将特征空间划分为J个不同的区域，生成分类树。                 \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用决策树算法对iris分类：\n",
    "'''\n",
    "criterion:{“gini”, “entropy”}, default=”gini”\n",
    "max_depth:树的最大深度。\n",
    "min_samples_split:拆分内部节点所需的最少样本数\n",
    "min_samples_leaf :在叶节点处需要的最小样本数。\n",
    "\n",
    "'''\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_iris = DecisionTreeClassifier(min_samples_leaf=5)\n",
    "tree_iris.fit(X,y)\n",
    "tree_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 支持向量机SVM：                                   \n",
    "   支持向量机SVM是20世纪90年代在计算机界发展起来的一种分类算法，在许多问题中都被证明有较好的效果，被认为是适应性最广的算法之一。                         \n",
    "   ![jupyter](./1.28.png)                               \n",
    "   支持向量机的基本原理非常简单，如图所视，白色和蓝色的点各为一类，我们的目标是找到一个分割平面将两个类别分开。通常来说，如果数据本身是线性可分的，那么事实上存在无数个这样的超平面。这是因为给定一个分割平面稍微上移下移或旋转这个超平面，只要不接触这些观测点，仍然可以将数据分开。一个很自然的想法就是找到**最大间隔超平面**，即找到一个分割平面距离最近的观测点最远。下面我们来严格推导：                   \n",
    "   我们根据距离超平米那最近的点，只要同时缩放w和b可以得到：$w^Tx_1 + b = 1$与$w^Tx_2+b = -1$，因此：                      \n",
    "   $$\n",
    "  \\begin{array}{l}\n",
    "   w^{T} x_{1}+b=1 \\\\\n",
    "    w^{T} x_{2}+b=-1 \\\\\n",
    "    \\left(w^{T} x_{1}+b\\right)-\\left(w^{T} x_{2}+b\\right)=2 \\\\\n",
    "   w^{T}\\left(x_{1}-x_{2}\\right)=2 \\\\\n",
    "   \\qquad \\begin{array}{l}\n",
    "   w^{T}\\left(x_{1}-x_{2}\\right)=\\|w\\|_{2}\\left\\|x_{1}-x_{2}\\right\\|_{2} \\cos \\theta=2 \\\\\n",
    "   \\left\\|x_{1}-x_{2}\\right\\|_{2} \\cos \\theta=\\frac{2}{\\|w\\|_{2}}\n",
    "   \\end{array} \\\\\n",
    "    \\qquad \\begin{array}{l}\n",
    "   d_{1}=d_{2}=\\frac{\\left\\|x_{1}-x_{2}\\right\\|_{2} \\cos \\theta}{2}=\\frac{\\frac{2}{\\|w\\|_{2}}}{2}=\\frac{1}{\\|w\\|_{2}} \\\\\n",
    "   d_{1}+d_{2}=\\frac{2}{\\|w\\|_{2}}\n",
    "   \\end{array}\n",
    "   \\end{array}\n",
    "   $$                        \n",
    "   由此可知道SVM模型的具体形式：                           \n",
    "   $$\n",
    "  \\begin{aligned}\n",
    "\\min _{w, b} & \\frac{1}{2}\\|w\\|^{2} \\\\\n",
    "\\text { s.t. } & y^{(i)}\\left(w^{T} x^{(i)}+b\\right) \\geq 1, \\quad i=1, \\ldots, n\n",
    "\\end{aligned}\n",
    "   $$                     \n",
    "   可以将约束条件写为: $g_{i}(w)=-y^{(i)}\\left(w^{T} x^{(i)}+b\\right)+1 \\leq 0  $                                                     \n",
    "   可以将优化问题拉格朗日化\n",
    "   $$\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left[y^{(i)}\\left(w^{T} x^{(i)}+b\\right)-1\\right]\n",
    "   $$                        \n",
    "   因此：                   \n",
    "   $$\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left[y^{(i)}\\left(w^{T} x^{(i)}+b\\right)-1\\right]\n",
    "   $$                   \n",
    "   欲构造 dual 问题, 首先求拉格朗日化的问题中  $\\mathrm{w} $ 和  $\\mathrm{b} $ 的值, 对 $ \\mathrm{w}$  求梯度, 令梯度为  0,  可求得 w:              \n",
    "   对 b 求梯度, 令梯度为 0, 可得：              \n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial b} \\mathcal{L}(w, b, \\alpha)=\\sum_{i=1}^{n} \\alpha_{i} y^{(i)}=0\n",
    "   $$               \n",
    "\n",
    "   将  $\\mathrm{w}$  带入拉格朗日化的原问题可得                        \n",
    "   $$\n",
    "   \\begin{array}{l}\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} y^{(i)} y^{(j)} \\alpha_{i} \\alpha_{j}\\left(x^{(i)}\\right)^{T} x^{(j)}-b \\sum_{i=1}^{n} \\alpha_{i} y^{(i)} \\\\\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} y^{(i)} y^{(j)} \\alpha_{i} \\alpha_{j}\\left(x^{(i)}\\right)^{T} x^{(j)}\n",
    "   \\end{array}\n",
    "   $$                           \n",
    "   因此：                           \n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "    &\\text { 对拉格朗日化的原问题求最小值, 得到了 } \\mathrm{w} \\text { , 现在可以构造 dual 问題 }\\\\\n",
    "    &\\begin{aligned}\n",
    "    \\max _{\\alpha} & W(\\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} y^{(i)} y^{(j)} \\alpha_{i} \\alpha_{j}\\left\\langle x^{(i)}, x^{(j)}\\right\\rangle \\\\\n",
    "    \\text { s.t. } & \\alpha_{i} \\geq 0, \\quad i=1, \\ldots, n \\\\\n",
    "    & \\sum_{i=1}^{n} \\alpha_{i} y^{(i)}=0\n",
    "    \\end{aligned}\\\\\n",
    "    &\\text { 可以推导出 b的值为: } b^{*}=-\\frac{\\max _{i: y^{(i)}=-1} w^{* T} x^{(i)}+\\min _{i: y^{(i)}=1} w^{* T} x^{(i)}}{2}\\\\\n",
    "    &\\begin{array}{r}\n",
    "    \\text { SVM的决策子如下,值的符号为类别. } \\\\\n",
    "    \\qquad w^{T} x+b=\\left(\\sum_{i=1}^{n} \\alpha_{i} y^{(i)} x^{(i)}\\right)^{T} x+b=\\sum_{i=1}^{n} \\alpha_{i} y^{(i)}\\left\\langle x^{(i)}, x\\right\\rangle+b\n",
    "    \\end{array}\n",
    "    \\end{aligned}\n",
    "    $$                              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 非线性支持向量机：                     \n",
    "   在刚刚的讨论中，我们都是着重讨论了线性支持向量机是如何工作的，但是在现实生活中，我们很难碰到线性可分的数据集，如：             \n",
    "   ![jupyter](./1.29.png)                         \n",
    "   那我们应该如何处理非线性问题呢？答案就是将数据投影至更加高的维度！                    \n",
    "   ![jupyter](./1.30.png)                     \n",
    "   上图中，在一维数据做不到线性可分，我们将数据投影至二维平面就可以成功线性可分。那么，我们来详细探讨下这其中的奥妙：                        \n",
    "   \n",
    "   $$\n",
    "   \\begin{array}{l}\n",
    "   \\Phi: \\mathcal{X} \\mapsto \\hat{\\mathcal{X}}=\\Phi(\\mathbf{x}) \\\\\n",
    "   \\Phi\\left(\\left[x_{i 1}, x_{i 2}\\right]\\right)=\\left[x_{i 1}, x_{i 2}, x_{i 1} x_{i 2}, x_{i 1}^{2}, x_{i 2}^{2}\\right]\n",
    "   \\end{array}\n",
    "   $$                      \n",
    "   如果我们使用上面公式的形式将低维数据拓展至高维数据，则必须面临一个很大的问题，那就是：维度爆炸导致的计算量太大的问题。假如是一个2维特征的数据，我们可以将其映射到5维来做特征的内积，如果原始空间是三维，可以映射到到19维空间，似乎还可以处理。但是如果我们的低维特征是100个维度，1000个维度呢？那么我们要将其映射到超级高的维度来计算特征的内积。这时候映射成的高维维度是爆炸性增长的，这个计算量实在是太大了，而且如果遇到无穷维的情况，就根本无从计算了。能不能呢个避免这个问题呢？核函数隆重登场：                       \n",
    "   回顾线性可分SVM的优化目标函数：                  \n",
    "   $$\n",
    "   \\underbrace{ min }_{\\alpha}  \\frac{1}{2}\\sum\\limits_{i=1,j=1}^{m}\\alpha_i\\alpha_jy_iy_jx_i \\bullet x_j - \\sum\\limits_{i=1}^{m}\\alpha_i\\\\\n",
    "   s.t. \\; \\sum\\limits_{i=1}^{m}\\alpha_iy_i = 0\\\\\n",
    "   0 \\leq \\alpha_i \\leq C\n",
    "   $$                  \n",
    "   注意到上式低维特征仅仅以内积$x_i \\bullet x_j$ 的形式出现，如果我们定义一个低维特征空间到高维特征空间的映射$\\phi$，将所有特征映射到一个更高的维度，让数据线性可分，我们就可以继续按前两篇的方法来优化目标函数，求出分离超平面和分类决策函数了。也就是说现在的SVM的优化目标函数变成：                         \n",
    "   $$\n",
    "   \\begin{array}{c}\n",
    "   \\underbrace{\\min }_{\\alpha} \\frac{1}{2} \\sum_{i=1, j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\phi\\left(x_{i}\\right) \\bullet \\phi\\left(x_{j}\\right)-\\sum_{i=1}^{m} \\alpha_{i} \\\\\n",
    "   \\text { s. } t . \\sum_{i=1}^{m} \\alpha_{i} y_{i}=0 \\\\\n",
    "   0 \\leq \\alpha_{i} \\leq C\n",
    "   \\end{array}\n",
    "   $$                    \n",
    "   可以看到，和线性可分SVM的优化目标函数的区别仅仅是将内积$x_i \\bullet x_j$替换为$\\phi(x_i) \\bullet \\phi(x_j)$。我们要将其映射到超级高的维度来计算特征的内积。这时候映射成的高维维度是爆炸性增长的，这个计算量实在是太大了，而且如果遇到无穷维的情况，就根本无从计算了。下面引入核函数：               \n",
    "   假设$\\phi$是一个从低维的输入空间$\\chi$（欧式空间的子集或者离散集合）到高维的希尔伯特空间的$\\mathcal{H}$映射。那么如果存在函数$K(x,z)$，对于任意$x, z \\in \\chi$，都有：                    \n",
    "   $$\n",
    "   K(x, z) = \\phi(x) \\bullet \\phi(z)\n",
    "   $$                       \n",
    "   那么我们就称$K(x, z)$为核函数。                   \n",
    "   仔细发现，$K(x, z)$的计算是在低维特征空间来计算的，它避免了在刚才我们提到了在高维维度空间计算内积的恐怖计算量。也就是说，我们可以好好享受在高维特征空间线性可分的利益，却避免了高维特征空间恐怖的内积计算量。下面介绍几种常用的核函数：                     \n",
    "   (1)  多项式核函数：                   \n",
    "   多项式核函数（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：                         \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\left(\\left\\langle\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right\\rangle+c\\right)^{d}\n",
    "   $$             \n",
    "   C用来控制低阶项的强度，C=0,d=1代表无核函数。                       \n",
    "   (2) 高斯核函数：                                                   \n",
    "   高斯核函数（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是非线性分类SVM最主流的核函数。libsvm默认的核函数就是它。表达式为：                             \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|_{2}^{2}}{2 \\sigma^{2}}\\right)\n",
    "   $$                              \n",
    "   使用高斯核函数之前需要将特征标准化，因此这里衡量的是样本之间的相似度。                    \n",
    "   (3) Sigmoid核函数：                    \n",
    "   Sigmoid核函数（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：               \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\tanh \\left(\\alpha \\mathbf{x}_{i}^{\\top} \\mathbf{x}_{j}+c\\right)\n",
    "   $$                        \n",
    "   此时的SVM相当于没有隐藏层的简单神经网络。                     \n",
    "   (4) 余弦相似度核：                  \n",
    "   常用于衡量两段文字的余弦相似度，表达式为：                    \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\frac{\\mathbf{x}_{i}^{\\top} \\mathbf{x}_{j}}{\\left\\|\\mathbf{x}_{i}\\right\\|\\left\\|\\mathbf{x}_{j}\\right\\|}\n",
    "   $$\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "'''\n",
    "C:正则化参数。正则化的强度与C成反比。必须严格为正。惩罚是平方的l2惩罚。\n",
    "kernel:{'linear'，'poly'，'rbf'，'sigmoid'，'precomputed'}，默认='rbf'\n",
    "degree:多项式和的阶数\n",
    "gamma:“ rbf”，“ poly”和“ Sigmoid”的内核系数。\n",
    "shrinking:是否软间隔分类，默认true\n",
    "\n",
    "'''\n",
    "svc_iris = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_iris.fit(X, y)\n",
    "svc_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) 评估模型的性能并调参:                        \n",
    "更详细的可以查看笔者的知乎：https://zhuanlan.zhihu.com/p/140040705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网格搜索经历时间：4.300 S\n",
      "0.9800000000000001\n",
      "{'svc__C': 1.0, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索进行超参数调优：\n",
    "# 方式1：网格搜索GridSearchCV()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))\n",
    "param_range = [0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0]\n",
    "param_grid = [{'svc__C':param_range,'svc__kernel':['linear']},{'svc__C':param_range,'svc__gamma':param_range,'svc__kernel':['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1)\n",
    "gs = gs.fit(X,y)\n",
    "end_time = time.time()\n",
    "print(\"网格搜索经历时间：%.3f S\" % float(end_time-start_time))\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机网格搜索经历时间：0.942 S\n",
      "0.9733333333333334\n",
      "{'svc__kernel': 'linear', 'svc__C': 100.0}\n"
     ]
    }
   ],
   "source": [
    "# 方式2：随机网格搜索RandomizedSearchCV()\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))\n",
    "param_range = [0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0]\n",
    "param_grid = [{'svc__C':param_range,'svc__kernel':['linear']},{'svc__C':param_range,'svc__gamma':param_range,'svc__kernel':['rbf']}]\n",
    "# param_grid = [{'svc__C':param_range,'svc__kernel':['linear','rbf'],'svc__gamma':param_range}]\n",
    "gs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_grid,scoring='accuracy',cv=10,n_jobs=-1)\n",
    "gs = gs.fit(X,y)\n",
    "end_time = time.time()\n",
    "print(\"随机网格搜索经历时间：%.3f S\" % float(end_time-start_time))\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当类别为两类时，可以绘制混淆矩阵与ROC曲线**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 混淆矩阵：\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\",header=None)\n",
    "'''\n",
    "乳腺癌数据集：569个恶性和良性肿瘤细胞的样本，M为恶性，B为良性\n",
    "'''\n",
    "# 做基本的数据预处理\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df.iloc[:,2:].values\n",
    "y = df.iloc[:,1].values\n",
    "le = LabelEncoder()    #将M-B等字符串编码成计算机能识别的0-1\n",
    "y = le.fit_transform(y)\n",
    "le.transform(['M','B'])\n",
    "# 数据切分8：2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)\n",
    "from sklearn.svm import SVC\n",
    "pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe_svc.fit(X_train,y_train)\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test,y_pred=y_pred)\n",
    "fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues,alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j,y=i,s=confmat[i,j],va='center',ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制ROC曲线：\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "scorer = make_scorer(f1_score,pos_label=0)\n",
    "gs = GridSearchCV(estimator=pipe_svc,param_grid=param_grid,scoring=scorer,cv=10)\n",
    "y_pred = gs.fit(X_train,y_train).decision_function(X_test)\n",
    "#y_pred = gs.predict(X_test)\n",
    "fpr,tpr,threshold = roc_curve(y_test, y_pred) ###计算真阳率和假阳率\n",
    "roc_auc = auc(fpr,tpr) ###计算auc的值\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) ###假阳率为横坐标，真阳率为纵坐标做曲线\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.结语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章中，我们重点讨论了各种回归和分类算法的具体推导与简单应用，并且给出了如何使用sklearn这个强大的python工具库进行简单的机器学习模型的建模代码。本章的重点是各个基础算法的掌握，包括回归和分类(重点是分类)算法以及怎么用网格搜索以及其他搜索方式进行调参。简单模型在进行复杂项目的时候往往显得力不从心，那么在下一章中，我们将开始本次开源项目的主题----集成学习，我们着重讨论如何将本章所学的基础模型进行集成，变成功能更加强大的集成模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了巩固本章的理解，在这里给个小任务，大家结合sklearn的fetch_lfw_people数据集，进行一次实战。fetch_lfw_people数据集是一个图像数据集，详细内容可以参照：                   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html                   \n",
    "案例的内容是对图像进行识别并分类。               \n",
    "参考资料：                \n",
    "https://blog.csdn.net/cwlseu/article/details/52356665                      \n",
    "https://blog.csdn.net/jasonzhoujx/article/details/81905923"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
