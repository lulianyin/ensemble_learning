{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习案例一 （幸福感预测）\n",
    "\n",
    "### 背景介绍\n",
    "\n",
    "幸福感是一个古老而深刻的话题，是人类世代追求的方向。与幸福感相关的因素成千上万、因人而异，大如国计民生，小如路边烤红薯，都会对幸福感产生影响。这些错综复杂的因素中，我们能找到其中的共性，一窥幸福感的要义吗？\n",
    "\n",
    "另外，在社会科学领域，幸福感的研究占有重要的位置。这个涉及了哲学、心理学、社会学、经济学等多方学科的话题复杂而有趣；同时与大家生活息息相关，每个人对幸福感都有自己的衡量标准。如果能发现影响幸福感的共性，生活中是不是将多一些乐趣；如果能找到影响幸福感的政策因素，便能优化资源配置来提升国民的幸福感。目前社会科学研究注重变量的可解释性和未来政策的落地，主要采用了线性回归和逻辑回归的方法，在收入、健康、职业、社交关系、休闲方式等经济人口因素；以及政府公共服务、宏观经济环境、税负等宏观因素上有了一系列的推测和发现。\n",
    "\n",
    "该案例为幸福感预测这一经典课题，希望在现有社会科学研究外有其他维度的算法尝试，结合多学科各自优势，挖掘潜在的影响因素，发现更多可解释、可理解的相关关系。\n",
    "\n",
    "具体来说，该案例就是一个数据挖掘类型的比赛——幸福感预测的baseline。具体来说，我们需要使用包括个体变量（性别、年龄、地域、职业、健康、婚姻与政治面貌等等）、家庭变量（父母、配偶、子女、家庭资本等等）、社会态度（公平、信用、公共服务等等）等139维度的信息来预测其对幸福感的影响。\n",
    "\n",
    "我们的数据来源于国家官方的《中国综合社会调查（CGSS）》文件中的调查结果中的数据，数据来源可靠可依赖:)\n",
    "\n",
    "### 数据信息\n",
    "赛题要求使用以上 **139** 维的特征，使用 **8000** 余组数据进行对于个人幸福感的预测（预测值为1，2，3，4，5，其中1代表幸福感最低，5代表幸福感最高）。\n",
    "因为考虑到变量个数较多，部分变量间关系复杂，数据分为完整版和精简版两类。可从精简版入手熟悉赛题后，使用完整版挖掘更多信息。在这里我直接使用了完整版的数据。赛题也给出了index文件中包含每个变量对应的问卷题目，以及变量取值的含义；survey文件中为原版问卷，作为补充以方便理解问题背景。\n",
    "\n",
    "### 评价指标\n",
    "最终的评价指标为均方误差MSE，即：\n",
    "$$Score = \\frac{1}{n} \\sum_1 ^n (y_i - y ^*)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error,mean_absolute_error, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import ExtraTreesRegressor as etr\n",
    "from sklearn.linear_model import BayesianRidge as br\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.kernel_ridge import KernelRidge as kr\n",
    "from sklearn.model_selection import  KFold, StratifiedKFold,GroupKFold, RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') #消除warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", parse_dates=['survey_time'],encoding='latin-1') \n",
    "test = pd.read_csv(\"test.csv\", parse_dates=['survey_time'],encoding='latin-1') #latin-1向下兼容ASCII\n",
    "train = train[train[\"happiness\"]!=-8].reset_index(drop=True)\n",
    "train_data_copy = train.copy() #删去\"happiness\" 为-8的行\n",
    "target_col = \"happiness\" #目标列\n",
    "target = train_data_copy[target_col]\n",
    "del train_data_copy[target_col] #去除目标列\n",
    "\n",
    "data = pd.concat([train_data_copy,test],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7988.000000\n",
       "mean        3.867927\n",
       "std         0.818717\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         4.000000\n",
       "75%         4.000000\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.happiness.describe() #数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4001.603906</td>\n",
       "      <td>2.309864e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2001.75</td>\n",
       "      <td>4003.5</td>\n",
       "      <td>6002.25</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happiness</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.867927</td>\n",
       "      <td>8.187167e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey_type</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.405984</td>\n",
       "      <td>4.911122e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>province</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>15.161617</td>\n",
       "      <td>8.915476e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>42.583250</td>\n",
       "      <td>2.718434e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>42.0</td>\n",
       "      <td>65.00</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>70.646470</td>\n",
       "      <td>3.873718e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.00</td>\n",
       "      <td>73.0</td>\n",
       "      <td>104.00</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.530295</td>\n",
       "      <td>4.991126e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birth</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1964.702303</td>\n",
       "      <td>1.684841e+01</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>1952.00</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1977.00</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nationality</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.374061</td>\n",
       "      <td>1.529899e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.776915</td>\n",
       "      <td>1.053975e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion_freq</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.427016</td>\n",
       "      <td>1.408912e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edu</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.881698</td>\n",
       "      <td>3.149408e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edu_status</th>\n",
       "      <td>6868.0</td>\n",
       "      <td>3.508736</td>\n",
       "      <td>1.098636e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edu_yr</th>\n",
       "      <td>6018.0</td>\n",
       "      <td>1576.040047</td>\n",
       "      <td>8.065383e+02</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1960.00</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>1999.00</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>33343.753005</td>\n",
       "      <td>2.331573e+05</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1780.00</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>36000.00</td>\n",
       "      <td>9999990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.319980</td>\n",
       "      <td>1.132188e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join_party</th>\n",
       "      <td>823.0</td>\n",
       "      <td>1784.809235</td>\n",
       "      <td>6.030403e+02</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1971.00</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>2002.00</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floor_area</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>115.356113</td>\n",
       "      <td>8.731841e+01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.00</td>\n",
       "      <td>96.0</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_0</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>8.634687e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_1</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.472834</td>\n",
       "      <td>4.992927e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_2</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>4.433521e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_3</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.101152</td>\n",
       "      <td>3.015484e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_4</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.102779</td>\n",
       "      <td>3.036892e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_5</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.023660</td>\n",
       "      <td>1.519986e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_6</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>6.317009e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_7</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>1.484210e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_8</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.136079</td>\n",
       "      <td>3.428940e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height_cm</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>163.984977</td>\n",
       "      <td>8.085915e+00</td>\n",
       "      <td>114.0</td>\n",
       "      <td>158.00</td>\n",
       "      <td>164.0</td>\n",
       "      <td>170.00</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_jin</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>121.575238</td>\n",
       "      <td>2.318954e+01</td>\n",
       "      <td>40.0</td>\n",
       "      <td>105.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>135.25</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.601277</td>\n",
       "      <td>1.093424e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health_problem</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.788558</td>\n",
       "      <td>1.360236e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depression</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.822609</td>\n",
       "      <td>1.045936e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hukou</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.893090</td>\n",
       "      <td>1.348626e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hukou_loc</th>\n",
       "      <td>7984.0</td>\n",
       "      <td>1.376378</td>\n",
       "      <td>6.767358e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_1</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.893215</td>\n",
       "      <td>1.130183e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_2</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.712569</td>\n",
       "      <td>9.599504e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_3</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.799199</td>\n",
       "      <td>1.097473e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_4</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.917626</td>\n",
       "      <td>1.061865e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_5</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.369179</td>\n",
       "      <td>1.698742e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_6</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.607787</td>\n",
       "      <td>1.179634e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_1</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.626815</td>\n",
       "      <td>1.062616e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_2</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.501127</td>\n",
       "      <td>9.613144e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_3</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.454807</td>\n",
       "      <td>1.225776e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_4</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.765273</td>\n",
       "      <td>1.460334e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_5</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.405483</td>\n",
       "      <td>1.411007e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_6</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.759264</td>\n",
       "      <td>9.189016e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_7</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.557086</td>\n",
       "      <td>1.230175e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_8</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.594892</td>\n",
       "      <td>1.566244e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_9</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.524412</td>\n",
       "      <td>1.615401e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_10</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.716575</td>\n",
       "      <td>9.745944e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_11</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.485228</td>\n",
       "      <td>1.247974e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure_12</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.431022</td>\n",
       "      <td>1.915028e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socialize</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.793440</td>\n",
       "      <td>1.069816e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relax</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.294191</td>\n",
       "      <td>1.062033e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learn</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.928643</td>\n",
       "      <td>1.182979e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_neighbor</th>\n",
       "      <td>7193.0</td>\n",
       "      <td>3.477548</td>\n",
       "      <td>2.052990e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_friend</th>\n",
       "      <td>7193.0</td>\n",
       "      <td>3.625886</td>\n",
       "      <td>1.983464e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socia_outing</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.830871</td>\n",
       "      <td>1.564942e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equity</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.139960</td>\n",
       "      <td>1.267597e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.200551</td>\n",
       "      <td>2.018157e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_10_before</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.446920</td>\n",
       "      <td>2.161517e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_10_after</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.453806</td>\n",
       "      <td>3.607616e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_14</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.931522</td>\n",
       "      <td>2.299954e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_exper</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.979594</td>\n",
       "      <td>1.752412e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_status</th>\n",
       "      <td>2946.0</td>\n",
       "      <td>3.163272</td>\n",
       "      <td>1.736697e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_yr</th>\n",
       "      <td>2946.0</td>\n",
       "      <td>14.503394</td>\n",
       "      <td>1.145812e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_type</th>\n",
       "      <td>2946.0</td>\n",
       "      <td>0.912424</td>\n",
       "      <td>1.386254e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_manage</th>\n",
       "      <td>2946.0</td>\n",
       "      <td>2.658859</td>\n",
       "      <td>1.694709e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insur_1</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.062969</td>\n",
       "      <td>5.072810e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insur_2</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.233475</td>\n",
       "      <td>8.094819e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insur_3</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.823986</td>\n",
       "      <td>7.925189e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insur_4</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.838633</td>\n",
       "      <td>8.421860e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_income</th>\n",
       "      <td>7987.0</td>\n",
       "      <td>67823.748591</td>\n",
       "      <td>2.911651e+05</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>13000.00</td>\n",
       "      <td>38208.0</td>\n",
       "      <td>70000.00</td>\n",
       "      <td>9999992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_m</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.882699</td>\n",
       "      <td>1.520820e+00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_status</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.601152</td>\n",
       "      <td>1.051287e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.065223</td>\n",
       "      <td>7.860543e-01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.819479</td>\n",
       "      <td>4.879529e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_0</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.014397</td>\n",
       "      <td>1.191264e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_1</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.909990</td>\n",
       "      <td>2.862140e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_2</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.061467</td>\n",
       "      <td>2.402003e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_3</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.020781</td>\n",
       "      <td>1.426600e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_4</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>6.881205e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_5</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>4.329579e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_6</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_7</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>2.739811e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest_8</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>2.739811e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>son</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.925263</td>\n",
       "      <td>9.461779e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.768528</td>\n",
       "      <td>9.761785e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minor_child</th>\n",
       "      <td>6923.0</td>\n",
       "      <td>0.458616</td>\n",
       "      <td>8.119034e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.234477</td>\n",
       "      <td>1.423383e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_1st</th>\n",
       "      <td>7161.0</td>\n",
       "      <td>1826.893870</td>\n",
       "      <td>5.396614e+02</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1970.00</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1997.00</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_birth</th>\n",
       "      <td>6272.0</td>\n",
       "      <td>1963.955517</td>\n",
       "      <td>1.451021e+01</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>1953.00</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1975.00</td>\n",
       "      <td>2004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_now</th>\n",
       "      <td>6220.0</td>\n",
       "      <td>1869.426367</td>\n",
       "      <td>4.716124e+02</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1975.00</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>1999.00</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_edu</th>\n",
       "      <td>6272.0</td>\n",
       "      <td>4.621014</td>\n",
       "      <td>3.063256e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_political</th>\n",
       "      <td>6272.0</td>\n",
       "      <td>1.322226</td>\n",
       "      <td>1.039162e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_hukou</th>\n",
       "      <td>6272.0</td>\n",
       "      <td>1.845185</td>\n",
       "      <td>1.393808e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_income</th>\n",
       "      <td>6272.0</td>\n",
       "      <td>28378.836894</td>\n",
       "      <td>1.694809e+05</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>30000.00</td>\n",
       "      <td>8999999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_work_exper</th>\n",
       "      <td>6272.0</td>\n",
       "      <td>2.838967</td>\n",
       "      <td>1.732527e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_work_status</th>\n",
       "      <td>2560.0</td>\n",
       "      <td>3.184766</td>\n",
       "      <td>1.742915e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_work_type</th>\n",
       "      <td>2560.0</td>\n",
       "      <td>0.967187</td>\n",
       "      <td>1.201348e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_birth</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1109.601277</td>\n",
       "      <td>9.622638e+02</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_edu</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.047196</td>\n",
       "      <td>3.715351e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_political</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.081497</td>\n",
       "      <td>1.729046e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_work_14</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.747997</td>\n",
       "      <td>4.209187e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_birth</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1151.908237</td>\n",
       "      <td>9.563642e+02</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>1949.00</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_edu</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.556084</td>\n",
       "      <td>3.072470e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_political</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.901102</td>\n",
       "      <td>1.310830e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_work_14</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.028918</td>\n",
       "      <td>5.512865e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status_peer</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.229469</td>\n",
       "      <td>9.515878e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status_3_before</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.705809</td>\n",
       "      <td>9.584227e-01</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>view</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.310466</td>\n",
       "      <td>1.962336e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inc_ability</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.100275</td>\n",
       "      <td>3.403146e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inc_exp</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>234020.321607</td>\n",
       "      <td>2.978092e+06</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>80000.00</td>\n",
       "      <td>100000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_1</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.787682</td>\n",
       "      <td>1.473862e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_2</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.177391</td>\n",
       "      <td>2.117071e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_3</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.143090</td>\n",
       "      <td>3.976541e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_4</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.965073</td>\n",
       "      <td>3.935903e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_5</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>4.178142</td>\n",
       "      <td>1.205529e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_6</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.022909</td>\n",
       "      <td>4.183873e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_7</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.626189</td>\n",
       "      <td>2.026981e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_8</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.231347</td>\n",
       "      <td>3.972178e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_9</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>2.507511</td>\n",
       "      <td>2.774541e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_10</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>0.670255</td>\n",
       "      <td>4.734512e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_11</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>-2.444792</td>\n",
       "      <td>5.566703e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_12</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>-0.176515</td>\n",
       "      <td>5.251543e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust_13</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>1.762394</td>\n",
       "      <td>1.672026e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbor_familiarity</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>3.723335</td>\n",
       "      <td>1.142888e+00</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_1</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>70.840260</td>\n",
       "      <td>2.115602e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>79.5</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_2</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>68.214572</td>\n",
       "      <td>2.050004e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_3</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>62.787932</td>\n",
       "      <td>2.472359e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_4</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>66.346895</td>\n",
       "      <td>2.202885e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_5</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>62.821670</td>\n",
       "      <td>2.344003e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>55.75</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_6</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>67.090761</td>\n",
       "      <td>2.156666e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_7</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>66.120931</td>\n",
       "      <td>2.306532e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_8</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>65.651352</td>\n",
       "      <td>2.380348e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_service_9</th>\n",
       "      <td>7988.0</td>\n",
       "      <td>67.188408</td>\n",
       "      <td>2.247259e+01</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count           mean           std     min       25%  \\\n",
       "id                    7988.0    4001.603906  2.309864e+03     1.0   2001.75   \n",
       "happiness             7988.0       3.867927  8.187167e-01     1.0      4.00   \n",
       "survey_type           7988.0       1.405984  4.911122e-01     1.0      1.00   \n",
       "province              7988.0      15.161617  8.915476e+00     1.0      7.00   \n",
       "city                  7988.0      42.583250  2.718434e+01     1.0     18.00   \n",
       "county                7988.0      70.646470  3.873718e+01     1.0     37.00   \n",
       "gender                7988.0       1.530295  4.991126e-01     1.0      1.00   \n",
       "birth                 7988.0    1964.702303  1.684841e+01  1921.0   1952.00   \n",
       "nationality           7988.0       1.374061  1.529899e+00    -8.0      1.00   \n",
       "religion              7988.0       0.776915  1.053975e+00    -8.0      1.00   \n",
       "religion_freq         7988.0       1.427016  1.408912e+00    -8.0      1.00   \n",
       "edu                   7988.0       4.881698  3.149408e+00    -8.0      3.00   \n",
       "edu_status            6868.0       3.508736  1.098636e+00    -8.0      4.00   \n",
       "edu_yr                6018.0    1576.040047  8.065383e+02    -3.0   1960.00   \n",
       "income                7988.0   33343.753005  2.331573e+05    -3.0   1780.00   \n",
       "political             7988.0       1.319980  1.132188e+00    -8.0      1.00   \n",
       "join_party             823.0    1784.809235  6.030403e+02    -3.0   1971.00   \n",
       "floor_area            7988.0     115.356113  8.731841e+01     3.0     64.00   \n",
       "property_0            7988.0       0.007511  8.634687e-02     0.0      0.00   \n",
       "property_1            7988.0       0.472834  4.992927e-01     0.0      0.00   \n",
       "property_2            7988.0       0.268778  4.433521e-01     0.0      0.00   \n",
       "property_3            7988.0       0.101152  3.015484e-01     0.0      0.00   \n",
       "property_4            7988.0       0.102779  3.036892e-01     0.0      0.00   \n",
       "property_5            7988.0       0.023660  1.519986e-01     0.0      0.00   \n",
       "property_6            7988.0       0.004006  6.317009e-02     0.0      0.00   \n",
       "property_7            7988.0       0.022534  1.484210e-01     0.0      0.00   \n",
       "property_8            7988.0       0.136079  3.428940e-01     0.0      0.00   \n",
       "height_cm             7988.0     163.984977  8.085915e+00   114.0    158.00   \n",
       "weight_jin            7988.0     121.575238  2.318954e+01    40.0    105.00   \n",
       "health                7988.0       3.601277  1.093424e+00    -8.0      3.00   \n",
       "health_problem        7988.0       3.788558  1.360236e+00    -8.0      3.00   \n",
       "depression            7988.0       3.822609  1.045936e+00    -8.0      3.00   \n",
       "hukou                 7988.0       1.893090  1.348626e+00     1.0      1.00   \n",
       "hukou_loc             7984.0       1.376378  6.767358e-01     1.0      1.00   \n",
       "media_1               7988.0       1.893215  1.130183e+00    -8.0      1.00   \n",
       "media_2               7988.0       1.712569  9.599504e-01    -8.0      1.00   \n",
       "media_3               7988.0       1.799199  1.097473e+00    -8.0      1.00   \n",
       "media_4               7988.0       3.917626  1.061865e+00    -8.0      3.00   \n",
       "media_5               7988.0       2.369179  1.698742e+00    -8.0      1.00   \n",
       "media_6               7988.0       1.607787  1.179634e+00    -8.0      1.00   \n",
       "leisure_1             7988.0       1.626815  1.062616e+00    -8.0      1.00   \n",
       "leisure_2             7988.0       4.501127  9.613144e-01    -8.0      4.00   \n",
       "leisure_3             7988.0       3.454807  1.225776e+00    -8.0      3.00   \n",
       "leisure_4             7988.0       3.765273  1.460334e+00    -8.0      3.00   \n",
       "leisure_5             7988.0       4.405483  1.411007e+00    -8.0      4.00   \n",
       "leisure_6             7988.0       3.759264  9.189016e-01    -8.0      3.00   \n",
       "leisure_7             7988.0       3.557086  1.230175e+00    -8.0      3.00   \n",
       "leisure_8             7988.0       3.594892  1.566244e+00    -8.0      2.00   \n",
       "leisure_9             7988.0       3.524412  1.615401e+00    -8.0      2.00   \n",
       "leisure_10            7988.0       4.716575  9.745944e-01    -8.0      5.00   \n",
       "leisure_11            7988.0       4.485228  1.247974e+00    -8.0      4.00   \n",
       "leisure_12            7988.0       3.431022  1.915028e+00    -8.0      1.00   \n",
       "socialize             7988.0       2.793440  1.069816e+00    -8.0      2.00   \n",
       "relax                 7988.0       3.294191  1.062033e+00    -8.0      3.00   \n",
       "learn                 7988.0       1.928643  1.182979e+00    -8.0      1.00   \n",
       "social_neighbor       7193.0       3.477548  2.052990e+00    -8.0      2.00   \n",
       "social_friend         7193.0       3.625886  1.983464e+00    -8.0      2.00   \n",
       "socia_outing          7988.0       1.830871  1.564942e+00    -8.0      1.00   \n",
       "equity                7988.0       3.139960  1.267597e+00    -8.0      2.00   \n",
       "class                 7988.0       4.200551  2.018157e+00    -8.0      3.00   \n",
       "class_10_before       7988.0       3.446920  2.161517e+00    -8.0      2.00   \n",
       "class_10_after        7988.0       4.453806  3.607616e+00    -8.0      4.00   \n",
       "class_14              7988.0       2.931522  2.299954e+00    -8.0      2.00   \n",
       "work_exper            7988.0       2.979594  1.752412e+00     1.0      1.00   \n",
       "work_status           2946.0       3.163272  1.736697e+00    -8.0      3.00   \n",
       "work_yr               2946.0      14.503394  1.145812e+01    -3.0      5.00   \n",
       "work_type             2946.0       0.912424  1.386254e+00    -8.0      1.00   \n",
       "work_manage           2946.0       2.658859  1.694709e+00    -8.0      2.00   \n",
       "insur_1               7988.0       1.062969  5.072810e-01    -8.0      1.00   \n",
       "insur_2               7988.0       1.233475  8.094819e-01    -8.0      1.00   \n",
       "insur_3               7988.0       1.823986  7.925189e-01    -8.0      2.00   \n",
       "insur_4               7988.0       1.838633  8.421860e-01    -8.0      2.00   \n",
       "family_income         7987.0   67823.748591  2.911651e+05    -3.0  13000.00   \n",
       "family_m              7988.0       2.882699  1.520820e+00    -3.0      2.00   \n",
       "family_status         7988.0       2.601152  1.051287e+00    -8.0      2.00   \n",
       "house                 7988.0       1.065223  7.860543e-01    -3.0      1.00   \n",
       "car                   7988.0       1.819479  4.879529e-01    -8.0      2.00   \n",
       "invest_0              7988.0       0.014397  1.191264e-01     0.0      0.00   \n",
       "invest_1              7988.0       0.909990  2.862140e-01     0.0      1.00   \n",
       "invest_2              7988.0       0.061467  2.402003e-01     0.0      0.00   \n",
       "invest_3              7988.0       0.020781  1.426600e-01     0.0      0.00   \n",
       "invest_4              7988.0       0.004757  6.881205e-02     0.0      0.00   \n",
       "invest_5              7988.0       0.001878  4.329579e-02     0.0      0.00   \n",
       "invest_6              7988.0       0.000000  0.000000e+00     0.0      0.00   \n",
       "invest_7              7988.0       0.000751  2.739811e-02     0.0      0.00   \n",
       "invest_8              7988.0       0.000751  2.739811e-02     0.0      0.00   \n",
       "son                   7988.0       0.925263  9.461779e-01    -8.0      0.00   \n",
       "daughter              7988.0       0.768528  9.761785e-01    -8.0      0.00   \n",
       "minor_child           6923.0       0.458616  8.119034e-01    -8.0      0.00   \n",
       "marital               7988.0       3.234477  1.423383e+00     1.0      3.00   \n",
       "marital_1st           7161.0    1826.893870  5.396614e+02    -3.0   1970.00   \n",
       "s_birth               6272.0    1963.955517  1.451021e+01  1907.0   1953.00   \n",
       "marital_now           6220.0    1869.426367  4.716124e+02    -3.0   1975.00   \n",
       "s_edu                 6272.0       4.621014  3.063256e+00    -8.0      3.00   \n",
       "s_political           6272.0       1.322226  1.039162e+00    -8.0      1.00   \n",
       "s_hukou               6272.0       1.845185  1.393808e+00    -8.0      1.00   \n",
       "s_income              6272.0   28378.836894  1.694809e+05    -3.0      0.00   \n",
       "s_work_exper          6272.0       2.838967  1.732527e+00     1.0      1.00   \n",
       "s_work_status         2560.0       3.184766  1.742915e+00    -8.0      3.00   \n",
       "s_work_type           2560.0       0.967187  1.201348e+00    -8.0      1.00   \n",
       "f_birth               7988.0    1109.601277  9.622638e+02    -3.0     -2.00   \n",
       "f_edu                 7988.0       2.047196  3.715351e+00    -8.0      1.00   \n",
       "f_political           7988.0       1.081497  1.729046e+00    -8.0      1.00   \n",
       "f_work_14             7988.0       2.747997  4.209187e+00    -8.0      1.00   \n",
       "m_birth               7988.0    1151.908237  9.563642e+02    -3.0     -2.00   \n",
       "m_edu                 7988.0       1.556084  3.072470e+00    -8.0      1.00   \n",
       "m_political           7988.0       0.901102  1.310830e+00    -8.0      1.00   \n",
       "m_work_14             7988.0       4.028918  5.512865e+00    -8.0      2.00   \n",
       "status_peer           7988.0       2.229469  9.515878e-01    -8.0      2.00   \n",
       "status_3_before       7988.0       1.705809  9.584227e-01    -8.0      1.00   \n",
       "view                  7988.0       3.310466  1.962336e+00    -8.0      3.00   \n",
       "inc_ability           7988.0       1.100275  3.403146e+00    -8.0      2.00   \n",
       "inc_exp               7988.0  234020.321607  2.978092e+06    -3.0  10000.00   \n",
       "trust_1               7988.0       3.787682  1.473862e+00    -8.0      3.00   \n",
       "trust_2               7988.0       3.177391  2.117071e+00    -8.0      3.00   \n",
       "trust_3               7988.0       2.143090  3.976541e+00    -8.0      3.00   \n",
       "trust_4               7988.0       1.965073  3.935903e+00    -8.0      2.00   \n",
       "trust_5               7988.0       4.178142  1.205529e+00    -8.0      4.00   \n",
       "trust_6               7988.0       2.022909  4.183873e+00    -8.0      3.00   \n",
       "trust_7               7988.0       2.626189  2.026981e+00    -8.0      2.00   \n",
       "trust_8               7988.0       2.231347  3.972178e+00    -8.0      3.00   \n",
       "trust_9               7988.0       2.507511  2.774541e+00    -8.0      2.00   \n",
       "trust_10              7988.0       0.670255  4.734512e+00    -8.0      1.00   \n",
       "trust_11              7988.0      -2.444792  5.566703e+00    -8.0     -8.00   \n",
       "trust_12              7988.0      -0.176515  5.251543e+00    -8.0     -8.00   \n",
       "trust_13              7988.0       1.762394  1.672026e+00    -8.0      1.00   \n",
       "neighbor_familiarity  7988.0       3.723335  1.142888e+00    -8.0      3.00   \n",
       "public_service_1      7988.0      70.840260  2.115602e+01    -3.0     60.00   \n",
       "public_service_2      7988.0      68.214572  2.050004e+01    -3.0     60.00   \n",
       "public_service_3      7988.0      62.787932  2.472359e+01    -3.0     50.00   \n",
       "public_service_4      7988.0      66.346895  2.202885e+01    -3.0     60.00   \n",
       "public_service_5      7988.0      62.821670  2.344003e+01    -3.0     55.75   \n",
       "public_service_6      7988.0      67.090761  2.156666e+01    -3.0     60.00   \n",
       "public_service_7      7988.0      66.120931  2.306532e+01    -3.0     60.00   \n",
       "public_service_8      7988.0      65.651352  2.380348e+01    -3.0     60.00   \n",
       "public_service_9      7988.0      67.188408  2.247259e+01    -3.0     60.00   \n",
       "\n",
       "                          50%       75%          max  \n",
       "id                     4003.5   6002.25       8000.0  \n",
       "happiness                 4.0      4.00          5.0  \n",
       "survey_type               1.0      2.00          2.0  \n",
       "province                 15.0     22.00         31.0  \n",
       "city                     42.0     65.00         89.0  \n",
       "county                   73.0    104.00        134.0  \n",
       "gender                    2.0      2.00          2.0  \n",
       "birth                  1965.0   1977.00       1997.0  \n",
       "nationality               1.0      1.00          8.0  \n",
       "religion                  1.0      1.00          1.0  \n",
       "religion_freq             1.0      1.00          9.0  \n",
       "edu                       4.0      6.00         14.0  \n",
       "edu_status                4.0      4.00          4.0  \n",
       "edu_yr                 1981.0   1999.00       2015.0  \n",
       "income                15000.0  36000.00    9999990.0  \n",
       "political                 1.0      1.00          4.0  \n",
       "join_party             1985.0   2002.00       2015.0  \n",
       "floor_area               96.0    130.00       1300.0  \n",
       "property_0                0.0      0.00          1.0  \n",
       "property_1                0.0      1.00          1.0  \n",
       "property_2                0.0      1.00          1.0  \n",
       "property_3                0.0      0.00          1.0  \n",
       "property_4                0.0      0.00          1.0  \n",
       "property_5                0.0      0.00          1.0  \n",
       "property_6                0.0      0.00          1.0  \n",
       "property_7                0.0      0.00          1.0  \n",
       "property_8                0.0      0.00          1.0  \n",
       "height_cm               164.0    170.00        191.0  \n",
       "weight_jin              120.0    135.25        260.0  \n",
       "health                    4.0      4.00          5.0  \n",
       "health_problem            4.0      5.00          5.0  \n",
       "depression                4.0      5.00          5.0  \n",
       "hukou                     1.0      2.00          8.0  \n",
       "hukou_loc                 1.0      2.00          4.0  \n",
       "media_1                   1.0      2.00          5.0  \n",
       "media_2                   1.0      2.00          5.0  \n",
       "media_3                   1.0      2.00          5.0  \n",
       "media_4                   4.0      5.00          5.0  \n",
       "media_5                   1.0      4.00          5.0  \n",
       "media_6                   1.0      2.00          5.0  \n",
       "leisure_1                 1.0      2.00          5.0  \n",
       "leisure_2                 5.0      5.00          5.0  \n",
       "leisure_3                 3.0      4.00          5.0  \n",
       "leisure_4                 4.0      5.00          5.0  \n",
       "leisure_5                 5.0      5.00          5.0  \n",
       "leisure_6                 4.0      4.00          5.0  \n",
       "leisure_7                 4.0      4.00          5.0  \n",
       "leisure_8                 4.0      5.00          5.0  \n",
       "leisure_9                 4.0      5.00          5.0  \n",
       "leisure_10                5.0      5.00          5.0  \n",
       "leisure_11                5.0      5.00          5.0  \n",
       "leisure_12                5.0      5.00          5.0  \n",
       "socialize                 3.0      4.00          5.0  \n",
       "relax                     3.0      4.00          5.0  \n",
       "learn                     2.0      3.00          5.0  \n",
       "social_neighbor           3.0      5.00          7.0  \n",
       "social_friend             3.0      5.00          7.0  \n",
       "socia_outing              1.0      2.00          6.0  \n",
       "equity                    3.0      4.00          5.0  \n",
       "class                     5.0      5.00         10.0  \n",
       "class_10_before           4.0      5.00         10.0  \n",
       "class_10_after            5.0      6.00         10.0  \n",
       "class_14                  3.0      4.00         10.0  \n",
       "work_exper                3.0      5.00          6.0  \n",
       "work_status               3.0      3.00          9.0  \n",
       "work_yr                  12.0     22.00         55.0  \n",
       "work_type                 1.0      1.00          2.0  \n",
       "work_manage               3.0      3.00          4.0  \n",
       "insur_1                   1.0      1.00          2.0  \n",
       "insur_2                   1.0      2.00          2.0  \n",
       "insur_3                   2.0      2.00          2.0  \n",
       "insur_4                   2.0      2.00          2.0  \n",
       "family_income         38208.0  70000.00    9999992.0  \n",
       "family_m                  3.0      4.00         50.0  \n",
       "family_status             3.0      3.00          5.0  \n",
       "house                     1.0      1.00         30.0  \n",
       "car                       2.0      2.00          2.0  \n",
       "invest_0                  0.0      0.00          1.0  \n",
       "invest_1                  1.0      1.00          1.0  \n",
       "invest_2                  0.0      0.00          1.0  \n",
       "invest_3                  0.0      0.00          1.0  \n",
       "invest_4                  0.0      0.00          1.0  \n",
       "invest_5                  0.0      0.00          1.0  \n",
       "invest_6                  0.0      0.00          0.0  \n",
       "invest_7                  0.0      0.00          1.0  \n",
       "invest_8                  0.0      0.00          1.0  \n",
       "son                       1.0      1.00          8.0  \n",
       "daughter                  1.0      1.00          7.0  \n",
       "minor_child               0.0      1.00          6.0  \n",
       "marital                   3.0      3.00          7.0  \n",
       "marital_1st            1985.0   1997.00       2015.0  \n",
       "s_birth                1964.0   1975.00       2004.0  \n",
       "marital_now            1987.0   1999.00       2015.0  \n",
       "s_edu                     4.0      6.00         14.0  \n",
       "s_political               1.0      1.00          4.0  \n",
       "s_hukou                   1.0      2.00          8.0  \n",
       "s_income              12000.0  30000.00    8999999.0  \n",
       "s_work_exper              3.0      5.00          6.0  \n",
       "s_work_status             3.0      3.00          9.0  \n",
       "s_work_type               1.0      1.00          2.0  \n",
       "f_birth                1921.0   1946.00       1979.0  \n",
       "f_edu                     2.0      4.00         14.0  \n",
       "f_political               1.0      1.00          4.0  \n",
       "f_work_14                 2.0      2.00         17.0  \n",
       "m_birth                1926.0   1949.00       1985.0  \n",
       "m_edu                     1.0      3.00         14.0  \n",
       "m_political               1.0      1.00          4.0  \n",
       "m_work_14                 2.0      2.00         17.0  \n",
       "status_peer               2.0      3.00          3.0  \n",
       "status_3_before           2.0      2.00          3.0  \n",
       "view                      4.0      4.00          5.0  \n",
       "inc_ability               2.0      3.00          4.0  \n",
       "inc_exp               40000.0  80000.00  100000000.0  \n",
       "trust_1                   4.0      4.00          5.0  \n",
       "trust_2                   4.0      4.00          5.0  \n",
       "trust_3                   4.0      4.00          5.0  \n",
       "trust_4                   3.0      4.00          5.0  \n",
       "trust_5                   4.0      5.00          5.0  \n",
       "trust_6                   4.0      4.00          5.0  \n",
       "trust_7                   3.0      4.00          5.0  \n",
       "trust_8                   4.0      4.00          5.0  \n",
       "trust_9                   3.0      4.00          5.0  \n",
       "trust_10                  3.0      4.00          5.0  \n",
       "trust_11                  1.0      3.00          5.0  \n",
       "trust_12                  3.0      4.00          5.0  \n",
       "trust_13                  2.0      3.00          5.0  \n",
       "neighbor_familiarity      4.0      5.00          5.0  \n",
       "public_service_1         79.5     80.00        100.0  \n",
       "public_service_2         70.0     80.00        100.0  \n",
       "public_service_3         70.0     80.00        100.0  \n",
       "public_service_4         70.0     80.00        100.0  \n",
       "public_service_5         70.0     80.00        100.0  \n",
       "public_service_6         70.0     80.00        100.0  \n",
       "public_service_7         70.0     80.00        100.0  \n",
       "public_service_8         70.0     80.00        100.0  \n",
       "public_service_9         70.0     80.00        100.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "首先需要对于数据中的连续出现的负数值进行处理。由于数据中的负数值只有-1，-2，-3，-8这几种数值，所以它们进行分别的操作，实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make feature +5\n",
    "#csv中有复数值：-1、-2、-3、-8，将他们视为有问题的特征，但是不删去\n",
    "def getres1(row):\n",
    "    return len([x for x in row.values if type(x)==int and x<0])\n",
    "\n",
    "def getres2(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-8])\n",
    "\n",
    "def getres3(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-1])\n",
    "\n",
    "def getres4(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-2])\n",
    "\n",
    "def getres5(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-3])\n",
    "\n",
    "#检查数据\n",
    "data['neg1'] = data[data.columns].apply(lambda row:getres1(row),axis=1)\n",
    "data.loc[data['neg1']>20,'neg1'] = 20  #平滑处理,最多出现20次\n",
    "\n",
    "data['neg2'] = data[data.columns].apply(lambda row:getres2(row),axis=1)\n",
    "data['neg3'] = data[data.columns].apply(lambda row:getres3(row),axis=1)\n",
    "data['neg4'] = data[data.columns].apply(lambda row:getres4(row),axis=1)\n",
    "data['neg5'] = data[data.columns].apply(lambda row:getres5(row),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填充缺失值，在这里我采取的方式是将缺失值补全，使用fillna(value)，其中value的数值根据具体的情况来确定。例如将大部分缺失信息认为是零，将家庭成员数认为是1，将家庭收入这个特征认为是66365，即所有家庭的收入平均值。部分实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3713033953997809"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[~data['hukou_loc'].isna()].hukou_loc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填充缺失值 共25列 去掉4列 填充21列\n",
    "#以下的列都是缺省的，视情况填补\n",
    "data['work_status'] = data['work_status'].fillna(0)\n",
    "data['work_yr'] = data['work_yr'].fillna(0)\n",
    "data['work_manage'] = data['work_manage'].fillna(0)\n",
    "data['work_type'] = data['work_type'].fillna(0)\n",
    "\n",
    "data['edu_yr'] = data['edu_yr'].fillna(0)\n",
    "data['edu_status'] = data['edu_status'].fillna(0)\n",
    "\n",
    "data['s_work_type'] = data['s_work_type'].fillna(0)\n",
    "data['s_work_status'] = data['s_work_status'].fillna(0)\n",
    "data['s_political'] = data['s_political'].fillna(0)\n",
    "data['s_hukou'] = data['s_hukou'].fillna(0)\n",
    "data['s_income'] = data['s_income'].fillna(0)\n",
    "data['s_birth'] = data['s_birth'].fillna(0)\n",
    "data['s_edu'] = data['s_edu'].fillna(0)\n",
    "data['s_work_exper'] = data['s_work_exper'].fillna(0)\n",
    "\n",
    "data['minor_child'] = data['minor_child'].fillna(0)\n",
    "data['marital_now'] = data['marital_now'].fillna(0)\n",
    "data['marital_1st'] = data['marital_1st'].fillna(0)\n",
    "data['social_neighbor']=data['social_neighbor'].fillna(0)\n",
    "data['social_friend']=data['social_friend'].fillna(0)\n",
    "data['hukou_loc']=data['hukou_loc'].fillna(data['hukou_loc'].mean()) #最少为1，表示户口\n",
    "data['family_income']=data['family_income'].fillna(data['family_income'].mean()) #删除问题值后的平均值,均值对不上？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，还有特殊格式的信息需要另外处理，比如与时间有关的信息，这里主要分为两部分进行处理：首先是将“连续”的年龄，进行分层处理，即划分年龄段，具体地在这里我们将年龄分为了6个区间。其次是计算具体的年龄，在Excel表格中，只有出生年月以及调查时间等信息，我们根据此计算出每一位调查者的真实年龄。具体实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#144+1 =145\n",
    "#继续进行特殊的列进行数据处理\n",
    "#读happiness_index.xlsx\n",
    "data['survey_time'] = pd.to_datetime(data['survey_time'], format='%Y-%m-%d',errors='coerce')#防止时间格式不同的报错errors='coerce‘\n",
    "data['survey_time'] = data['survey_time'].dt.year #仅仅是year，方便计算年龄\n",
    "data['age'] = data['survey_time']-data['birth']\n",
    "# print(data['age'],data['survey_time'],data['birth'])\n",
    "#年龄分层 145+1=146\n",
    "bins = [0,17,26,34,50,63,100]\n",
    "data['age_bin'] = pd.cut(data['age'], bins, labels=[0,1,2,3,4,5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。第三种方法是使用我们日常生活中的真实情况，例如“宗教信息”特征为负数的认为是“不信仰宗教”，并认为“参加宗教活动的频率”为1，即没有参加过宗教活动，主观的进行补全，这也是我在这一步骤中使用最多的一种方式。就像我自己填表一样，这里我全部都使用了我自己的想法进行缺省值的补全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对‘宗教’处理\n",
    "data.loc[data['religion']<0,'religion'] = 1 #1为不信仰宗教\n",
    "data.loc[data['religion_freq']<0,'religion_freq'] = 1 #1为从来没有参加过\n",
    "#对‘教育程度’处理\n",
    "data.loc[data['edu']<0,'edu'] = 4 #初中\n",
    "data.loc[data['edu_status']<0,'edu_status'] = 0\n",
    "data.loc[data['edu_yr']<0,'edu_yr'] = 0\n",
    "#对‘个人收入’处理\n",
    "data.loc[data['income']<0,'income'] = 0 #认为无收入\n",
    "#对‘政治面貌’处理\n",
    "data.loc[data['political']<0,'political'] = 1 #认为是群众\n",
    "#对体重处理\n",
    "data.loc[(data['weight_jin']<=80)&(data['height_cm']>=160),'weight_jin']= data['weight_jin']*2\n",
    "data.loc[data['weight_jin']<=60,'weight_jin']= data['weight_jin']*2  #个人的想法，哈哈哈，没有60斤的成年人吧\n",
    "#对身高处理\n",
    "data.loc[data['height_cm']<150,'height_cm'] = 150 #成年人的实际情况\n",
    "#对‘健康’处理\n",
    "data.loc[data['health']<0,'health'] = 4 #认为是比较健康\n",
    "data.loc[data['health_problem']<0,'health_problem'] = 4\n",
    "#对‘沮丧’处理\n",
    "data.loc[data['depression']<0,'depression'] = 4 #一般人都是很少吧\n",
    "#对‘媒体’处理\n",
    "data.loc[data['media_1']<0,'media_1'] = 1 #都是从不\n",
    "data.loc[data['media_2']<0,'media_2'] = 1\n",
    "data.loc[data['media_3']<0,'media_3'] = 1\n",
    "data.loc[data['media_4']<0,'media_4'] = 1\n",
    "data.loc[data['media_5']<0,'media_5'] = 1\n",
    "data.loc[data['media_6']<0,'media_6'] = 1\n",
    "#对‘空闲活动’处理\n",
    "data.loc[data['leisure_1']<0,'leisure_1'] = 1 #都是根据自己的想法\n",
    "data.loc[data['leisure_2']<0,'leisure_2'] = 5\n",
    "data.loc[data['leisure_3']<0,'leisure_3'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用众数（代码中使用mode()来实现异常值的修正），由于这里的特征是空闲活动，所以采用众数对于缺失值进行处理比较合理。具体的代码参考如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['leisure_4']<0,'leisure_4'] = data['leisure_4'].mode() #取众数\n",
    "data.loc[data['leisure_5']<0,'leisure_5'] = data['leisure_5'].mode()\n",
    "data.loc[data['leisure_6']<0,'leisure_6'] = data['leisure_6'].mode()\n",
    "data.loc[data['leisure_7']<0,'leisure_7'] = data['leisure_7'].mode()\n",
    "data.loc[data['leisure_8']<0,'leisure_8'] = data['leisure_8'].mode()\n",
    "data.loc[data['leisure_9']<0,'leisure_9'] = data['leisure_9'].mode()\n",
    "data.loc[data['leisure_10']<0,'leisure_10'] = data['leisure_10'].mode()\n",
    "data.loc[data['leisure_11']<0,'leisure_11'] = data['leisure_11'].mode()\n",
    "data.loc[data['leisure_12']<0,'leisure_12'] = data['leisure_12'].mode()\n",
    "data.loc[data['socialize']<0,'socialize'] = 2 #很少\n",
    "data.loc[data['relax']<0,'relax'] = 4 #经常\n",
    "data.loc[data['learn']<0,'learn'] = 1 #从不，哈哈哈哈\n",
    "#对‘社交’处理\n",
    "data.loc[data['social_neighbor']<0,'social_neighbor'] = 0\n",
    "data.loc[data['social_friend']<0,'social_friend'] = 0\n",
    "data.loc[data['socia_outing']<0,'socia_outing'] = 1\n",
    "data.loc[data['neighbor_familiarity']<0,'social_neighbor']= 4\n",
    "#对‘社会公平性’处理\n",
    "data.loc[data['equity']<0,'equity'] = 4\n",
    "#对‘社会等级’处理\n",
    "data.loc[data['class_10_before']<0,'class_10_before'] = 3\n",
    "data.loc[data['class']<0,'class'] = 5\n",
    "data.loc[data['class_10_after']<0,'class_10_after'] = 5\n",
    "data.loc[data['class_14']<0,'class_14'] = 2\n",
    "#对‘工作情况’处理\n",
    "data.loc[data['work_status']<0,'work_status'] = 0\n",
    "data.loc[data['work_yr']<0,'work_yr'] = 0\n",
    "data.loc[data['work_manage']<0,'work_manage'] = 0\n",
    "data.loc[data['work_type']<0,'work_type'] = 0\n",
    "#对‘社会保障’处理\n",
    "data.loc[data['insur_1']<0,'insur_1'] = 1\n",
    "data.loc[data['insur_2']<0,'insur_2'] = 1\n",
    "data.loc[data['insur_3']<0,'insur_3'] = 1\n",
    "data.loc[data['insur_4']<0,'insur_4'] = 1\n",
    "data.loc[data['insur_1']==0,'insur_1'] = 0\n",
    "data.loc[data['insur_2']==0,'insur_2'] = 0\n",
    "data.loc[data['insur_3']==0,'insur_3'] = 0\n",
    "data.loc[data['insur_4']==0,'insur_4'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取均值进行缺失值的补全（代码实现为means()），在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。具体的代码参考如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对家庭情况处理\n",
    "family_income_mean = data['family_income'].mean()\n",
    "data.loc[data['family_income']<0,'family_income'] = family_income_mean\n",
    "data.loc[data['family_m']<0,'family_m'] = 2\n",
    "data.loc[data['family_status']<0,'family_status'] = 3\n",
    "data.loc[data['house']<0,'house'] = 1\n",
    "data.loc[data['car']<0,'car'] = 0\n",
    "data.loc[data['car']==2,'car'] = 0\n",
    "data.loc[data['son']<0,'son'] = 1\n",
    "data.loc[data['daughter']<0,'daughter'] = 0\n",
    "data.loc[data['minor_child']<0,'minor_child'] = 0\n",
    "#对‘婚姻’处理\n",
    "data.loc[data['marital_1st']<0,'marital_1st'] = 0\n",
    "data.loc[data['marital_now']<0,'marital_now'] = 0\n",
    "#对‘配偶’处理\n",
    "data.loc[data['s_birth']<0,'s_birth'] = 0\n",
    "data.loc[data['s_edu']<0,'s_edu'] = 0\n",
    "data.loc[data['s_political']<0,'s_political'] = 0\n",
    "data.loc[data['s_hukou']<0,'s_hukou'] = 0\n",
    "data.loc[data['s_income']<0,'s_income'] = 0\n",
    "data.loc[data['s_work_type']<0,'s_work_type'] = 0\n",
    "data.loc[data['s_work_status']<0,'s_work_status'] = 0\n",
    "data.loc[data['s_work_exper']<0,'s_work_exper'] = 0\n",
    "#对‘父母情况’处理\n",
    "data.loc[data['f_birth']<0,'f_birth'] = 1945\n",
    "data.loc[data['f_edu']<0,'f_edu'] = 1\n",
    "data.loc[data['f_political']<0,'f_political'] = 1\n",
    "data.loc[data['f_work_14']<0,'f_work_14'] = 2\n",
    "data.loc[data['m_birth']<0,'m_birth'] = 1940\n",
    "data.loc[data['m_edu']<0,'m_edu'] = 1\n",
    "data.loc[data['m_political']<0,'m_political'] = 1\n",
    "data.loc[data['m_work_14']<0,'m_work_14'] = 2\n",
    "#和同龄人相比社会经济地位\n",
    "data.loc[data['status_peer']<0,'status_peer'] = 2\n",
    "#和3年前比社会经济地位\n",
    "data.loc[data['status_3_before']<0,'status_3_before'] = 2\n",
    "#对‘观点’处理\n",
    "data.loc[data['view']<0,'view'] = 4\n",
    "#对期望年收入处理\n",
    "data.loc[data['inc_ability']<=0,'inc_ability']= 2\n",
    "inc_exp_mean = data['inc_exp'].mean()\n",
    "data.loc[data['inc_exp']<=0,'inc_exp']= inc_exp_mean #取均值\n",
    "\n",
    "#部分特征处理，取众数\n",
    "for i in range(1,9+1):\n",
    "    data.loc[data['public_service_'+str(i)]<0,'public_service_'+str(i)] = data['public_service_'+str(i)].dropna().mode().values[0]\n",
    "for i in range(1,13+1):\n",
    "    data.loc[data['trust_'+str(i)]<0,'trust_'+str(i)] = data['trust_'+str(i)].dropna().mode().values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据增广\n",
    "\n",
    "这一步，我们需要进一步分析每一个特征之间的关系，从而进行数据增广。经过思考，这里我添加了如下的特征：第一次结婚年龄、最近结婚年龄、是否再婚、配偶年龄、配偶年龄差、各种收入比（与配偶之间的收入比、十年后预期收入与现在收入之比等等）、收入与住房面积比（其中也包括10年后期望收入等等各种情况）、社会阶级（10年后的社会阶级、14年后的社会阶级等等）、悠闲指数、满意指数、信任指数等等。除此之外，我还考虑了对于同一省、市、县进行了归一化。例如同一省市内的收入的平均值等以及一个个体相对于同省、市、县其他人的各个指标的情况。同时也考虑了对于同龄人之间的相互比较，即在同龄人中的收入情况、健康情况等等。具体的实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一次结婚年龄 147\n",
    "data['marital_1stbir'] = data['marital_1st'] - data['birth'] \n",
    "#最近结婚年龄 148\n",
    "data['marital_nowtbir'] = data['marital_now'] - data['birth'] \n",
    "#是否再婚 149\n",
    "data['mar'] = data['marital_nowtbir'] - data['marital_1stbir']\n",
    "#配偶年龄 150\n",
    "data['marital_sbir'] = data['marital_now']-data['s_birth']\n",
    "#配偶年龄差 151\n",
    "data['age_'] = data['marital_nowtbir'] - data['marital_sbir'] \n",
    "\n",
    "#收入比 151+7 =158\n",
    "data['income/s_income'] = data['income']/(data['s_income']+1)\n",
    "data['income+s_income'] = data['income']+(data['s_income']+1)\n",
    "data['income/family_income'] = data['income']/(data['family_income']+1)\n",
    "data['all_income/family_income'] = (data['income']+data['s_income'])/(data['family_income']+1)\n",
    "data['income/inc_exp'] = data['income']/(data['inc_exp']+1)\n",
    "data['family_income/m'] = data['family_income']/(data['family_m']+0.01)\n",
    "data['income/m'] = data['income']/(data['family_m']+0.01)\n",
    "\n",
    "#收入/面积比 158+4=162\n",
    "data['income/floor_area'] = data['income']/(data['floor_area']+0.01)\n",
    "data['all_income/floor_area'] = (data['income']+data['s_income'])/(data['floor_area']+0.01)\n",
    "data['family_income/floor_area'] = data['family_income']/(data['floor_area']+0.01)\n",
    "data['floor_area/m'] = data['floor_area']/(data['family_m']+0.01)\n",
    "\n",
    "#class 162+3=165\n",
    "data['class_10_diff'] = (data['class_10_after'] - data['class'])\n",
    "data['class_diff'] = data['class'] - data['class_10_before']\n",
    "data['class_14_diff'] = data['class'] - data['class_14']\n",
    "#悠闲指数 166\n",
    "leisure_fea_lis = ['leisure_'+str(i) for i in range(1,13)]\n",
    "data['leisure_sum'] = data[leisure_fea_lis].sum(axis=1) #skew\n",
    "#满意指数 167\n",
    "public_service_fea_lis = ['public_service_'+str(i) for i in range(1,10)]\n",
    "data['public_service_sum'] = data[public_service_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#信任指数 168\n",
    "trust_fea_lis = ['trust_'+str(i) for i in range(1,14)]\n",
    "data['trust_sum'] = data[trust_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#province mean 168+13=181\n",
    "data['province_income_mean'] = data.groupby(['province'])['income'].transform('mean').values\n",
    "data['province_family_income_mean'] = data.groupby(['province'])['family_income'].transform('mean').values\n",
    "data['province_equity_mean'] = data.groupby(['province'])['equity'].transform('mean').values\n",
    "data['province_depression_mean'] = data.groupby(['province'])['depression'].transform('mean').values\n",
    "data['province_floor_area_mean'] = data.groupby(['province'])['floor_area'].transform('mean').values\n",
    "data['province_health_mean'] = data.groupby(['province'])['health'].transform('mean').values\n",
    "data['province_class_10_diff_mean'] = data.groupby(['province'])['class_10_diff'].transform('mean').values\n",
    "data['province_class_mean'] = data.groupby(['province'])['class'].transform('mean').values\n",
    "data['province_health_problem_mean'] = data.groupby(['province'])['health_problem'].transform('mean').values\n",
    "data['province_family_status_mean'] = data.groupby(['province'])['family_status'].transform('mean').values\n",
    "data['province_leisure_sum_mean'] = data.groupby(['province'])['leisure_sum'].transform('mean').values\n",
    "data['province_public_service_sum_mean'] = data.groupby(['province'])['public_service_sum'].transform('mean').values\n",
    "data['province_trust_sum_mean'] = data.groupby(['province'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#city   mean 181+13=194\n",
    "data['city_income_mean'] = data.groupby(['city'])['income'].transform('mean').values\n",
    "data['city_family_income_mean'] = data.groupby(['city'])['family_income'].transform('mean').values\n",
    "data['city_equity_mean'] = data.groupby(['city'])['equity'].transform('mean').values\n",
    "data['city_depression_mean'] = data.groupby(['city'])['depression'].transform('mean').values\n",
    "data['city_floor_area_mean'] = data.groupby(['city'])['floor_area'].transform('mean').values\n",
    "data['city_health_mean'] = data.groupby(['city'])['health'].transform('mean').values\n",
    "data['city_class_10_diff_mean'] = data.groupby(['city'])['class_10_diff'].transform('mean').values\n",
    "data['city_class_mean'] = data.groupby(['city'])['class'].transform('mean').values\n",
    "data['city_health_problem_mean'] = data.groupby(['city'])['health_problem'].transform('mean').values\n",
    "data['city_family_status_mean'] = data.groupby(['city'])['family_status'].transform('mean').values\n",
    "data['city_leisure_sum_mean'] = data.groupby(['city'])['leisure_sum'].transform('mean').values\n",
    "data['city_public_service_sum_mean'] = data.groupby(['city'])['public_service_sum'].transform('mean').values\n",
    "data['city_trust_sum_mean'] = data.groupby(['city'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#county  mean 194 + 13 = 207\n",
    "data['county_income_mean'] = data.groupby(['county'])['income'].transform('mean').values\n",
    "data['county_family_income_mean'] = data.groupby(['county'])['family_income'].transform('mean').values\n",
    "data['county_equity_mean'] = data.groupby(['county'])['equity'].transform('mean').values\n",
    "data['county_depression_mean'] = data.groupby(['county'])['depression'].transform('mean').values\n",
    "data['county_floor_area_mean'] = data.groupby(['county'])['floor_area'].transform('mean').values\n",
    "data['county_health_mean'] = data.groupby(['county'])['health'].transform('mean').values\n",
    "data['county_class_10_diff_mean'] = data.groupby(['county'])['class_10_diff'].transform('mean').values\n",
    "data['county_class_mean'] = data.groupby(['county'])['class'].transform('mean').values\n",
    "data['county_health_problem_mean'] = data.groupby(['county'])['health_problem'].transform('mean').values\n",
    "data['county_family_status_mean'] = data.groupby(['county'])['family_status'].transform('mean').values\n",
    "data['county_leisure_sum_mean'] = data.groupby(['county'])['leisure_sum'].transform('mean').values\n",
    "data['county_public_service_sum_mean'] = data.groupby(['county'])['public_service_sum'].transform('mean').values\n",
    "data['county_trust_sum_mean'] = data.groupby(['county'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#ratio 相比同省 207 + 13 =220\n",
    "data['income/province'] = data['income']/(data['province_income_mean'])                                      \n",
    "data['family_income/province'] = data['family_income']/(data['province_family_income_mean'])   \n",
    "data['equity/province'] = data['equity']/(data['province_equity_mean'])       \n",
    "data['depression/province'] = data['depression']/(data['province_depression_mean'])                                                \n",
    "data['floor_area/province'] = data['floor_area']/(data['province_floor_area_mean'])\n",
    "data['health/province'] = data['health']/(data['province_health_mean'])\n",
    "data['class_10_diff/province'] = data['class_10_diff']/(data['province_class_10_diff_mean'])\n",
    "data['class/province'] = data['class']/(data['province_class_mean'])\n",
    "data['health_problem/province'] = data['health_problem']/(data['province_health_problem_mean'])\n",
    "data['family_status/province'] = data['family_status']/(data['province_family_status_mean'])\n",
    "data['leisure_sum/province'] = data['leisure_sum']/(data['province_leisure_sum_mean'])\n",
    "data['public_service_sum/province'] = data['public_service_sum']/(data['province_public_service_sum_mean'])\n",
    "data['trust_sum/province'] = data['trust_sum']/(data['province_trust_sum_mean']+1)\n",
    "\n",
    "#ratio 相比同市 220 + 13 =233\n",
    "data['income/city'] = data['income']/(data['city_income_mean'])                                      \n",
    "data['family_income/city'] = data['family_income']/(data['city_family_income_mean'])   \n",
    "data['equity/city'] = data['equity']/(data['city_equity_mean'])       \n",
    "data['depression/city'] = data['depression']/(data['city_depression_mean'])                                                \n",
    "data['floor_area/city'] = data['floor_area']/(data['city_floor_area_mean'])\n",
    "data['health/city'] = data['health']/(data['city_health_mean'])\n",
    "data['class_10_diff/city'] = data['class_10_diff']/(data['city_class_10_diff_mean'])\n",
    "data['class/city'] = data['class']/(data['city_class_mean'])\n",
    "data['health_problem/city'] = data['health_problem']/(data['city_health_problem_mean'])\n",
    "data['family_status/city'] = data['family_status']/(data['city_family_status_mean'])\n",
    "data['leisure_sum/city'] = data['leisure_sum']/(data['city_leisure_sum_mean'])\n",
    "data['public_service_sum/city'] = data['public_service_sum']/(data['city_public_service_sum_mean'])\n",
    "data['trust_sum/city'] = data['trust_sum']/(data['city_trust_sum_mean'])\n",
    "\n",
    "#ratio 相比同个地区 233 + 13 =246\n",
    "data['income/county'] = data['income']/(data['county_income_mean'])                                      \n",
    "data['family_income/county'] = data['family_income']/(data['county_family_income_mean'])   \n",
    "data['equity/county'] = data['equity']/(data['county_equity_mean'])       \n",
    "data['depression/county'] = data['depression']/(data['county_depression_mean'])                                                \n",
    "data['floor_area/county'] = data['floor_area']/(data['county_floor_area_mean'])\n",
    "data['health/county'] = data['health']/(data['county_health_mean'])\n",
    "data['class_10_diff/county'] = data['class_10_diff']/(data['county_class_10_diff_mean'])\n",
    "data['class/county'] = data['class']/(data['county_class_mean'])\n",
    "data['health_problem/county'] = data['health_problem']/(data['county_health_problem_mean'])\n",
    "data['family_status/county'] = data['family_status']/(data['county_family_status_mean'])\n",
    "data['leisure_sum/county'] = data['leisure_sum']/(data['county_leisure_sum_mean'])\n",
    "data['public_service_sum/county'] = data['public_service_sum']/(data['county_public_service_sum_mean'])\n",
    "data['trust_sum/county'] = data['trust_sum']/(data['county_trust_sum_mean'])\n",
    "\n",
    "#age   mean 246+ 13 =259\n",
    "data['age_income_mean'] = data.groupby(['age'])['income'].transform('mean').values\n",
    "data['age_family_income_mean'] = data.groupby(['age'])['family_income'].transform('mean').values\n",
    "data['age_equity_mean'] = data.groupby(['age'])['equity'].transform('mean').values\n",
    "data['age_depression_mean'] = data.groupby(['age'])['depression'].transform('mean').values\n",
    "data['age_floor_area_mean'] = data.groupby(['age'])['floor_area'].transform('mean').values\n",
    "data['age_health_mean'] = data.groupby(['age'])['health'].transform('mean').values\n",
    "data['age_class_10_diff_mean'] = data.groupby(['age'])['class_10_diff'].transform('mean').values\n",
    "data['age_class_mean'] = data.groupby(['age'])['class'].transform('mean').values\n",
    "data['age_health_problem_mean'] = data.groupby(['age'])['health_problem'].transform('mean').values\n",
    "data['age_family_status_mean'] = data.groupby(['age'])['family_status'].transform('mean').values\n",
    "data['age_leisure_sum_mean'] = data.groupby(['age'])['leisure_sum'].transform('mean').values\n",
    "data['age_public_service_sum_mean'] = data.groupby(['age'])['public_service_sum'].transform('mean').values\n",
    "data['age_trust_sum_mean'] = data.groupby(['age'])['trust_sum'].transform('mean').values\n",
    "\n",
    "# 和同龄人相比259 + 13 =272\n",
    "data['income/age'] = data['income']/(data['age_income_mean'])                                      \n",
    "data['family_income/age'] = data['family_income']/(data['age_family_income_mean'])   \n",
    "data['equity/age'] = data['equity']/(data['age_equity_mean'])       \n",
    "data['depression/age'] = data['depression']/(data['age_depression_mean'])                                                \n",
    "data['floor_area/age'] = data['floor_area']/(data['age_floor_area_mean'])\n",
    "data['health/age'] = data['health']/(data['age_health_mean'])\n",
    "data['class_10_diff/age'] = data['class_10_diff']/(data['age_class_10_diff_mean'])\n",
    "data['class/age'] = data['class']/(data['age_class_mean'])\n",
    "data['health_problem/age'] = data['health_problem']/(data['age_health_problem_mean'])\n",
    "data['family_status/age'] = data['family_status']/(data['age_family_status_mean'])\n",
    "data['leisure_sum/age'] = data['leisure_sum']/(data['age_leisure_sum_mean'])\n",
    "data['public_service_sum/age'] = data['public_service_sum']/(data['age_public_service_sum_mean'])\n",
    "data['trust_sum/age'] = data['trust_sum']/(data['age_trust_sum_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过如上的操作后，最终我们的特征从一开始的131维，扩充为了272维的特征。接下来考虑特征工程、训练模型以及模型融合的工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (10956, 272)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey_type</th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>survey_time</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>religion</th>\n",
       "      <th>religion_freq</th>\n",
       "      <th>edu</th>\n",
       "      <th>edu_other</th>\n",
       "      <th>edu_status</th>\n",
       "      <th>edu_yr</th>\n",
       "      <th>income</th>\n",
       "      <th>political</th>\n",
       "      <th>join_party</th>\n",
       "      <th>floor_area</th>\n",
       "      <th>property_0</th>\n",
       "      <th>property_1</th>\n",
       "      <th>property_2</th>\n",
       "      <th>property_3</th>\n",
       "      <th>property_4</th>\n",
       "      <th>property_5</th>\n",
       "      <th>property_6</th>\n",
       "      <th>property_7</th>\n",
       "      <th>property_8</th>\n",
       "      <th>property_other</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_jin</th>\n",
       "      <th>health</th>\n",
       "      <th>health_problem</th>\n",
       "      <th>depression</th>\n",
       "      <th>hukou</th>\n",
       "      <th>hukou_loc</th>\n",
       "      <th>media_1</th>\n",
       "      <th>media_2</th>\n",
       "      <th>media_3</th>\n",
       "      <th>media_4</th>\n",
       "      <th>media_5</th>\n",
       "      <th>media_6</th>\n",
       "      <th>leisure_1</th>\n",
       "      <th>leisure_2</th>\n",
       "      <th>leisure_3</th>\n",
       "      <th>leisure_4</th>\n",
       "      <th>leisure_5</th>\n",
       "      <th>leisure_6</th>\n",
       "      <th>leisure_7</th>\n",
       "      <th>leisure_8</th>\n",
       "      <th>leisure_9</th>\n",
       "      <th>leisure_10</th>\n",
       "      <th>leisure_11</th>\n",
       "      <th>leisure_12</th>\n",
       "      <th>socialize</th>\n",
       "      <th>relax</th>\n",
       "      <th>learn</th>\n",
       "      <th>social_neighbor</th>\n",
       "      <th>social_friend</th>\n",
       "      <th>socia_outing</th>\n",
       "      <th>equity</th>\n",
       "      <th>class</th>\n",
       "      <th>class_10_before</th>\n",
       "      <th>class_10_after</th>\n",
       "      <th>class_14</th>\n",
       "      <th>work_exper</th>\n",
       "      <th>work_status</th>\n",
       "      <th>work_yr</th>\n",
       "      <th>work_type</th>\n",
       "      <th>work_manage</th>\n",
       "      <th>insur_1</th>\n",
       "      <th>insur_2</th>\n",
       "      <th>insur_3</th>\n",
       "      <th>insur_4</th>\n",
       "      <th>family_income</th>\n",
       "      <th>family_m</th>\n",
       "      <th>family_status</th>\n",
       "      <th>house</th>\n",
       "      <th>car</th>\n",
       "      <th>invest_0</th>\n",
       "      <th>invest_1</th>\n",
       "      <th>invest_2</th>\n",
       "      <th>invest_3</th>\n",
       "      <th>invest_4</th>\n",
       "      <th>invest_5</th>\n",
       "      <th>invest_6</th>\n",
       "      <th>invest_7</th>\n",
       "      <th>invest_8</th>\n",
       "      <th>invest_other</th>\n",
       "      <th>son</th>\n",
       "      <th>daughter</th>\n",
       "      <th>minor_child</th>\n",
       "      <th>marital</th>\n",
       "      <th>marital_1st</th>\n",
       "      <th>s_birth</th>\n",
       "      <th>marital_now</th>\n",
       "      <th>s_edu</th>\n",
       "      <th>s_political</th>\n",
       "      <th>s_hukou</th>\n",
       "      <th>s_income</th>\n",
       "      <th>s_work_exper</th>\n",
       "      <th>s_work_status</th>\n",
       "      <th>s_work_type</th>\n",
       "      <th>f_birth</th>\n",
       "      <th>f_edu</th>\n",
       "      <th>f_political</th>\n",
       "      <th>f_work_14</th>\n",
       "      <th>m_birth</th>\n",
       "      <th>m_edu</th>\n",
       "      <th>m_political</th>\n",
       "      <th>m_work_14</th>\n",
       "      <th>status_peer</th>\n",
       "      <th>status_3_before</th>\n",
       "      <th>view</th>\n",
       "      <th>inc_ability</th>\n",
       "      <th>inc_exp</th>\n",
       "      <th>trust_1</th>\n",
       "      <th>trust_2</th>\n",
       "      <th>trust_3</th>\n",
       "      <th>trust_4</th>\n",
       "      <th>trust_5</th>\n",
       "      <th>trust_6</th>\n",
       "      <th>trust_7</th>\n",
       "      <th>trust_8</th>\n",
       "      <th>trust_9</th>\n",
       "      <th>trust_10</th>\n",
       "      <th>trust_11</th>\n",
       "      <th>trust_12</th>\n",
       "      <th>trust_13</th>\n",
       "      <th>neighbor_familiarity</th>\n",
       "      <th>public_service_1</th>\n",
       "      <th>public_service_2</th>\n",
       "      <th>public_service_3</th>\n",
       "      <th>public_service_4</th>\n",
       "      <th>public_service_5</th>\n",
       "      <th>public_service_6</th>\n",
       "      <th>public_service_7</th>\n",
       "      <th>public_service_8</th>\n",
       "      <th>public_service_9</th>\n",
       "      <th>neg1</th>\n",
       "      <th>neg2</th>\n",
       "      <th>neg3</th>\n",
       "      <th>neg4</th>\n",
       "      <th>neg5</th>\n",
       "      <th>age</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>marital_1stbir</th>\n",
       "      <th>marital_nowtbir</th>\n",
       "      <th>mar</th>\n",
       "      <th>marital_sbir</th>\n",
       "      <th>age_</th>\n",
       "      <th>income/s_income</th>\n",
       "      <th>income+s_income</th>\n",
       "      <th>income/family_income</th>\n",
       "      <th>all_income/family_income</th>\n",
       "      <th>income/inc_exp</th>\n",
       "      <th>family_income/m</th>\n",
       "      <th>income/m</th>\n",
       "      <th>income/floor_area</th>\n",
       "      <th>all_income/floor_area</th>\n",
       "      <th>family_income/floor_area</th>\n",
       "      <th>floor_area/m</th>\n",
       "      <th>class_10_diff</th>\n",
       "      <th>class_diff</th>\n",
       "      <th>class_14_diff</th>\n",
       "      <th>leisure_sum</th>\n",
       "      <th>public_service_sum</th>\n",
       "      <th>trust_sum</th>\n",
       "      <th>province_income_mean</th>\n",
       "      <th>province_family_income_mean</th>\n",
       "      <th>province_equity_mean</th>\n",
       "      <th>province_depression_mean</th>\n",
       "      <th>province_floor_area_mean</th>\n",
       "      <th>province_health_mean</th>\n",
       "      <th>province_class_10_diff_mean</th>\n",
       "      <th>province_class_mean</th>\n",
       "      <th>province_health_problem_mean</th>\n",
       "      <th>province_family_status_mean</th>\n",
       "      <th>province_leisure_sum_mean</th>\n",
       "      <th>province_public_service_sum_mean</th>\n",
       "      <th>province_trust_sum_mean</th>\n",
       "      <th>city_income_mean</th>\n",
       "      <th>city_family_income_mean</th>\n",
       "      <th>city_equity_mean</th>\n",
       "      <th>city_depression_mean</th>\n",
       "      <th>city_floor_area_mean</th>\n",
       "      <th>city_health_mean</th>\n",
       "      <th>city_class_10_diff_mean</th>\n",
       "      <th>city_class_mean</th>\n",
       "      <th>city_health_problem_mean</th>\n",
       "      <th>city_family_status_mean</th>\n",
       "      <th>city_leisure_sum_mean</th>\n",
       "      <th>city_public_service_sum_mean</th>\n",
       "      <th>city_trust_sum_mean</th>\n",
       "      <th>county_income_mean</th>\n",
       "      <th>county_family_income_mean</th>\n",
       "      <th>county_equity_mean</th>\n",
       "      <th>county_depression_mean</th>\n",
       "      <th>county_floor_area_mean</th>\n",
       "      <th>county_health_mean</th>\n",
       "      <th>county_class_10_diff_mean</th>\n",
       "      <th>county_class_mean</th>\n",
       "      <th>county_health_problem_mean</th>\n",
       "      <th>county_family_status_mean</th>\n",
       "      <th>county_leisure_sum_mean</th>\n",
       "      <th>county_public_service_sum_mean</th>\n",
       "      <th>county_trust_sum_mean</th>\n",
       "      <th>income/province</th>\n",
       "      <th>family_income/province</th>\n",
       "      <th>equity/province</th>\n",
       "      <th>depression/province</th>\n",
       "      <th>floor_area/province</th>\n",
       "      <th>health/province</th>\n",
       "      <th>class_10_diff/province</th>\n",
       "      <th>class/province</th>\n",
       "      <th>health_problem/province</th>\n",
       "      <th>family_status/province</th>\n",
       "      <th>leisure_sum/province</th>\n",
       "      <th>public_service_sum/province</th>\n",
       "      <th>trust_sum/province</th>\n",
       "      <th>income/city</th>\n",
       "      <th>family_income/city</th>\n",
       "      <th>equity/city</th>\n",
       "      <th>depression/city</th>\n",
       "      <th>floor_area/city</th>\n",
       "      <th>health/city</th>\n",
       "      <th>class_10_diff/city</th>\n",
       "      <th>class/city</th>\n",
       "      <th>health_problem/city</th>\n",
       "      <th>family_status/city</th>\n",
       "      <th>leisure_sum/city</th>\n",
       "      <th>public_service_sum/city</th>\n",
       "      <th>trust_sum/city</th>\n",
       "      <th>income/county</th>\n",
       "      <th>family_income/county</th>\n",
       "      <th>equity/county</th>\n",
       "      <th>depression/county</th>\n",
       "      <th>floor_area/county</th>\n",
       "      <th>health/county</th>\n",
       "      <th>class_10_diff/county</th>\n",
       "      <th>class/county</th>\n",
       "      <th>health_problem/county</th>\n",
       "      <th>family_status/county</th>\n",
       "      <th>leisure_sum/county</th>\n",
       "      <th>public_service_sum/county</th>\n",
       "      <th>trust_sum/county</th>\n",
       "      <th>age_income_mean</th>\n",
       "      <th>age_family_income_mean</th>\n",
       "      <th>age_equity_mean</th>\n",
       "      <th>age_depression_mean</th>\n",
       "      <th>age_floor_area_mean</th>\n",
       "      <th>age_health_mean</th>\n",
       "      <th>age_class_10_diff_mean</th>\n",
       "      <th>age_class_mean</th>\n",
       "      <th>age_health_problem_mean</th>\n",
       "      <th>age_family_status_mean</th>\n",
       "      <th>age_leisure_sum_mean</th>\n",
       "      <th>age_public_service_sum_mean</th>\n",
       "      <th>age_trust_sum_mean</th>\n",
       "      <th>income/age</th>\n",
       "      <th>family_income/age</th>\n",
       "      <th>equity/age</th>\n",
       "      <th>depression/age</th>\n",
       "      <th>floor_area/age</th>\n",
       "      <th>health/age</th>\n",
       "      <th>class_10_diff/age</th>\n",
       "      <th>class/age</th>\n",
       "      <th>health_problem/age</th>\n",
       "      <th>family_status/age</th>\n",
       "      <th>leisure_sum/age</th>\n",
       "      <th>public_service_sum/age</th>\n",
       "      <th>trust_sum/age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60000.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>29850.746269</td>\n",
       "      <td>9950.248756</td>\n",
       "      <td>444.345701</td>\n",
       "      <td>1333.037103</td>\n",
       "      <td>1333.037103</td>\n",
       "      <td>22.388060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>31</td>\n",
       "      <td>61859.505703</td>\n",
       "      <td>131638.952197</td>\n",
       "      <td>2.773764</td>\n",
       "      <td>3.954373</td>\n",
       "      <td>88.692205</td>\n",
       "      <td>3.701521</td>\n",
       "      <td>1.127376</td>\n",
       "      <td>4.572243</td>\n",
       "      <td>4.051331</td>\n",
       "      <td>2.716730</td>\n",
       "      <td>41.701521</td>\n",
       "      <td>574.441065</td>\n",
       "      <td>32.596958</td>\n",
       "      <td>43096.656716</td>\n",
       "      <td>93764.357804</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>4.011940</td>\n",
       "      <td>82.744776</td>\n",
       "      <td>3.650746</td>\n",
       "      <td>1.164179</td>\n",
       "      <td>4.465672</td>\n",
       "      <td>4.113433</td>\n",
       "      <td>2.665672</td>\n",
       "      <td>40.059701</td>\n",
       "      <td>564.402985</td>\n",
       "      <td>31.423881</td>\n",
       "      <td>28979.591837</td>\n",
       "      <td>60630.549238</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>4.081633</td>\n",
       "      <td>38.530612</td>\n",
       "      <td>3.489796</td>\n",
       "      <td>1.183673</td>\n",
       "      <td>4.938776</td>\n",
       "      <td>3.877551</td>\n",
       "      <td>2.489796</td>\n",
       "      <td>36.693878</td>\n",
       "      <td>575.489796</td>\n",
       "      <td>32.775510</td>\n",
       "      <td>0.323313</td>\n",
       "      <td>0.455792</td>\n",
       "      <td>1.081563</td>\n",
       "      <td>1.264423</td>\n",
       "      <td>0.507373</td>\n",
       "      <td>0.810478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656133</td>\n",
       "      <td>0.493665</td>\n",
       "      <td>0.736179</td>\n",
       "      <td>0.791338</td>\n",
       "      <td>0.731146</td>\n",
       "      <td>0.922703</td>\n",
       "      <td>0.464073</td>\n",
       "      <td>0.639902</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.246280</td>\n",
       "      <td>0.543841</td>\n",
       "      <td>0.821750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671791</td>\n",
       "      <td>0.486212</td>\n",
       "      <td>0.750280</td>\n",
       "      <td>0.823770</td>\n",
       "      <td>0.744149</td>\n",
       "      <td>0.986511</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.989600</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>1.167903</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.899333</td>\n",
       "      <td>0.729813</td>\n",
       "      <td>0.945828</td>\n",
       "      <td>24371.808219</td>\n",
       "      <td>68176.863103</td>\n",
       "      <td>3.061644</td>\n",
       "      <td>3.890411</td>\n",
       "      <td>109.662329</td>\n",
       "      <td>3.534247</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>4.390411</td>\n",
       "      <td>3.835616</td>\n",
       "      <td>2.726027</td>\n",
       "      <td>45.541096</td>\n",
       "      <td>630.027397</td>\n",
       "      <td>33.479452</td>\n",
       "      <td>0.820620</td>\n",
       "      <td>0.880064</td>\n",
       "      <td>0.979866</td>\n",
       "      <td>1.285211</td>\n",
       "      <td>0.410351</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683307</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>0.733668</td>\n",
       "      <td>0.724620</td>\n",
       "      <td>0.666638</td>\n",
       "      <td>0.925941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40000.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1972</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1973</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>85.0</td>\n",
       "      <td>70</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20001.0</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>13289.036545</td>\n",
       "      <td>6644.518272</td>\n",
       "      <td>181.801654</td>\n",
       "      <td>181.801654</td>\n",
       "      <td>363.603309</td>\n",
       "      <td>36.544850</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>43</td>\n",
       "      <td>29806.230241</td>\n",
       "      <td>37759.043495</td>\n",
       "      <td>3.249141</td>\n",
       "      <td>3.991409</td>\n",
       "      <td>148.463918</td>\n",
       "      <td>3.955326</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>4.414089</td>\n",
       "      <td>3.960481</td>\n",
       "      <td>2.737113</td>\n",
       "      <td>47.194158</td>\n",
       "      <td>631.831615</td>\n",
       "      <td>40.281787</td>\n",
       "      <td>64016.125654</td>\n",
       "      <td>34201.296600</td>\n",
       "      <td>3.371728</td>\n",
       "      <td>3.989529</td>\n",
       "      <td>157.528796</td>\n",
       "      <td>3.968586</td>\n",
       "      <td>0.942408</td>\n",
       "      <td>4.335079</td>\n",
       "      <td>3.963351</td>\n",
       "      <td>2.811518</td>\n",
       "      <td>48.303665</td>\n",
       "      <td>646.188482</td>\n",
       "      <td>40.115183</td>\n",
       "      <td>11628.659794</td>\n",
       "      <td>34759.169720</td>\n",
       "      <td>3.350515</td>\n",
       "      <td>3.958763</td>\n",
       "      <td>154.608247</td>\n",
       "      <td>3.927835</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>4.123711</td>\n",
       "      <td>3.948454</td>\n",
       "      <td>2.752577</td>\n",
       "      <td>48.628866</td>\n",
       "      <td>641.494845</td>\n",
       "      <td>41.268041</td>\n",
       "      <td>0.671001</td>\n",
       "      <td>1.059349</td>\n",
       "      <td>0.923321</td>\n",
       "      <td>0.751614</td>\n",
       "      <td>0.740921</td>\n",
       "      <td>1.264118</td>\n",
       "      <td>2.063830</td>\n",
       "      <td>1.359284</td>\n",
       "      <td>1.009978</td>\n",
       "      <td>1.461394</td>\n",
       "      <td>0.826373</td>\n",
       "      <td>1.068323</td>\n",
       "      <td>1.041622</td>\n",
       "      <td>0.312421</td>\n",
       "      <td>1.169546</td>\n",
       "      <td>0.889752</td>\n",
       "      <td>0.751969</td>\n",
       "      <td>0.698285</td>\n",
       "      <td>1.259894</td>\n",
       "      <td>2.122222</td>\n",
       "      <td>1.384058</td>\n",
       "      <td>1.009247</td>\n",
       "      <td>1.422719</td>\n",
       "      <td>0.807392</td>\n",
       "      <td>1.044587</td>\n",
       "      <td>1.071913</td>\n",
       "      <td>1.719889</td>\n",
       "      <td>1.150775</td>\n",
       "      <td>0.895385</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.711476</td>\n",
       "      <td>1.272966</td>\n",
       "      <td>2.179775</td>\n",
       "      <td>1.455000</td>\n",
       "      <td>1.013055</td>\n",
       "      <td>1.453184</td>\n",
       "      <td>0.801993</td>\n",
       "      <td>1.052230</td>\n",
       "      <td>1.041969</td>\n",
       "      <td>20771.900826</td>\n",
       "      <td>71278.333790</td>\n",
       "      <td>3.157025</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>115.446281</td>\n",
       "      <td>4.239669</td>\n",
       "      <td>1.975207</td>\n",
       "      <td>4.462810</td>\n",
       "      <td>4.487603</td>\n",
       "      <td>2.942149</td>\n",
       "      <td>38.545455</td>\n",
       "      <td>596.933884</td>\n",
       "      <td>36.181818</td>\n",
       "      <td>0.962839</td>\n",
       "      <td>0.561180</td>\n",
       "      <td>0.950262</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.952824</td>\n",
       "      <td>1.179337</td>\n",
       "      <td>1.012552</td>\n",
       "      <td>1.344444</td>\n",
       "      <td>0.891344</td>\n",
       "      <td>1.359551</td>\n",
       "      <td>1.011792</td>\n",
       "      <td>1.130778</td>\n",
       "      <td>1.188442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>126</td>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8000.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>80.0</td>\n",
       "      <td>75</td>\n",
       "      <td>79</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333278</td>\n",
       "      <td>8001.0</td>\n",
       "      <td>0.249969</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>2657.807309</td>\n",
       "      <td>664.451827</td>\n",
       "      <td>16.665278</td>\n",
       "      <td>66.661112</td>\n",
       "      <td>66.661112</td>\n",
       "      <td>39.867110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>27</td>\n",
       "      <td>16246.747967</td>\n",
       "      <td>95335.882250</td>\n",
       "      <td>3.298103</td>\n",
       "      <td>3.647696</td>\n",
       "      <td>107.902439</td>\n",
       "      <td>3.582656</td>\n",
       "      <td>1.029810</td>\n",
       "      <td>4.243902</td>\n",
       "      <td>3.596206</td>\n",
       "      <td>2.607046</td>\n",
       "      <td>46.653117</td>\n",
       "      <td>653.149051</td>\n",
       "      <td>30.392954</td>\n",
       "      <td>6522.105263</td>\n",
       "      <td>32477.322501</td>\n",
       "      <td>3.294737</td>\n",
       "      <td>3.957895</td>\n",
       "      <td>120.684211</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.505263</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>2.768421</td>\n",
       "      <td>48.263158</td>\n",
       "      <td>675.600000</td>\n",
       "      <td>33.221053</td>\n",
       "      <td>6522.105263</td>\n",
       "      <td>32477.322501</td>\n",
       "      <td>3.294737</td>\n",
       "      <td>3.957895</td>\n",
       "      <td>120.684211</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.505263</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>2.768421</td>\n",
       "      <td>48.263158</td>\n",
       "      <td>675.600000</td>\n",
       "      <td>33.221053</td>\n",
       "      <td>0.123102</td>\n",
       "      <td>0.083914</td>\n",
       "      <td>1.212818</td>\n",
       "      <td>1.370728</td>\n",
       "      <td>1.112116</td>\n",
       "      <td>1.116490</td>\n",
       "      <td>0.971053</td>\n",
       "      <td>1.178161</td>\n",
       "      <td>1.112283</td>\n",
       "      <td>1.150728</td>\n",
       "      <td>0.964566</td>\n",
       "      <td>1.146752</td>\n",
       "      <td>0.860066</td>\n",
       "      <td>0.306649</td>\n",
       "      <td>0.246326</td>\n",
       "      <td>1.214058</td>\n",
       "      <td>1.263298</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.109813</td>\n",
       "      <td>1.041096</td>\n",
       "      <td>1.083650</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>1.108644</td>\n",
       "      <td>0.812738</td>\n",
       "      <td>0.306649</td>\n",
       "      <td>0.246326</td>\n",
       "      <td>1.214058</td>\n",
       "      <td>1.263298</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.109813</td>\n",
       "      <td>1.041096</td>\n",
       "      <td>1.083650</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>1.108644</td>\n",
       "      <td>0.812738</td>\n",
       "      <td>23468.987342</td>\n",
       "      <td>51169.410130</td>\n",
       "      <td>3.025316</td>\n",
       "      <td>3.721519</td>\n",
       "      <td>123.415190</td>\n",
       "      <td>3.476793</td>\n",
       "      <td>0.839662</td>\n",
       "      <td>4.181435</td>\n",
       "      <td>3.789030</td>\n",
       "      <td>2.518987</td>\n",
       "      <td>46.561181</td>\n",
       "      <td>627.721519</td>\n",
       "      <td>33.594937</td>\n",
       "      <td>0.085219</td>\n",
       "      <td>0.156343</td>\n",
       "      <td>1.322176</td>\n",
       "      <td>1.343537</td>\n",
       "      <td>0.972328</td>\n",
       "      <td>1.150485</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>1.195762</td>\n",
       "      <td>1.055679</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>0.966470</td>\n",
       "      <td>1.193204</td>\n",
       "      <td>0.803693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>6420</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12000.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1943.0</td>\n",
       "      <td>-1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1943.0</td>\n",
       "      <td>6420.000000</td>\n",
       "      <td>6421.0</td>\n",
       "      <td>0.534955</td>\n",
       "      <td>0.534955</td>\n",
       "      <td>0.641936</td>\n",
       "      <td>3986.710963</td>\n",
       "      <td>2132.890365</td>\n",
       "      <td>82.297141</td>\n",
       "      <td>82.297141</td>\n",
       "      <td>153.826433</td>\n",
       "      <td>25.913621</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>44</td>\n",
       "      <td>26771.017391</td>\n",
       "      <td>61037.025502</td>\n",
       "      <td>3.401739</td>\n",
       "      <td>3.954783</td>\n",
       "      <td>113.826087</td>\n",
       "      <td>3.827826</td>\n",
       "      <td>0.918261</td>\n",
       "      <td>4.577391</td>\n",
       "      <td>3.923478</td>\n",
       "      <td>2.772174</td>\n",
       "      <td>45.375652</td>\n",
       "      <td>657.238261</td>\n",
       "      <td>38.469565</td>\n",
       "      <td>45077.070707</td>\n",
       "      <td>102113.618058</td>\n",
       "      <td>3.252525</td>\n",
       "      <td>4.030303</td>\n",
       "      <td>98.858586</td>\n",
       "      <td>4.070707</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>4.464646</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>2.696970</td>\n",
       "      <td>42.727273</td>\n",
       "      <td>631.636364</td>\n",
       "      <td>33.171717</td>\n",
       "      <td>45077.070707</td>\n",
       "      <td>102113.618058</td>\n",
       "      <td>3.252525</td>\n",
       "      <td>4.030303</td>\n",
       "      <td>98.858586</td>\n",
       "      <td>4.070707</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>4.464646</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>2.696970</td>\n",
       "      <td>42.727273</td>\n",
       "      <td>631.636364</td>\n",
       "      <td>33.171717</td>\n",
       "      <td>0.239812</td>\n",
       "      <td>0.196602</td>\n",
       "      <td>1.175869</td>\n",
       "      <td>1.011434</td>\n",
       "      <td>0.685256</td>\n",
       "      <td>1.044980</td>\n",
       "      <td>2.178030</td>\n",
       "      <td>1.092325</td>\n",
       "      <td>1.019504</td>\n",
       "      <td>1.082183</td>\n",
       "      <td>0.947645</td>\n",
       "      <td>1.156354</td>\n",
       "      <td>1.114783</td>\n",
       "      <td>0.142423</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>1.229814</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.789006</td>\n",
       "      <td>0.982630</td>\n",
       "      <td>2.869565</td>\n",
       "      <td>1.119910</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.112360</td>\n",
       "      <td>1.006383</td>\n",
       "      <td>1.203224</td>\n",
       "      <td>1.326431</td>\n",
       "      <td>0.142423</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>1.229814</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.789006</td>\n",
       "      <td>0.982630</td>\n",
       "      <td>2.869565</td>\n",
       "      <td>1.119910</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.112360</td>\n",
       "      <td>1.006383</td>\n",
       "      <td>1.203224</td>\n",
       "      <td>1.326431</td>\n",
       "      <td>15072.375000</td>\n",
       "      <td>123040.453017</td>\n",
       "      <td>3.348214</td>\n",
       "      <td>3.598214</td>\n",
       "      <td>121.433036</td>\n",
       "      <td>3.133929</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>4.169643</td>\n",
       "      <td>3.366071</td>\n",
       "      <td>2.580357</td>\n",
       "      <td>47.812500</td>\n",
       "      <td>658.687500</td>\n",
       "      <td>33.821429</td>\n",
       "      <td>0.425945</td>\n",
       "      <td>0.097529</td>\n",
       "      <td>1.194667</td>\n",
       "      <td>1.111663</td>\n",
       "      <td>0.642329</td>\n",
       "      <td>1.276353</td>\n",
       "      <td>4.977778</td>\n",
       "      <td>1.199143</td>\n",
       "      <td>1.188329</td>\n",
       "      <td>1.162630</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>1.153810</td>\n",
       "      <td>1.300950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>66365.63755</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1970</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1972</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16550.034302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>947.945116</td>\n",
       "      <td>17.456359</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>44</td>\n",
       "      <td>33930.569444</td>\n",
       "      <td>73738.687545</td>\n",
       "      <td>3.309028</td>\n",
       "      <td>4.093750</td>\n",
       "      <td>57.526389</td>\n",
       "      <td>3.774306</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.729167</td>\n",
       "      <td>44.447917</td>\n",
       "      <td>638.840278</td>\n",
       "      <td>30.152778</td>\n",
       "      <td>33930.569444</td>\n",
       "      <td>73738.687545</td>\n",
       "      <td>3.309028</td>\n",
       "      <td>4.093750</td>\n",
       "      <td>57.526389</td>\n",
       "      <td>3.774306</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.729167</td>\n",
       "      <td>44.447917</td>\n",
       "      <td>638.840278</td>\n",
       "      <td>30.152778</td>\n",
       "      <td>28430.769231</td>\n",
       "      <td>85328.927891</td>\n",
       "      <td>3.589744</td>\n",
       "      <td>3.974359</td>\n",
       "      <td>57.553846</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.641026</td>\n",
       "      <td>4.179487</td>\n",
       "      <td>3.025641</td>\n",
       "      <td>44.846154</td>\n",
       "      <td>552.051282</td>\n",
       "      <td>35.487179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900011</td>\n",
       "      <td>0.604407</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>1.216833</td>\n",
       "      <td>1.324747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.212121</td>\n",
       "      <td>1.099237</td>\n",
       "      <td>0.899930</td>\n",
       "      <td>0.704401</td>\n",
       "      <td>1.412394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900011</td>\n",
       "      <td>0.604407</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>1.216833</td>\n",
       "      <td>1.324747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.212121</td>\n",
       "      <td>1.099237</td>\n",
       "      <td>0.899930</td>\n",
       "      <td>0.704401</td>\n",
       "      <td>1.459235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777762</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.754839</td>\n",
       "      <td>1.216252</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274648</td>\n",
       "      <td>1.196319</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.891938</td>\n",
       "      <td>0.815142</td>\n",
       "      <td>1.239884</td>\n",
       "      <td>14158.165138</td>\n",
       "      <td>69150.138536</td>\n",
       "      <td>3.082569</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>119.192661</td>\n",
       "      <td>4.247706</td>\n",
       "      <td>1.779817</td>\n",
       "      <td>4.220183</td>\n",
       "      <td>4.477064</td>\n",
       "      <td>2.743119</td>\n",
       "      <td>38.266055</td>\n",
       "      <td>617.995413</td>\n",
       "      <td>39.376147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959733</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.587284</td>\n",
       "      <td>1.177106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236957</td>\n",
       "      <td>1.116803</td>\n",
       "      <td>1.093645</td>\n",
       "      <td>1.045313</td>\n",
       "      <td>0.728161</td>\n",
       "      <td>1.117428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  survey_type  province  city  county  survey_time  gender  birth  \\\n",
       "0   1            1        12    32      59         1970       1   1959   \n",
       "1   2            2        18    52      85         1970       1   1992   \n",
       "2   3            2        29    83     126         1970       2   1967   \n",
       "3   4            2        10    28      51         1970       2   1943   \n",
       "4   5            1         7    18      36         1970       2   1994   \n",
       "\n",
       "   nationality  religion  religion_freq  edu edu_other  edu_status  edu_yr  \\\n",
       "0            1         1              1   11         0         4.0     0.0   \n",
       "1            1         1              1   12         0         4.0  2013.0   \n",
       "2            1         0              3    4         0         4.0     0.0   \n",
       "3            1         1              1    3         0         4.0  1959.0   \n",
       "4            1         1              1   12         0         1.0  2014.0   \n",
       "\n",
       "   income  political  join_party  floor_area  property_0  property_1  \\\n",
       "0   20000          1         0.0        45.0           0           1   \n",
       "1   20000          1         0.0       110.0           0           0   \n",
       "2    2000          1         0.0       120.0           0           1   \n",
       "3    6420          1         0.0        78.0           0           0   \n",
       "4       0          2         0.0        70.0           0           0   \n",
       "\n",
       "   property_2  property_3  property_4  property_5  property_6  property_7  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           1           0           0           0   \n",
       "2           1           0           0           0           0           0   \n",
       "3           0           1           0           0           0           0   \n",
       "4           0           0           1           0           0           0   \n",
       "\n",
       "   property_8 property_other  height_cm  weight_jin  health  health_problem  \\\n",
       "0           0              0        176         155       3               2   \n",
       "1           0              0        170         110       5               4   \n",
       "2           0              0        160         122       4               4   \n",
       "3           0              0        163         170       4               4   \n",
       "4           0              0        165         110       5               5   \n",
       "\n",
       "   depression  hukou  hukou_loc  media_1  media_2  media_3  media_4  media_5  \\\n",
       "0           5      5        2.0        4        2        5        5        4   \n",
       "1           3      1        1.0        2        2        1        3        5   \n",
       "2           5      1        1.0        2        2        2        5        1   \n",
       "3           4      1        2.0        2        1        1        5        1   \n",
       "4           3      2        3.0        1        3        4        2        5   \n",
       "\n",
       "   media_6  leisure_1  leisure_2  leisure_3  leisure_4  leisure_5  leisure_6  \\\n",
       "0        3          1          4          3        1.0        2.0        3.0   \n",
       "1        1          2          3          4        3.0        5.0        4.0   \n",
       "2        3          1          4          4        3.0        5.0        4.0   \n",
       "3        1          1          5          2        4.0        5.0        4.0   \n",
       "4        5          3          3          3        2.0        4.0        4.0   \n",
       "\n",
       "   leisure_7  leisure_8  leisure_9  leisure_10  leisure_11  leisure_12  \\\n",
       "0        4.0        1.0        4.0         5.0         4.0         1.0   \n",
       "1        3.0        2.0        3.0         4.0         5.0         1.0   \n",
       "2        4.0        2.0        3.0         5.0         5.0         5.0   \n",
       "3        5.0        1.0        1.0         5.0         5.0         5.0   \n",
       "4        3.0        5.0        2.0         5.0         5.0         1.0   \n",
       "\n",
       "   socialize  relax  learn  social_neighbor  social_friend  socia_outing  \\\n",
       "0          2      4      3              3.0            3.0             2   \n",
       "1          2      4      3              6.0            2.0             1   \n",
       "2          3      4      2              2.0            5.0             2   \n",
       "3          2      4      4              1.0            6.0             1   \n",
       "4          4      3      4              7.0            5.0             3   \n",
       "\n",
       "   equity  class  class_10_before  class_10_after  class_14  work_exper  \\\n",
       "0       3      3                3               3         1           1   \n",
       "1       3      6                4               8         5           1   \n",
       "2       4      5                4               6         3           2   \n",
       "3       4      5                5               7         2           4   \n",
       "4       2      1                1               1         4           6   \n",
       "\n",
       "   work_status  work_yr  work_type  work_manage  insur_1  insur_2  insur_3  \\\n",
       "0          3.0     30.0        1.0          2.0        1        1        1   \n",
       "1          3.0      2.0        1.0          3.0        1        1        1   \n",
       "2          0.0      0.0        0.0          0.0        1        1        2   \n",
       "3          0.0      0.0        0.0          0.0        2        2        2   \n",
       "4          0.0      0.0        0.0          0.0        1        2        2   \n",
       "\n",
       "   insur_4  family_income  family_m  family_status  house  car  invest_0  \\\n",
       "0        2    60000.00000         2              2      1    0         0   \n",
       "1        1    40000.00000         3              4      1    0         0   \n",
       "2        2     8000.00000         3              3      1    0         0   \n",
       "3        2    12000.00000         3              3      1    1         0   \n",
       "4        2    66365.63755         4              3      1    1         0   \n",
       "\n",
       "   invest_1  invest_2  invest_3  invest_4  invest_5  invest_6  invest_7  \\\n",
       "0         1         0         0         0         0         0         0   \n",
       "1         1         0         0         0         0         0         0   \n",
       "2         1         0         0         0         0         0         0   \n",
       "3         1         0         0         0         0         0         0   \n",
       "4         1         0         0         0         0         0         0   \n",
       "\n",
       "   invest_8 invest_other  son  daughter  minor_child  marital  marital_1st  \\\n",
       "0         0            0    1         0          0.0        3       1984.0   \n",
       "1         0            0    0         0          0.0        1          0.0   \n",
       "2         0            0    0         2          1.0        3       1990.0   \n",
       "3         0            0    1         4          0.0        7       1960.0   \n",
       "4         0            0    0         0          0.0        1          0.0   \n",
       "\n",
       "   s_birth  marital_now  s_edu  s_political  s_hukou  s_income  s_work_exper  \\\n",
       "0   1958.0       1984.0    6.0          1.0      5.0   40000.0           5.0   \n",
       "1      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "2   1968.0       1990.0    3.0          1.0      1.0    6000.0           3.0   \n",
       "3      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "4      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "\n",
       "   s_work_status  s_work_type  f_birth  f_edu  f_political  f_work_14  \\\n",
       "0            0.0          0.0     1945      4            4          1   \n",
       "1            0.0          0.0     1972      3            1          2   \n",
       "2            0.0          0.0     1945      1            1          2   \n",
       "3            0.0          0.0     1945     14            1          2   \n",
       "4            0.0          0.0     1970      6            1         10   \n",
       "\n",
       "   m_birth  m_edu  m_political  m_work_14  status_peer  status_3_before  view  \\\n",
       "0     1940      4            1          1            3                2     4   \n",
       "1     1973      3            1          2            1                1     4   \n",
       "2     1940      1            1          2            2                1     4   \n",
       "3     1940      1            1          2            2                1     3   \n",
       "4     1972      4            1         15            3                2     3   \n",
       "\n",
       "   inc_ability   inc_exp  trust_1  trust_2  trust_3  trust_4  trust_5  \\\n",
       "0            3   50000.0        4        2        4        4        5   \n",
       "1            2   50000.0        5        4        4        3        5   \n",
       "2            2   80000.0        3        3        3        3        4   \n",
       "3            2   10000.0        3        3        4        3        5   \n",
       "4            2  200000.0        4        3        3        3        5   \n",
       "\n",
       "   trust_6  trust_7  trust_8  trust_9  trust_10  trust_11  trust_12  trust_13  \\\n",
       "0        3        2        3        4         3        -8         4         1   \n",
       "1        3        3        3        2         3         3         3         2   \n",
       "2        3        3        3        3         3        -8         3         1   \n",
       "3        3        3        5        4         3         3         3         2   \n",
       "4        5        3        4        3         3         3         3         2   \n",
       "\n",
       "   neighbor_familiarity  public_service_1  public_service_2  public_service_3  \\\n",
       "0                     4                50              60.0                50   \n",
       "1                     3                90              70.0                70   \n",
       "2                     4                90              80.0                75   \n",
       "3                     3               100              90.0                70   \n",
       "4                     2                50              50.0                50   \n",
       "\n",
       "   public_service_4  public_service_5  public_service_6  public_service_7  \\\n",
       "0                50              30.0                30                50   \n",
       "1                80              85.0                70                90   \n",
       "2                79              80.0                90                90   \n",
       "3                80              80.0                90                90   \n",
       "4                50              50.0                50                50   \n",
       "\n",
       "   public_service_8  public_service_9  neg1  neg2  neg3  neg4  neg5  age  \\\n",
       "0                50                50     5     3     0     2     0   11   \n",
       "1                60                60     0     0     0     0     0  -22   \n",
       "2                90                75     3     1     0     2     0    3   \n",
       "3                80                80     2     0     0     2     0   27   \n",
       "4                50                50     2     1     1     0     0  -24   \n",
       "\n",
       "  age_bin  marital_1stbir  marital_nowtbir     mar  marital_sbir    age_  \\\n",
       "0       0            25.0             25.0     0.0          26.0    -1.0   \n",
       "1     NaN         -1992.0          -1992.0     0.0           0.0 -1992.0   \n",
       "2       0            23.0             23.0     0.0          22.0     1.0   \n",
       "3       2            17.0          -1943.0 -1960.0           0.0 -1943.0   \n",
       "4     NaN         -1994.0          -1994.0     0.0           0.0 -1994.0   \n",
       "\n",
       "   income/s_income  income+s_income  income/family_income  \\\n",
       "0         0.499988          60001.0              0.333328   \n",
       "1     20000.000000          20001.0              0.499988   \n",
       "2         0.333278           8001.0              0.249969   \n",
       "3      6420.000000           6421.0              0.534955   \n",
       "4         0.000000              1.0              0.000000   \n",
       "\n",
       "   all_income/family_income  income/inc_exp  family_income/m     income/m  \\\n",
       "0                  0.999983        0.399992     29850.746269  9950.248756   \n",
       "1                  0.499988        0.399992     13289.036545  6644.518272   \n",
       "2                  0.999875        0.025000      2657.807309   664.451827   \n",
       "3                  0.534955        0.641936      3986.710963  2132.890365   \n",
       "4                  0.000000        0.000000     16550.034302     0.000000   \n",
       "\n",
       "   income/floor_area  all_income/floor_area  family_income/floor_area  \\\n",
       "0         444.345701            1333.037103               1333.037103   \n",
       "1         181.801654             181.801654                363.603309   \n",
       "2          16.665278              66.661112                 66.661112   \n",
       "3          82.297141              82.297141                153.826433   \n",
       "4           0.000000               0.000000                947.945116   \n",
       "\n",
       "   floor_area/m  class_10_diff  class_diff  class_14_diff  leisure_sum  \\\n",
       "0     22.388060              0           0              2         33.0   \n",
       "1     36.544850              2           2              1         39.0   \n",
       "2     39.867110              1           1              2         45.0   \n",
       "3     25.913621              2           0              3         43.0   \n",
       "4     17.456359              0           0             -3         40.0   \n",
       "\n",
       "   public_service_sum  trust_sum  province_income_mean  \\\n",
       "0               420.0         31          61859.505703   \n",
       "1               675.0         43          29806.230241   \n",
       "2               749.0         27          16246.747967   \n",
       "3               760.0         44          26771.017391   \n",
       "4               450.0         44          33930.569444   \n",
       "\n",
       "   province_family_income_mean  province_equity_mean  \\\n",
       "0                131638.952197              2.773764   \n",
       "1                 37759.043495              3.249141   \n",
       "2                 95335.882250              3.298103   \n",
       "3                 61037.025502              3.401739   \n",
       "4                 73738.687545              3.309028   \n",
       "\n",
       "   province_depression_mean  province_floor_area_mean  province_health_mean  \\\n",
       "0                  3.954373                 88.692205              3.701521   \n",
       "1                  3.991409                148.463918              3.955326   \n",
       "2                  3.647696                107.902439              3.582656   \n",
       "3                  3.954783                113.826087              3.827826   \n",
       "4                  4.093750                 57.526389              3.774306   \n",
       "\n",
       "   province_class_10_diff_mean  province_class_mean  \\\n",
       "0                     1.127376             4.572243   \n",
       "1                     0.969072             4.414089   \n",
       "2                     1.029810             4.243902   \n",
       "3                     0.918261             4.577391   \n",
       "4                     0.527778             4.444444   \n",
       "\n",
       "   province_health_problem_mean  province_family_status_mean  \\\n",
       "0                      4.051331                     2.716730   \n",
       "1                      3.960481                     2.737113   \n",
       "2                      3.596206                     2.607046   \n",
       "3                      3.923478                     2.772174   \n",
       "4                      4.125000                     2.729167   \n",
       "\n",
       "   province_leisure_sum_mean  province_public_service_sum_mean  \\\n",
       "0                  41.701521                        574.441065   \n",
       "1                  47.194158                        631.831615   \n",
       "2                  46.653117                        653.149051   \n",
       "3                  45.375652                        657.238261   \n",
       "4                  44.447917                        638.840278   \n",
       "\n",
       "   province_trust_sum_mean  city_income_mean  city_family_income_mean  \\\n",
       "0                32.596958      43096.656716             93764.357804   \n",
       "1                40.281787      64016.125654             34201.296600   \n",
       "2                30.392954       6522.105263             32477.322501   \n",
       "3                38.469565      45077.070707            102113.618058   \n",
       "4                30.152778      33930.569444             73738.687545   \n",
       "\n",
       "   city_equity_mean  city_depression_mean  city_floor_area_mean  \\\n",
       "0          2.800000              4.011940             82.744776   \n",
       "1          3.371728              3.989529            157.528796   \n",
       "2          3.294737              3.957895            120.684211   \n",
       "3          3.252525              4.030303             98.858586   \n",
       "4          3.309028              4.093750             57.526389   \n",
       "\n",
       "   city_health_mean  city_class_10_diff_mean  city_class_mean  \\\n",
       "0          3.650746                 1.164179         4.465672   \n",
       "1          3.968586                 0.942408         4.335079   \n",
       "2          3.684211                 0.789474         4.505263   \n",
       "3          4.070707                 0.696970         4.464646   \n",
       "4          3.774306                 0.527778         4.444444   \n",
       "\n",
       "   city_health_problem_mean  city_family_status_mean  city_leisure_sum_mean  \\\n",
       "0                  4.113433                 2.665672              40.059701   \n",
       "1                  3.963351                 2.811518              48.303665   \n",
       "2                  3.842105                 2.768421              48.263158   \n",
       "3                  4.090909                 2.696970              42.727273   \n",
       "4                  4.125000                 2.729167              44.447917   \n",
       "\n",
       "   city_public_service_sum_mean  city_trust_sum_mean  county_income_mean  \\\n",
       "0                    564.402985            31.423881        28979.591837   \n",
       "1                    646.188482            40.115183        11628.659794   \n",
       "2                    675.600000            33.221053         6522.105263   \n",
       "3                    631.636364            33.171717        45077.070707   \n",
       "4                    638.840278            30.152778        28430.769231   \n",
       "\n",
       "   county_family_income_mean  county_equity_mean  county_depression_mean  \\\n",
       "0               60630.549238            2.571429                4.081633   \n",
       "1               34759.169720            3.350515                3.958763   \n",
       "2               32477.322501            3.294737                3.957895   \n",
       "3              102113.618058            3.252525                4.030303   \n",
       "4               85328.927891            3.589744                3.974359   \n",
       "\n",
       "   county_floor_area_mean  county_health_mean  county_class_10_diff_mean  \\\n",
       "0               38.530612            3.489796                   1.183673   \n",
       "1              154.608247            3.927835                   0.917526   \n",
       "2              120.684211            3.684211                   0.789474   \n",
       "3               98.858586            4.070707                   0.696970   \n",
       "4               57.553846            4.000000                   0.666667   \n",
       "\n",
       "   county_class_mean  county_health_problem_mean  county_family_status_mean  \\\n",
       "0           4.938776                    3.877551                   2.489796   \n",
       "1           4.123711                    3.948454                   2.752577   \n",
       "2           4.505263                    3.842105                   2.768421   \n",
       "3           4.464646                    4.090909                   2.696970   \n",
       "4           3.641026                    4.179487                   3.025641   \n",
       "\n",
       "   county_leisure_sum_mean  county_public_service_sum_mean  \\\n",
       "0                36.693878                      575.489796   \n",
       "1                48.628866                      641.494845   \n",
       "2                48.263158                      675.600000   \n",
       "3                42.727273                      631.636364   \n",
       "4                44.846154                      552.051282   \n",
       "\n",
       "   county_trust_sum_mean  income/province  family_income/province  \\\n",
       "0              32.775510         0.323313                0.455792   \n",
       "1              41.268041         0.671001                1.059349   \n",
       "2              33.221053         0.123102                0.083914   \n",
       "3              33.171717         0.239812                0.196602   \n",
       "4              35.487179         0.000000                0.900011   \n",
       "\n",
       "   equity/province  depression/province  floor_area/province  health/province  \\\n",
       "0         1.081563             1.264423             0.507373         0.810478   \n",
       "1         0.923321             0.751614             0.740921         1.264118   \n",
       "2         1.212818             1.370728             1.112116         1.116490   \n",
       "3         1.175869             1.011434             0.685256         1.044980   \n",
       "4         0.604407             0.732824             1.216833         1.324747   \n",
       "\n",
       "   class_10_diff/province  class/province  health_problem/province  \\\n",
       "0                0.000000        0.656133                 0.493665   \n",
       "1                2.063830        1.359284                 1.009978   \n",
       "2                0.971053        1.178161                 1.112283   \n",
       "3                2.178030        1.092325                 1.019504   \n",
       "4                0.000000        0.225000                 1.212121   \n",
       "\n",
       "   family_status/province  leisure_sum/province  public_service_sum/province  \\\n",
       "0                0.736179              0.791338                     0.731146   \n",
       "1                1.461394              0.826373                     1.068323   \n",
       "2                1.150728              0.964566                     1.146752   \n",
       "3                1.082183              0.947645                     1.156354   \n",
       "4                1.099237              0.899930                     0.704401   \n",
       "\n",
       "   trust_sum/province  income/city  family_income/city  equity/city  \\\n",
       "0            0.922703     0.464073            0.639902     1.071429   \n",
       "1            1.041622     0.312421            1.169546     0.889752   \n",
       "2            0.860066     0.306649            0.246326     1.214058   \n",
       "3            1.114783     0.142423            0.117516     1.229814   \n",
       "4            1.412394     0.000000            0.900011     0.604407   \n",
       "\n",
       "   depression/city  floor_area/city  health/city  class_10_diff/city  \\\n",
       "0         1.246280         0.543841     0.821750            0.000000   \n",
       "1         0.751969         0.698285     1.259894            2.122222   \n",
       "2         1.263298         0.994331     1.085714            1.266667   \n",
       "3         0.992481         0.789006     0.982630            2.869565   \n",
       "4         0.732824         1.216833     1.324747            0.000000   \n",
       "\n",
       "   class/city  health_problem/city  family_status/city  leisure_sum/city  \\\n",
       "0    0.671791             0.486212            0.750280          0.823770   \n",
       "1    1.384058             1.009247            1.422719          0.807392   \n",
       "2    1.109813             1.041096            1.083650          0.932388   \n",
       "3    1.119910             0.977778            1.112360          1.006383   \n",
       "4    0.225000             1.212121            1.099237          0.899930   \n",
       "\n",
       "   public_service_sum/city  trust_sum/city  income/county  \\\n",
       "0                 0.744149        0.986511       0.690141   \n",
       "1                 1.044587        1.071913       1.719889   \n",
       "2                 1.108644        0.812738       0.306649   \n",
       "3                 1.203224        1.326431       0.142423   \n",
       "4                 0.704401        1.459235       0.000000   \n",
       "\n",
       "   family_income/county  equity/county  depression/county  floor_area/county  \\\n",
       "0              0.989600       1.166667           1.225000           1.167903   \n",
       "1              1.150775       0.895385           0.757812           0.711476   \n",
       "2              0.246326       1.214058           1.263298           0.994331   \n",
       "3              0.117516       1.229814           0.992481           0.789006   \n",
       "4              0.777762       0.557143           0.754839           1.216252   \n",
       "\n",
       "   health/county  class_10_diff/county  class/county  health_problem/county  \\\n",
       "0       0.859649              0.000000      0.607438               0.515789   \n",
       "1       1.272966              2.179775      1.455000               1.013055   \n",
       "2       1.085714              1.266667      1.109813               1.041096   \n",
       "3       0.982630              2.869565      1.119910               0.977778   \n",
       "4       1.250000              0.000000      0.274648               1.196319   \n",
       "\n",
       "   family_status/county  leisure_sum/county  public_service_sum/county  \\\n",
       "0              0.803279            0.899333                   0.729813   \n",
       "1              1.453184            0.801993                   1.052230   \n",
       "2              1.083650            0.932388                   1.108644   \n",
       "3              1.112360            1.006383                   1.203224   \n",
       "4              0.991525            0.891938                   0.815142   \n",
       "\n",
       "   trust_sum/county  age_income_mean  age_family_income_mean  age_equity_mean  \\\n",
       "0          0.945828     24371.808219            68176.863103         3.061644   \n",
       "1          1.041969     20771.900826            71278.333790         3.157025   \n",
       "2          0.812738     23468.987342            51169.410130         3.025316   \n",
       "3          1.326431     15072.375000           123040.453017         3.348214   \n",
       "4          1.239884     14158.165138            69150.138536         3.082569   \n",
       "\n",
       "   age_depression_mean  age_floor_area_mean  age_health_mean  \\\n",
       "0             3.890411           109.662329         3.534247   \n",
       "1             4.090909           115.446281         4.239669   \n",
       "2             3.721519           123.415190         3.476793   \n",
       "3             3.598214           121.433036         3.133929   \n",
       "4             4.000000           119.192661         4.247706   \n",
       "\n",
       "   age_class_10_diff_mean  age_class_mean  age_health_problem_mean  \\\n",
       "0                0.479452        4.390411                 3.835616   \n",
       "1                1.975207        4.462810                 4.487603   \n",
       "2                0.839662        4.181435                 3.789030   \n",
       "3                0.401786        4.169643                 3.366071   \n",
       "4                1.779817        4.220183                 4.477064   \n",
       "\n",
       "   age_family_status_mean  age_leisure_sum_mean  age_public_service_sum_mean  \\\n",
       "0                2.726027             45.541096                   630.027397   \n",
       "1                2.942149             38.545455                   596.933884   \n",
       "2                2.518987             46.561181                   627.721519   \n",
       "3                2.580357             47.812500                   658.687500   \n",
       "4                2.743119             38.266055                   617.995413   \n",
       "\n",
       "   age_trust_sum_mean  income/age  family_income/age  equity/age  \\\n",
       "0           33.479452    0.820620           0.880064    0.979866   \n",
       "1           36.181818    0.962839           0.561180    0.950262   \n",
       "2           33.594937    0.085219           0.156343    1.322176   \n",
       "3           33.821429    0.425945           0.097529    1.194667   \n",
       "4           39.376147    0.000000           0.959733    0.648810   \n",
       "\n",
       "   depression/age  floor_area/age  health/age  class_10_diff/age  class/age  \\\n",
       "0        1.285211        0.410351    0.848837           0.000000   0.683307   \n",
       "1        0.733333        0.952824    1.179337           1.012552   1.344444   \n",
       "2        1.343537        0.972328    1.150485           1.190955   1.195762   \n",
       "3        1.111663        0.642329    1.276353           4.977778   1.199143   \n",
       "4        0.750000        0.587284    1.177106           0.000000   0.236957   \n",
       "\n",
       "   health_problem/age  family_status/age  leisure_sum/age  \\\n",
       "0            0.521429           0.733668         0.724620   \n",
       "1            0.891344           1.359551         1.011792   \n",
       "2            1.055679           1.190955         0.966470   \n",
       "3            1.188329           1.162630         0.899346   \n",
       "4            1.116803           1.093645         1.045313   \n",
       "\n",
       "   public_service_sum/age  trust_sum/age  \n",
       "0                0.666638       0.925941  \n",
       "1                1.130778       1.188442  \n",
       "2                1.193204       0.803693  \n",
       "3                1.153810       1.300950  \n",
       "4                0.728161       1.117428  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape',data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还应该删去有效样本数很少的特征，例如负值太多的特征或者是缺失值太多的特征，这里我一共删除了包括“目前的最高教育程度”在内的9类特征，得到了最终的263维的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 263)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#272-9=263\n",
    "#删除数值特别少的和之前用过的特征\n",
    "del_list=['id','survey_time','edu_other','invest_other','property_other','join_party','province','city','county']\n",
    "use_feature = [clo for clo in data.columns if clo not in del_list]\n",
    "data.fillna(0,inplace=True) #还是补0\n",
    "train_shape = train.shape[0] #一共的数据量，训练集\n",
    "features = data[use_feature].columns #删除后所有的特征\n",
    "X_train_263 = data[:train_shape][use_feature].values\n",
    "y_train = target\n",
    "X_test_263 = data[train_shape:][use_feature].values\n",
    "X_train_263.shape #最终一种263个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里选择了最重要的49个特征，作为除了以上263维特征外的另外一组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 49)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_fea_49 = ['equity','depression','health','class','family_status','health_problem','class_10_after',\n",
    "           'equity/province','equity/city','equity/county',\n",
    "           'depression/province','depression/city','depression/county',\n",
    "           'health/province','health/city','health/county',\n",
    "           'class/province','class/city','class/county',\n",
    "           'family_status/province','family_status/city','family_status/county',\n",
    "           'family_income/province','family_income/city','family_income/county',\n",
    "           'floor_area/province','floor_area/city','floor_area/county',\n",
    "           'leisure_sum/province','leisure_sum/city','leisure_sum/county',\n",
    "           'public_service_sum/province','public_service_sum/city','public_service_sum/county',\n",
    "           'trust_sum/province','trust_sum/city','trust_sum/county',\n",
    "           'income/m','public_service_sum','class_diff','status_3_before','age_income_mean','age_floor_area_mean',\n",
    "           'weight_jin','height_cm',\n",
    "           'health/age','depression/age','equity/age','leisure_sum/age'\n",
    "          ]\n",
    "train_shape = train.shape[0]\n",
    "X_train_49 = data[:train_shape][imp_fea_49].values\n",
    "X_test_49 = data[train_shape:][imp_fea_49].values\n",
    "X_train_49.shape #最重要的49个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择需要进行onehot编码的离散变量进行one-hot编码，再合成为第三类特征，共383维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 383)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fea = ['survey_type','gender','nationality','edu_status','political','hukou','hukou_loc','work_exper','work_status','work_type',\n",
    "           'work_manage','marital','s_political','s_hukou','s_work_exper','s_work_status','s_work_type','f_political','f_work_14',\n",
    "           'm_political','m_work_14']\n",
    "noc_fea = [clo for clo in use_feature if clo not in cat_fea]\n",
    "\n",
    "onehot_data = data[cat_fea].values\n",
    "enc = preprocessing.OneHotEncoder(categories = 'auto')\n",
    "oh_data=enc.fit_transform(onehot_data).toarray()\n",
    "oh_data.shape #变为onehot编码格式\n",
    "\n",
    "X_train_oh = oh_data[:train_shape,:]\n",
    "X_test_oh = oh_data[train_shape:,:]\n",
    "X_train_oh.shape #其中的训练集\n",
    "\n",
    "X_train_383 = np.column_stack([data[:train_shape][noc_fea].values,X_train_oh])#先是noc，再是cat_fea\n",
    "X_test_383 = np.column_stack([data[train_shape:][noc_fea].values,X_test_oh])\n",
    "X_train_383.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于此，我们构建完成了三种特征工程（训练数据集），其一是上面提取的最重要的49中特征，其中包括健康程度、社会阶级、在同龄人中的收入情况等等特征。其二是扩充后的263维特征（这里可以认为是初始特征）。其三是使用One-hot编码后的特征，这里要使用One-hot进行编码的原因在于，有部分特征为分离值，例如性别中男女，男为1，女为2，我们想使用One-hot将其变为男为0，女为1，来增强机器学习算法的鲁棒性能；再如民族这个特征，原本是1-56这56个数值，如果直接分类会让分类器的鲁棒性变差，所以使用One-hot编码将其变为6个特征进行非零即一的处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征建模\n",
    "\n",
    "首先我们对于原始的263维的特征，使用lightGBM进行处理，这里我们使用5折交叉验证的方法：\n",
    "\n",
    "1.lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.504831\tvalid_1's l2: 0.514779\n",
      "[1000]\ttraining's l2: 0.456617\tvalid_1's l2: 0.478354\n",
      "[1500]\ttraining's l2: 0.430795\tvalid_1's l2: 0.464106\n",
      "[2000]\ttraining's l2: 0.412914\tvalid_1's l2: 0.456932\n",
      "[2500]\ttraining's l2: 0.39852\tvalid_1's l2: 0.452579\n",
      "[3000]\ttraining's l2: 0.38622\tvalid_1's l2: 0.450379\n",
      "[3500]\ttraining's l2: 0.375324\tvalid_1's l2: 0.449067\n",
      "[4000]\ttraining's l2: 0.365297\tvalid_1's l2: 0.448197\n",
      "[4500]\ttraining's l2: 0.356273\tvalid_1's l2: 0.447401\n",
      "[5000]\ttraining's l2: 0.347783\tvalid_1's l2: 0.446872\n",
      "[5500]\ttraining's l2: 0.339751\tvalid_1's l2: 0.446579\n",
      "[6000]\ttraining's l2: 0.332232\tvalid_1's l2: 0.446175\n",
      "[6500]\ttraining's l2: 0.325035\tvalid_1's l2: 0.44604\n",
      "[7000]\ttraining's l2: 0.318044\tvalid_1's l2: 0.445975\n",
      "[7500]\ttraining's l2: 0.311344\tvalid_1's l2: 0.446181\n",
      "Early stopping, best iteration is:\n",
      "[6774]\ttraining's l2: 0.321144\tvalid_1's l2: 0.445938\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.500328\tvalid_1's l2: 0.524323\n",
      "[1000]\ttraining's l2: 0.450724\tvalid_1's l2: 0.492723\n",
      "[1500]\ttraining's l2: 0.423762\tvalid_1's l2: 0.481982\n",
      "[2000]\ttraining's l2: 0.405039\tvalid_1's l2: 0.477319\n",
      "[2500]\ttraining's l2: 0.390525\tvalid_1's l2: 0.475206\n",
      "[3000]\ttraining's l2: 0.378438\tvalid_1's l2: 0.474289\n",
      "[3500]\ttraining's l2: 0.367607\tvalid_1's l2: 0.473805\n",
      "[4000]\ttraining's l2: 0.357867\tvalid_1's l2: 0.473342\n",
      "[4500]\ttraining's l2: 0.348735\tvalid_1's l2: 0.473352\n",
      "Early stopping, best iteration is:\n",
      "[4101]\ttraining's l2: 0.355934\tvalid_1's l2: 0.473254\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.503294\tvalid_1's l2: 0.518843\n",
      "[1000]\ttraining's l2: 0.453933\tvalid_1's l2: 0.485183\n",
      "[1500]\ttraining's l2: 0.42778\tvalid_1's l2: 0.472082\n",
      "[2000]\ttraining's l2: 0.409914\tvalid_1's l2: 0.465696\n",
      "[2500]\ttraining's l2: 0.395446\tvalid_1's l2: 0.461917\n",
      "[3000]\ttraining's l2: 0.38337\tvalid_1's l2: 0.459031\n",
      "[3500]\ttraining's l2: 0.37265\tvalid_1's l2: 0.457705\n",
      "[4000]\ttraining's l2: 0.362945\tvalid_1's l2: 0.456521\n",
      "[4500]\ttraining's l2: 0.353795\tvalid_1's l2: 0.455834\n",
      "[5000]\ttraining's l2: 0.345338\tvalid_1's l2: 0.455261\n",
      "[5500]\ttraining's l2: 0.337172\tvalid_1's l2: 0.454958\n",
      "[6000]\ttraining's l2: 0.32957\tvalid_1's l2: 0.454812\n",
      "[6500]\ttraining's l2: 0.322294\tvalid_1's l2: 0.45485\n",
      "[7000]\ttraining's l2: 0.315272\tvalid_1's l2: 0.45511\n",
      "Early stopping, best iteration is:\n",
      "[6203]\ttraining's l2: 0.326545\tvalid_1's l2: 0.454691\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.507021\tvalid_1's l2: 0.503616\n",
      "[1000]\ttraining's l2: 0.459037\tvalid_1's l2: 0.464049\n",
      "[1500]\ttraining's l2: 0.433348\tvalid_1's l2: 0.448351\n",
      "[2000]\ttraining's l2: 0.415569\tvalid_1's l2: 0.440324\n",
      "[2500]\ttraining's l2: 0.401339\tvalid_1's l2: 0.435838\n",
      "[3000]\ttraining's l2: 0.389172\tvalid_1's l2: 0.432713\n",
      "[3500]\ttraining's l2: 0.378452\tvalid_1's l2: 0.430553\n",
      "[4000]\ttraining's l2: 0.368621\tvalid_1's l2: 0.429648\n",
      "[4500]\ttraining's l2: 0.359415\tvalid_1's l2: 0.428398\n",
      "[5000]\ttraining's l2: 0.350917\tvalid_1's l2: 0.427835\n",
      "[5500]\ttraining's l2: 0.342856\tvalid_1's l2: 0.427405\n",
      "[6000]\ttraining's l2: 0.335175\tvalid_1's l2: 0.427183\n",
      "[6500]\ttraining's l2: 0.327814\tvalid_1's l2: 0.427106\n",
      "[7000]\ttraining's l2: 0.3208\tvalid_1's l2: 0.427473\n",
      "Early stopping, best iteration is:\n",
      "[6559]\ttraining's l2: 0.326986\tvalid_1's l2: 0.427003\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.499983\tvalid_1's l2: 0.530981\n",
      "[1000]\ttraining's l2: 0.451967\tvalid_1's l2: 0.497488\n",
      "[1500]\ttraining's l2: 0.426654\tvalid_1's l2: 0.482417\n",
      "[2000]\ttraining's l2: 0.409201\tvalid_1's l2: 0.473451\n",
      "[2500]\ttraining's l2: 0.395494\tvalid_1's l2: 0.468592\n",
      "[3000]\ttraining's l2: 0.383659\tvalid_1's l2: 0.465795\n",
      "[3500]\ttraining's l2: 0.373248\tvalid_1's l2: 0.463553\n",
      "[4000]\ttraining's l2: 0.363564\tvalid_1's l2: 0.462202\n",
      "[4500]\ttraining's l2: 0.354694\tvalid_1's l2: 0.461171\n",
      "[5000]\ttraining's l2: 0.346382\tvalid_1's l2: 0.46009\n",
      "[5500]\ttraining's l2: 0.338451\tvalid_1's l2: 0.459071\n",
      "[6000]\ttraining's l2: 0.330766\tvalid_1's l2: 0.458635\n",
      "[6500]\ttraining's l2: 0.323561\tvalid_1's l2: 0.458323\n",
      "[7000]\ttraining's l2: 0.316701\tvalid_1's l2: 0.458129\n",
      "[7500]\ttraining's l2: 0.310136\tvalid_1's l2: 0.457841\n",
      "[8000]\ttraining's l2: 0.303898\tvalid_1's l2: 0.457229\n",
      "[8500]\ttraining's l2: 0.297673\tvalid_1's l2: 0.457044\n",
      "[9000]\ttraining's l2: 0.29169\tvalid_1's l2: 0.457072\n",
      "[9500]\ttraining's l2: 0.285967\tvalid_1's l2: 0.456687\n",
      "[10000]\ttraining's l2: 0.280293\tvalid_1's l2: 0.45661\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l2: 0.280293\tvalid_1's l2: 0.45661\n",
      "CV score: 0.45150218\n"
     ]
    }
   ],
   "source": [
    "##### lgb_263 #\n",
    "#lightGBM决策树\n",
    "lgb_263_param = {\n",
    "'num_leaves': 7, \n",
    "'min_data_in_leaf': 20, #叶子可能具有的最小记录数\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.003,\n",
    "\"boosting\": \"gbdt\", #用gbdt算法\n",
    "\"feature_fraction\": 0.18, #例如 0.18时，意味着在每次迭代中随机选择18％的参数来建树\n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.55, #每次迭代时用的数据比例\n",
    "\"bagging_seed\": 14,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l1\": 0.1005,\n",
    "\"lambda_l2\": 0.1996, \n",
    "\"verbosity\": -1}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)   #交叉切分：5\n",
    "oof_lgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_lgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_263[val_idx], y_train[val_idx])#train:val=4:1\n",
    "\n",
    "    num_round = 10000\n",
    "    lgb_263 = lgb.train(lgb_263_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 800)\n",
    "    oof_lgb_263[val_idx] = lgb_263.predict(X_train_263[val_idx], num_iteration=lgb_263.best_iteration)\n",
    "    predictions_lgb_263 += lgb_263.predict(X_test_263, num_iteration=lgb_263.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我使用已经训练完的lightGBM的模型进行特征重要性的判断以及可视化，从结果我们可以看出，排在重要性第一位的是health/age，就是同龄人中的健康程度，与我们主观的看法基本一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x2016 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------特征重要性\n",
    "\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "df = pd.DataFrame(data[use_feature].columns.tolist(), columns=['feature'])\n",
    "df['importance']=list(lgb_263.feature_importance())\n",
    "df = df.sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df.head(50))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面，我们使用常见的机器学习方法，对于263维特征进行建模：\n",
    "\n",
    "2.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[20:31:35] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:31:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40424\tvalid_data-rmse:3.38328\n",
      "[500]\ttrain-rmse:0.40805\tvalid_data-rmse:0.70588\n",
      "[1000]\ttrain-rmse:0.27046\tvalid_data-rmse:0.70760\n",
      "[1263]\ttrain-rmse:0.21887\tvalid_data-rmse:0.70932\n",
      "fold n°2\n",
      "[20:31:52] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:31:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39810\tvalid_data-rmse:3.40789\n",
      "[500]\ttrain-rmse:0.40719\tvalid_data-rmse:0.69456\n",
      "[1000]\ttrain-rmse:0.27402\tvalid_data-rmse:0.69501\n",
      "[1151]\ttrain-rmse:0.24326\tvalid_data-rmse:0.69493\n",
      "fold n°3\n",
      "[20:32:08] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:32:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40184\tvalid_data-rmse:3.39295\n",
      "[500]\ttrain-rmse:0.41334\tvalid_data-rmse:0.66250\n",
      "[932]\ttrain-rmse:0.29183\tvalid_data-rmse:0.66479\n",
      "fold n°4\n",
      "[20:32:23] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:32:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40239\tvalid_data-rmse:3.39012\n",
      "[500]\ttrain-rmse:0.41021\tvalid_data-rmse:0.66575\n",
      "[1000]\ttrain-rmse:0.27491\tvalid_data-rmse:0.66431\n",
      "[1463]\ttrain-rmse:0.18999\tvalid_data-rmse:0.66557\n",
      "fold n°5\n",
      "[20:32:44] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:32:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39343\tvalid_data-rmse:3.42628\n",
      "[500]\ttrain-rmse:0.41705\tvalid_data-rmse:0.64937\n",
      "[1000]\ttrain-rmse:0.27907\tvalid_data-rmse:0.64914\n",
      "[1197]\ttrain-rmse:0.23932\tvalid_data-rmse:0.64986\n",
      "CV score: 0.45559330\n"
     ]
    }
   ],
   "source": [
    "##### xgb_263\n",
    "#xgboost\n",
    "xgb_263_params = {'eta': 0.02,  #lr\n",
    "              'max_depth': 6,  \n",
    "              'min_child_weight':3,#最小叶子节点样本权重和\n",
    "              'gamma':0, #指定节点分裂所需的最小损失函数下降值。\n",
    "              'subsample': 0.7,  #控制对于每棵树，随机采样的比例\n",
    "              'colsample_bytree': 0.3,  #用来控制每棵随机采样的列数的占比 (每一列是一个特征)。\n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_xgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_263[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_263 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_263_params)\n",
    "    oof_xgb_263[val_idx] = xgb_263.predict(xgb.DMatrix(X_train_263[val_idx]), ntree_limit=xgb_263.best_ntree_limit)\n",
    "    predictions_xgb_263 += xgb_263.predict(xgb.DMatrix(X_test_263), ntree_limit=xgb_263.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RandomForestRegressor随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   24.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   24.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   24.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   24.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   26.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.47838998\n"
     ]
    }
   ],
   "source": [
    "#RandomForestRegressor随机森林\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_rfr_263 = np.zeros(len(X_train_263))\n",
    "predictions_rfr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    rfr_263 = rfr(n_estimators=1600,max_depth=9, min_samples_leaf=9, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.25,verbose=1,n_jobs=-1)\n",
    "    #verbose = 0 为不在标准输出流输出日志信息\n",
    "#verbose = 1 为输出进度条记录\n",
    "#verbose = 2 为每个epoch输出一行记录\n",
    "    rfr_263.fit(tr_x,tr_y)\n",
    "    oof_rfr_263[val_idx] = rfr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_rfr_263 += rfr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_rfr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6482           0.0034           25.34s\n",
      "         2           0.6537           0.0033           24.90s\n",
      "         3           0.6617           0.0032           24.80s\n",
      "         4           0.6416           0.0032           24.75s\n",
      "         5           0.6499           0.0031           24.72s\n",
      "         6           0.6371           0.0029           24.63s\n",
      "         7           0.6440           0.0029           24.72s\n",
      "         8           0.6333           0.0030           24.79s\n",
      "         9           0.6137           0.0034           24.66s\n",
      "        10           0.6091           0.0027           24.80s\n",
      "        20           0.5881           0.0025           24.02s\n",
      "        30           0.5759           0.0024           23.28s\n",
      "        40           0.5442           0.0016           22.44s\n",
      "        50           0.5040           0.0012           21.71s\n",
      "        60           0.4940           0.0011           20.91s\n",
      "        70           0.4714           0.0012           20.11s\n",
      "        80           0.4644           0.0007           19.32s\n",
      "        90           0.4392           0.0007           18.48s\n",
      "       100           0.4321           0.0005           17.67s\n",
      "       200           0.3487           0.0002           10.57s\n",
      "       300           0.2941          -0.0000            4.85s\n",
      "       400           0.2700           0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6598           0.0034           19.97s\n",
      "         2           0.6540           0.0032           20.20s\n",
      "         3           0.6546           0.0033           20.20s\n",
      "         4           0.6481           0.0032           20.04s\n",
      "         5           0.6452           0.0031           19.67s\n",
      "         6           0.6392           0.0032           19.82s\n",
      "         7           0.6278           0.0031           19.81s\n",
      "         8           0.6434           0.0029           19.73s\n",
      "         9           0.6312           0.0027           19.69s\n",
      "        10           0.6285           0.0026           19.52s\n",
      "        20           0.5822           0.0025           18.71s\n",
      "        30           0.5561           0.0020           18.06s\n",
      "        40           0.5325           0.0016           17.41s\n",
      "        50           0.5070           0.0014           16.77s\n",
      "        60           0.4921           0.0012           16.24s\n",
      "        70           0.4984           0.0009           15.68s\n",
      "        80           0.4660           0.0009           15.09s\n",
      "        90           0.4312           0.0004           14.54s\n",
      "       100           0.4270           0.0006           13.96s\n",
      "       200           0.3563           0.0001            8.87s\n",
      "       300           0.3101           0.0000            4.23s\n",
      "       400           0.2629           0.0000            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6653           0.0034           20.03s\n",
      "         2           0.6595           0.0031           20.30s\n",
      "         3           0.6252           0.0033           20.27s\n",
      "         4           0.6452           0.0031           20.31s\n",
      "         5           0.6550           0.0032           20.13s\n",
      "         6           0.6345           0.0031           19.97s\n",
      "         7           0.6472           0.0028           19.92s\n",
      "         8           0.6317           0.0028           19.72s\n",
      "         9           0.6420           0.0028           19.67s\n",
      "        10           0.6165           0.0028           19.69s\n",
      "        20           0.5978           0.0022           18.82s\n",
      "        30           0.5556           0.0018           18.13s\n",
      "        40           0.5468           0.0017           17.46s\n",
      "        50           0.5216           0.0013           16.98s\n",
      "        60           0.4896           0.0012           16.43s\n",
      "        70           0.4662           0.0010           15.91s\n",
      "        80           0.4586           0.0009           15.30s\n",
      "        90           0.4268           0.0009           14.73s\n",
      "       100           0.4290           0.0006           14.19s\n",
      "       200           0.3582           0.0000            8.91s\n",
      "       300           0.3090           0.0001            4.23s\n",
      "       400           0.2807          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6462           0.0033           18.23s\n",
      "         2           0.6559           0.0036           18.78s\n",
      "         3           0.6304           0.0031           19.61s\n",
      "         4           0.6646           0.0029           19.83s\n",
      "         5           0.6329           0.0031           19.46s\n",
      "         6           0.6444           0.0026           19.41s\n",
      "         7           0.6546           0.0029           19.46s\n",
      "         8           0.6214           0.0028           19.51s\n",
      "         9           0.6312           0.0026           19.47s\n",
      "        10           0.6318           0.0028           19.38s\n",
      "        20           0.5806           0.0024           18.85s\n",
      "        30           0.5512           0.0019           18.12s\n",
      "        40           0.5138           0.0019           17.52s\n",
      "        50           0.5233           0.0015           16.96s\n",
      "        60           0.4967           0.0012           16.41s\n",
      "        70           0.4777           0.0008           15.88s\n",
      "        80           0.4575           0.0008           15.31s\n",
      "        90           0.4482           0.0007           14.76s\n",
      "       100           0.4328           0.0005           14.20s\n",
      "       200           0.3479           0.0002            8.97s\n",
      "       300           0.3032           0.0000            4.29s\n",
      "       400           0.2543          -0.0000            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6699           0.0038           20.86s\n",
      "         2           0.6481           0.0035           20.21s\n",
      "         3           0.6668           0.0035           20.27s\n",
      "         4           0.6468           0.0034           20.14s\n",
      "         5           0.6619           0.0034           20.13s\n",
      "         6           0.6445           0.0026           20.08s\n",
      "         7           0.6242           0.0037           20.16s\n",
      "         8           0.6168           0.0031           20.11s\n",
      "         9           0.6188           0.0031           19.93s\n",
      "        10           0.6204           0.0032           19.78s\n",
      "        20           0.5929           0.0024           18.86s\n",
      "        30           0.5588           0.0022           18.13s\n",
      "        40           0.5319           0.0017           17.56s\n",
      "        50           0.5131           0.0015           17.02s\n",
      "        60           0.4778           0.0013           16.42s\n",
      "        70           0.4857           0.0009           15.85s\n",
      "        80           0.4551           0.0009           15.31s\n",
      "        90           0.4307           0.0007           14.75s\n",
      "       100           0.4279           0.0007           14.19s\n",
      "       200           0.3367           0.0001            8.92s\n",
      "       300           0.2957           0.0000            4.25s\n",
      "       400           0.2606          -0.0000            0.00s\n",
      "CV score: 0.45861147\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingRegressor梯度提升决策树\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_263 = np.zeros(train_shape)\n",
    "predictions_gbr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_263 = gbr(n_estimators=400, learning_rate=0.01,subsample=0.65,max_depth=7, min_samples_leaf=20,\n",
    "            max_features=0.22,verbose=1)\n",
    "    gbr_263.fit(tr_x,tr_y)\n",
    "    oof_gbr_263[val_idx] = gbr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_gbr_263 += gbr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ExtraTreesRegressor 极端随机森林回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    9.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.48641226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesRegressor 极端随机森林回归\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_etr_263 = np.zeros(train_shape)\n",
    "predictions_etr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    etr_263 = etr(n_estimators=1000,max_depth=8, min_samples_leaf=12, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.4,verbose=1,n_jobs=-1)\n",
    "    etr_263.fit(tr_x,tr_y)\n",
    "    oof_etr_263[val_idx] = etr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_etr_263 += etr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_etr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTreesRegressor 与随机森林的区别\n",
    "* 随机森林应用的是Bagging模型，而ET是使用所有的训练样本得到每棵决策树，也就是每棵决策树应用的是相同的全部训练样本\n",
    "* 随机森林是在一个随机子集内得到最佳分叉属性，而ET是完全随机的得到分叉值，从而实现对决策树进行分叉的。\n",
    "模型的方差相对于RF进一步减少，但是bias相对于RF进一步增大。在某些时候，Extra tree的泛化能力比RF更好.\n",
    "参考链接：[从随机森林到极端随机森林，再到深度森林](https://blog.csdn.net/andy_shenzl/article/details/106861385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上5种模型的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4492349899187444"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack2 = np.vstack([oof_lgb_263,oof_xgb_263,oof_gbr_263,oof_rfr_263,oof_etr_263]).transpose()\n",
    "# transpose()函数的作用就是调换x,y,z的位置,也就是数组的索引值\n",
    "test_stack2 = np.vstack([predictions_lgb_263, predictions_xgb_263,predictions_gbr_263,predictions_rfr_263,predictions_etr_263]).transpose()\n",
    "\n",
    "#交叉验证:5折，重复2次\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack2 = np.zeros(train_stack2.shape[0])\n",
    "predictions_lr2 = np.zeros(test_stack2.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack2,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack2[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack2[val_idx], target.iloc[val_idx].values\n",
    "    #Kernel Ridge Regression\n",
    "    lr2 = kr()\n",
    "    lr2.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack2[val_idx] = lr2.predict(val_data)\n",
    "    predictions_lr2 += lr2.predict(test_stack2) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们对于49维的数据进行与上述263维数据相同的操作\n",
    "\n",
    "1.lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.476379\tvalid_1's l2: 0.477946\n",
      "[2000]\ttraining's l2: 0.436394\tvalid_1's l2: 0.451203\n",
      "[3000]\ttraining's l2: 0.413986\tvalid_1's l2: 0.444246\n",
      "[4000]\ttraining's l2: 0.395922\tvalid_1's l2: 0.441219\n",
      "[5000]\ttraining's l2: 0.380525\tvalid_1's l2: 0.439789\n",
      "[6000]\ttraining's l2: 0.366601\tvalid_1's l2: 0.439056\n",
      "[7000]\ttraining's l2: 0.35393\tvalid_1's l2: 0.438707\n",
      "[8000]\ttraining's l2: 0.342405\tvalid_1's l2: 0.439183\n",
      "Early stopping, best iteration is:\n",
      "[7155]\ttraining's l2: 0.352046\tvalid_1's l2: 0.438672\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.467189\tvalid_1's l2: 0.504924\n",
      "[2000]\ttraining's l2: 0.42594\tvalid_1's l2: 0.49036\n",
      "[3000]\ttraining's l2: 0.403413\tvalid_1's l2: 0.488121\n",
      "[4000]\ttraining's l2: 0.386029\tvalid_1's l2: 0.48756\n",
      "[5000]\ttraining's l2: 0.371207\tvalid_1's l2: 0.486987\n",
      "[6000]\ttraining's l2: 0.35795\tvalid_1's l2: 0.486776\n",
      "[7000]\ttraining's l2: 0.345735\tvalid_1's l2: 0.487085\n",
      "Early stopping, best iteration is:\n",
      "[6082]\ttraining's l2: 0.356929\tvalid_1's l2: 0.486682\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.470713\tvalid_1's l2: 0.497604\n",
      "[2000]\ttraining's l2: 0.430719\tvalid_1's l2: 0.477024\n",
      "[3000]\ttraining's l2: 0.40834\tvalid_1's l2: 0.470886\n",
      "[4000]\ttraining's l2: 0.390636\tvalid_1's l2: 0.468189\n",
      "[5000]\ttraining's l2: 0.375266\tvalid_1's l2: 0.466879\n",
      "[6000]\ttraining's l2: 0.361347\tvalid_1's l2: 0.466204\n",
      "[7000]\ttraining's l2: 0.348823\tvalid_1's l2: 0.466285\n",
      "Early stopping, best iteration is:\n",
      "[6167]\ttraining's l2: 0.359242\tvalid_1's l2: 0.466073\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.469247\tvalid_1's l2: 0.496795\n",
      "[2000]\ttraining's l2: 0.428654\tvalid_1's l2: 0.477241\n",
      "[3000]\ttraining's l2: 0.406188\tvalid_1's l2: 0.47311\n",
      "[4000]\ttraining's l2: 0.38824\tvalid_1's l2: 0.471653\n",
      "[5000]\ttraining's l2: 0.372674\tvalid_1's l2: 0.471472\n",
      "[6000]\ttraining's l2: 0.358735\tvalid_1's l2: 0.47137\n",
      "Early stopping, best iteration is:\n",
      "[5268]\ttraining's l2: 0.368859\tvalid_1's l2: 0.471349\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.468059\tvalid_1's l2: 0.506138\n",
      "[2000]\ttraining's l2: 0.428456\tvalid_1's l2: 0.486929\n",
      "[3000]\ttraining's l2: 0.40654\tvalid_1's l2: 0.481997\n",
      "[4000]\ttraining's l2: 0.389078\tvalid_1's l2: 0.479317\n",
      "[5000]\ttraining's l2: 0.373973\tvalid_1's l2: 0.47793\n",
      "[6000]\ttraining's l2: 0.360309\tvalid_1's l2: 0.476776\n",
      "[7000]\ttraining's l2: 0.347771\tvalid_1's l2: 0.476343\n",
      "Early stopping, best iteration is:\n",
      "[6655]\ttraining's l2: 0.352026\tvalid_1's l2: 0.47626\n",
      "CV score: 0.46780261\n"
     ]
    }
   ],
   "source": [
    "##### lgb_49\n",
    "lgb_49_param = {\n",
    "'num_leaves': 9,\n",
    "'min_data_in_leaf': 23,\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.002,\n",
    "\"boosting\": \"gbdt\",\n",
    "\"feature_fraction\": 0.45,\n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.65,\n",
    "\"bagging_seed\": 15,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l2\": 0.2, \n",
    "\"verbosity\": -1}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)   \n",
    "oof_lgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_lgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 12000\n",
    "    lgb_49 = lgb.train(lgb_49_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
    "    oof_lgb_49[val_idx] = lgb_49.predict(X_train_49[val_idx], num_iteration=lgb_49.best_iteration)\n",
    "    predictions_lgb_49 += lgb_49.predict(X_test_49, num_iteration=lgb_49.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[20:38:24] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:38:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40432\tvalid_data-rmse:3.38307\n",
      "[500]\ttrain-rmse:0.52770\tvalid_data-rmse:0.72110\n",
      "[1000]\ttrain-rmse:0.43563\tvalid_data-rmse:0.72245\n",
      "[1290]\ttrain-rmse:0.39008\tvalid_data-rmse:0.72513\n",
      "fold n°2\n",
      "[20:38:31] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:38:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39816\tvalid_data-rmse:3.40784\n",
      "[500]\ttrain-rmse:0.52871\tvalid_data-rmse:0.70336\n",
      "[1000]\ttrain-rmse:0.43793\tvalid_data-rmse:0.70446\n",
      "[1354]\ttrain-rmse:0.38671\tvalid_data-rmse:0.70693\n",
      "fold n°3\n",
      "[20:38:39] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:38:39] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40183\tvalid_data-rmse:3.39291\n",
      "[500]\ttrain-rmse:0.53169\tvalid_data-rmse:0.66896\n",
      "[1000]\ttrain-rmse:0.44129\tvalid_data-rmse:0.67058\n",
      "[1052]\ttrain-rmse:0.43246\tvalid_data-rmse:0.67004\n",
      "fold n°4\n",
      "[20:38:44] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:38:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40242\tvalid_data-rmse:3.39014\n",
      "[500]\ttrain-rmse:0.53218\tvalid_data-rmse:0.67783\n",
      "[1000]\ttrain-rmse:0.44361\tvalid_data-rmse:0.67978\n",
      "[1166]\ttrain-rmse:0.41769\tvalid_data-rmse:0.68077\n",
      "fold n°5\n",
      "[20:38:49] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:38:49] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39347\tvalid_data-rmse:3.42619\n",
      "[500]\ttrain-rmse:0.53565\tvalid_data-rmse:0.66150\n",
      "[1000]\ttrain-rmse:0.44204\tvalid_data-rmse:0.66241\n",
      "[1347]\ttrain-rmse:0.39023\tvalid_data-rmse:0.66474\n",
      "CV score: 0.47102844\n"
     ]
    }
   ],
   "source": [
    "##### xgb_49\n",
    "xgb_49_params = {'eta': 0.02, \n",
    "              'max_depth': 5, \n",
    "              'min_child_weight':3,\n",
    "              'gamma':0,\n",
    "              'subsample': 0.7, \n",
    "              'colsample_bytree': 0.35, \n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_xgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_49 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_49_params)\n",
    "    oof_xgb_49[val_idx] = xgb_49.predict(xgb.DMatrix(X_train_49[val_idx]), ntree_limit=xgb_49.best_ntree_limit)\n",
    "    predictions_xgb_49 += xgb_49.predict(xgb.DMatrix(X_test_49), ntree_limit=xgb_49.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6773           0.0034           12.75s\n",
      "         2           0.6736           0.0030           12.63s\n",
      "         3           0.6529           0.0034           12.41s\n",
      "         4           0.6484           0.0031           12.21s\n",
      "         5           0.6496           0.0030           12.32s\n",
      "         6           0.6520           0.0028           12.29s\n",
      "         7           0.6418           0.0028           12.19s\n",
      "         8           0.6421           0.0028           11.99s\n",
      "         9           0.6403           0.0026           11.76s\n",
      "        10           0.6302           0.0027           11.60s\n",
      "        20           0.6010           0.0024           10.33s\n",
      "        30           0.5921           0.0018            9.79s\n",
      "        40           0.5546           0.0016            9.37s\n",
      "        50           0.5448           0.0013            9.00s\n",
      "        60           0.5083           0.0012            8.73s\n",
      "        70           0.4929           0.0009            8.49s\n",
      "        80           0.4959           0.0006            8.24s\n",
      "        90           0.4705           0.0009            8.00s\n",
      "       100           0.4607           0.0006            7.78s\n",
      "       200           0.3889           0.0000            5.87s\n",
      "       300           0.3620           0.0000            4.12s\n",
      "       400           0.3409          -0.0000            2.60s\n",
      "       500           0.3305          -0.0001            1.25s\n",
      "       600           0.3183          -0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6593           0.0032            7.95s\n",
      "         2           0.6658           0.0032            8.05s\n",
      "         3           0.6583           0.0033            8.18s\n",
      "         4           0.6531           0.0030            8.24s\n",
      "         5           0.6243           0.0029            8.16s\n",
      "         6           0.6366           0.0030            8.21s\n",
      "         7           0.6439           0.0033            8.22s\n",
      "         8           0.6276           0.0030            8.23s\n",
      "         9           0.6432           0.0025            8.19s\n",
      "        10           0.6344           0.0026            8.22s\n",
      "        20           0.5918           0.0022            8.03s\n",
      "        30           0.5639           0.0018            7.83s\n",
      "        40           0.5395           0.0017            7.62s\n",
      "        50           0.5286           0.0014            7.41s\n",
      "        60           0.5129           0.0011            7.21s\n",
      "        70           0.4883           0.0010            7.04s\n",
      "        80           0.4752           0.0008            6.86s\n",
      "        90           0.4721           0.0005            6.70s\n",
      "       100           0.4582           0.0006            6.57s\n",
      "       200           0.4042           0.0001            4.99s\n",
      "       300           0.3696          -0.0001            3.56s\n",
      "       400           0.3498          -0.0000            2.26s\n",
      "       500           0.3269          -0.0000            1.09s\n",
      "       600           0.3195          -0.0001            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6870           0.0035            8.00s\n",
      "         2           0.6626           0.0028            8.13s\n",
      "         3           0.6807           0.0028            8.07s\n",
      "         4           0.6546           0.0029            8.07s\n",
      "         5           0.6601           0.0030            8.02s\n",
      "         6           0.6511           0.0028            8.04s\n",
      "         7           0.6477           0.0030            8.05s\n",
      "         8           0.6300           0.0030            8.02s\n",
      "         9           0.6371           0.0028            8.07s\n",
      "        10           0.6339           0.0028            8.11s\n",
      "        20           0.6011           0.0022            7.60s\n",
      "        30           0.5781           0.0020            7.41s\n",
      "        40           0.5493           0.0016            7.23s\n",
      "        50           0.5314           0.0012            7.09s\n",
      "        60           0.5103           0.0011            6.89s\n",
      "        70           0.4942           0.0010            6.75s\n",
      "        80           0.4864           0.0008            6.61s\n",
      "        90           0.4658           0.0006            6.50s\n",
      "       100           0.4618           0.0005            6.34s\n",
      "       200           0.4006           0.0001            4.80s\n",
      "       300           0.3703           0.0000            3.42s\n",
      "       400           0.3470          -0.0001            2.20s\n",
      "       500           0.3384          -0.0000            1.06s\n",
      "       600           0.3169          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6710           0.0029            7.93s\n",
      "         2           0.6437           0.0037            8.05s\n",
      "         3           0.6648           0.0035            8.11s\n",
      "         4           0.6396           0.0035            8.08s\n",
      "         5           0.6480           0.0031            8.02s\n",
      "         6           0.6390           0.0030            7.98s\n",
      "         7           0.6584           0.0029            8.00s\n",
      "         8           0.6362           0.0029            7.94s\n",
      "         9           0.6239           0.0034            7.99s\n",
      "        10           0.6394           0.0028            7.99s\n",
      "        20           0.6107           0.0022            7.61s\n",
      "        30           0.5944           0.0019            7.45s\n",
      "        40           0.5665           0.0015            7.25s\n",
      "        50           0.5182           0.0013            7.13s\n",
      "        60           0.5018           0.0012            6.97s\n",
      "        70           0.4987           0.0009            6.93s\n",
      "        80           0.4986           0.0007            6.76s\n",
      "        90           0.4728           0.0006            6.59s\n",
      "       100           0.4535           0.0005            6.41s\n",
      "       200           0.3933           0.0001            4.89s\n",
      "       300           0.3617           0.0000            3.55s\n",
      "       400           0.3448          -0.0001            2.26s\n",
      "       500           0.3237          -0.0000            1.09s\n",
      "       600           0.3115           0.0000            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6578           0.0036            8.20s\n",
      "         2           0.6748           0.0033            8.13s\n",
      "         3           0.6733           0.0032            8.16s\n",
      "         4           0.6501           0.0032            8.02s\n",
      "         5           0.6540           0.0030            8.05s\n",
      "         6           0.6576           0.0030            8.39s\n",
      "         7           0.6518           0.0032            8.40s\n",
      "         8           0.6311           0.0029            8.34s\n",
      "         9           0.6355           0.0028            8.30s\n",
      "        10           0.6432           0.0031            8.26s\n",
      "        20           0.5990           0.0025            7.71s\n",
      "        30           0.5686           0.0021            7.48s\n",
      "        40           0.5368           0.0016            7.24s\n",
      "        50           0.5330           0.0014            7.10s\n",
      "        60           0.5174           0.0011            6.93s\n",
      "        70           0.4904           0.0010            6.77s\n",
      "        80           0.4866           0.0007            6.61s\n",
      "        90           0.4607           0.0008            6.47s\n",
      "       100           0.4522           0.0006            6.31s\n",
      "       200           0.3850           0.0000            4.82s\n",
      "       300           0.3534          -0.0000            3.45s\n",
      "       400           0.3337          -0.0001            2.22s\n",
      "       500           0.3260          -0.0001            1.07s\n",
      "       600           0.3122          -0.0001            0.00s\n",
      "CV score: 0.47042587\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_49 = np.zeros(train_shape)\n",
    "predictions_gbr_49 = np.zeros(len(X_test_49))\n",
    "#GradientBoostingRegressor梯度提升决策树\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_49 = gbr(n_estimators=600, learning_rate=0.01,subsample=0.65,max_depth=6, min_samples_leaf=20,\n",
    "            max_features=0.35,verbose=1)\n",
    "    gbr_49.fit(tr_x,tr_y)\n",
    "    oof_gbr_49[val_idx] = gbr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_gbr_49 += gbr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上3种模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4665538958844673"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack3 = np.vstack([oof_lgb_49,oof_xgb_49,oof_gbr_49]).transpose()\n",
    "test_stack3 = np.vstack([predictions_lgb_49, predictions_xgb_49,predictions_gbr_49]).transpose()\n",
    "#\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack3 = np.zeros(train_stack3.shape[0])\n",
    "predictions_lr3 = np.zeros(test_stack3.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack3,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack3[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack3[val_idx], target.iloc[val_idx].values\n",
    "        #Kernel Ridge Regression\n",
    "    lr3 = kr()\n",
    "    lr3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack3[val_idx] = lr3.predict(val_data)\n",
    "    predictions_lr3 += lr3.predict(test_stack3) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们对于383维的数据进行与上述263以及49维数据相同的操作\n",
    "\n",
    "1. Kernel Ridge Regression 基于核的岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.51412085\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_383 = np.zeros(train_shape)\n",
    "predictions_kr_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #Kernel Ridge Regression 岭回归\n",
    "    kr_383 = kr()\n",
    "    kr_383.fit(tr_x,tr_y)\n",
    "    oof_kr_383[val_idx] = kr_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_kr_383 += kr_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核岭回归: 计算成本要比标准线性回归高，要得到模型的系数向量需要使用矩阵求逆操作;在需要进行非线性拟合或者属性比训练样例多的情况下，使用核岭回归还是有优势的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用普通岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48687670\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_383 = np.zeros(train_shape)\n",
    "predictions_ridge_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #使用岭回归\n",
    "    ridge_383 = Ridge(alpha=1200)\n",
    "    ridge_383.fit(tr_x,tr_y)\n",
    "    oof_ridge_383[val_idx] = ridge_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_ridge_383 += ridge_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53296555\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_383 = np.zeros(train_shape)\n",
    "predictions_en_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #ElasticNet 弹性网络\n",
    "    en_383 = en(alpha=1.0,l1_ratio=0.06)\n",
    "    en_383.fit(tr_x,tr_y)\n",
    "    oof_en_383[val_idx] = en_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_en_383 += en_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 使用BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48717350\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_383 = np.zeros(train_shape)\n",
    "predictions_br_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #BayesianRidge 贝叶斯回归\n",
    "    br_383 = br()\n",
    "    br_383.fit(tr_x,tr_y)\n",
    "    oof_br_383[val_idx] = br_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_br_383 += br_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先学习贝叶斯线性回归：https://blog.csdn.net/daunxx/article/details/51725086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上4种模型的基于383个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4878208624815748"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack1 = np.vstack([oof_br_383,oof_kr_383,oof_en_383,oof_ridge_383]).transpose()\n",
    "test_stack1 = np.vstack([predictions_br_383, predictions_kr_383,predictions_en_383,predictions_ridge_383]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack1 = np.zeros(train_stack1.shape[0])\n",
    "predictions_lr1 = np.zeros(test_stack1.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack1,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack1[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack1[val_idx], target.iloc[val_idx].values\n",
    "    # LinearRegression简单的线性回归\n",
    "    lr1 = lr()\n",
    "    lr1.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack1[val_idx] = lr1.predict(val_data)\n",
    "    predictions_lr1 += lr1.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于49维的特征是最重要的特征，所以这里考虑增加更多的模型进行49维特征的数据的构建工作。\n",
    "1. KernelRidge 核岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.50254410\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_49 = np.zeros(train_shape)\n",
    "predictions_kr_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    kr_49 = kr()\n",
    "    kr_49.fit(tr_x,tr_y)\n",
    "    oof_kr_49[val_idx] = kr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_kr_49 += kr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ridge 岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49451286\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_49 = np.zeros(train_shape)\n",
    "predictions_ridge_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    ridge_49 = Ridge(alpha=6)\n",
    "    ridge_49.fit(tr_x,tr_y)\n",
    "    oof_ridge_49[val_idx] = ridge_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_ridge_49 += ridge_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49534564\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_49 = np.zeros(train_shape)\n",
    "predictions_br_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    br_49 = br()\n",
    "    br_49.fit(tr_x,tr_y)\n",
    "    oof_br_49[val_idx] = br_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_br_49 += br_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53841695\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_49 = np.zeros(train_shape)\n",
    "predictions_en_49 = np.zeros(len(X_test_49))\n",
    "#\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    en_49 = en(alpha=1.0,l1_ratio=0.05)\n",
    "    en_49.fit(tr_x,tr_y)\n",
    "    oof_en_49[val_idx] = en_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_en_49 += en_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了以上4种新模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4949144061269495"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack4 = np.vstack([oof_br_49,oof_kr_49,oof_en_49,oof_ridge_49]).transpose()\n",
    "test_stack4 = np.vstack([predictions_br_49, predictions_kr_49,predictions_en_49,predictions_ridge_49]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack4 = np.zeros(train_stack4.shape[0])\n",
    "predictions_lr4 = np.zeros(test_stack4.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack4,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack4[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack4[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr4 = lr()\n",
    "    lr4.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack4[val_idx] = lr4.predict(val_data)\n",
    "    predictions_lr4 += lr4.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合\n",
    "\n",
    "这里对于上述四种集成学习的模型的预测结果进行加权的求和，得到最终的结果，当然这种方式是很不准确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45339634819308466"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#和下面作对比\n",
    "mean_squared_error(target.values, 0.7*(0.6*oof_stack2 + 0.4*oof_stack3)+0.3*(0.55*oof_stack1+0.45*oof_stack4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更好的方式是将以上的4中集成学习模型再次进行集成学习的训练，这里直接使用LinearRegression简单线性回归的进行集成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.448928002544107"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack5 = np.vstack([oof_stack1,oof_stack2,oof_stack3,oof_stack4]).transpose()\n",
    "test_stack5 = np.vstack([predictions_lr1, predictions_lr2,predictions_lr3,predictions_lr4]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack5 = np.zeros(train_stack5.shape[0])\n",
    "predictions_lr5= np.zeros(test_stack5.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack5,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack5[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack5[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr5 = lr()\n",
    "    lr5.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack5[val_idx] = lr5.predict(val_data)\n",
    "    predictions_lr5 += lr5.predict(test_stack5) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果保存\n",
    "\n",
    "进行index的读取工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.879550\n",
       "std         0.461673\n",
       "min         1.631419\n",
       "25%         3.665978\n",
       "50%         3.951340\n",
       "75%         4.186127\n",
       "max         5.030780\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example = pd.read_csv('submit_example.csv',sep=',',encoding='latin-1')\n",
    "\n",
    "submit_example['happiness'] = predictions_lr5\n",
    "\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行结果保存，这里我们预测出的值是1-5的连续值，但是我们的ground truth是整数值，所以为了进一步优化我们的结果，我们对于结果进行了整数解的近似，并保存到了csv文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.879562\n",
       "std         0.461564\n",
       "min         1.631419\n",
       "25%         3.665978\n",
       "50%         3.951340\n",
       "75%         4.186127\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example.loc[submit_example['happiness']>4.96,'happiness']= 5\n",
    "submit_example.loc[submit_example['happiness']<=1.04,'happiness']= 1\n",
    "submit_example.loc[(submit_example['happiness']>1.96)&(submit_example['happiness']<2.04),'happiness']= 2\n",
    "\n",
    "submit_example.to_csv(\"submision.csv\",index=False)\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网格、随机搜索进一步调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-faf11c0848e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
