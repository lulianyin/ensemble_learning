{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们使用SVR的例子，结合管道来进行调优："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.187 +/- 0.649\n"
     ]
    }
   ],
   "source": [
    "# 我们先来对未调参的SVR进行评价： \n",
    "from sklearn.svm import SVR     # 引入SVR类\n",
    "from sklearn.pipeline import make_pipeline   # 引入管道简化学习流程\n",
    "from sklearn.preprocessing import StandardScaler # 由于SVR基于距离计算，引入对数据进行标准化的类\n",
    "from sklearn.model_selection import GridSearchCV  # 引入网格搜索调优\n",
    "from sklearn.model_selection import cross_val_score # 引入K折交叉验证\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "boston = datasets.load_boston()     # 返回一个类似于字典的类\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "pipe_SVR = make_pipeline(StandardScaler(),\n",
    "                                                         SVR())\n",
    "score1 = cross_val_score(estimator=pipe_SVR,\n",
    "                                                     X = X,\n",
    "                                                     y = y,\n",
    "                                                     scoring = 'r2',\n",
    "                                                      cv = 10)       # 10折交叉验证\n",
    "print(\"CV accuracy: %.3f +/- %.3f\" % ((np.mean(score1)),np.std(score1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网格搜索最优得分： 0.6081303070817233\n",
      "网格搜索最优参数组合：\n",
      " {'svr__C': 1000.0, 'svr__gamma': 0.001, 'svr__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# 下面我们使用网格搜索来对SVR调参：\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_svr = Pipeline([(\"StandardScaler\",StandardScaler()),\n",
    "                                                         (\"svr\",SVR())])\n",
    "param_range = [0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0]\n",
    "param_grid = [{\"svr__C\":param_range,\"svr__kernel\":[\"linear\"]},  # 注意__是指两个下划线，一个下划线会报错的\n",
    "                            {\"svr__C\":param_range,\"svr__gamma\":param_range,\"svr__kernel\":[\"rbf\"]}]\n",
    "gs = GridSearchCV(estimator=pipe_svr,\n",
    "                                                     param_grid = param_grid,\n",
    "                                                     scoring = 'r2',\n",
    "                                                      cv = 10)       # 10折交叉验证\n",
    "gs = gs.fit(X,y)\n",
    "print(\"网格搜索最优得分：\",gs.best_score_)\n",
    "print(\"网格搜索最优参数组合：\\n\",gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机搜索最优得分： 0.30021249798866756\n",
      "随机搜索最优参数组合：\n",
      " {'svr__C': 1.4195029566223933, 'svr__gamma': 1.8683733769303625, 'svr__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# 下面我们使用随机搜索来对SVR调参：\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform  # 引入均匀分布设置参数\n",
    "pipe_svr = Pipeline([(\"StandardScaler\",StandardScaler()),\n",
    "                                                         (\"svr\",SVR())])\n",
    "distributions = dict(svr__C=uniform(loc=1.0, scale=4),    # 构建连续参数的分布\n",
    "                     svr__kernel=[\"linear\",\"rbf\"],                                   # 离散参数的集合\n",
    "                    svr__gamma=uniform(loc=0, scale=4))\n",
    "\n",
    "rs = RandomizedSearchCV(estimator=pipe_svr,\n",
    "                                                     param_distributions = distributions,\n",
    "                                                     scoring = 'r2',\n",
    "                                                      cv = 10)       # 10折交叉验证\n",
    "rs = rs.fit(X,y)\n",
    "print(\"随机搜索最优得分：\",rs.best_score_)\n",
    "print(\"随机搜索最优参数组合：\\n\",rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过我们不懈的努力，从收集数据集并选择合适的特征、选择度量模型性能的指标、选择具体的模型并进行训练以优化模型到评估模型的性能并调参，我们认识到了如何使用sklearn构建简单回归模型。在本章的最后，我们会给出一个具体的案例，整合回归的内容。下面我们来看看机器学习另外一类大问题：分类。与回归一样，分类问题在机器学习的地位非常重要，甚至有的地方用的比回归问题还要多，因此分类问题是十分重要的！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用sklearn构建完整的分类项目                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 收集数据集并选择合适的特征：在数据集上我们使用我们比较熟悉的IRIS鸢尾花数据集。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature = iris.feature_names\n",
    "data = pd.DataFrame(X,columns=feature)\n",
    "data['target'] = y\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个特征的相关解释：\n",
    "   - sepal length (cm)：花萼长度(厘米)\n",
    "   - sepal width (cm)：花萼宽度(厘米)\n",
    "   - petal length (cm)：花瓣长度(厘米)\n",
    "   - petal width (cm)：花瓣宽度(厘米)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 选择度量模型性能的指标：                                    \n",
    "度量分类模型的指标和回归的指标有很大的差异，首先是因为分类问题本身的因变量是离散变量，因此像定义回归的指标那样，单单衡量预测值和因变量的相似度可能行不通。其次，在分类任务中，我们对于每个类别犯错的代价不尽相同，例如：我们将癌症患者错误预测为无癌症和无癌症患者错误预测为癌症患者，在医院和个人的代价都是不同的，前者会使得患者无法得到及时的救治而耽搁了最佳治疗时间甚至付出生命的代价，而后者只需要在后续的治疗过程中继续取证就好了，因此我们很不希望出现前者，当我们发生了前者这样的错误的时候会认为建立的模型是很差的。为了解决这些问题，我们必须将各种情况分开讨论，然后给出评价指标。             \n",
    "   - 真阳性TP：预测值和真实值都为正例；                        \n",
    "   - 真阴性TN：预测值与真实值都为正例；                     \n",
    "   - 假阳性FP：预测值为正，实际值为负；\n",
    "   - 假阴性FN：预测值为负，实际值为正；                      \n",
    "   ![jupyter](./1.22.png)                                       \n",
    "分类模型的指标：                    \n",
    "   - 准确率：分类正确的样本数占总样本的比例，即：$ACC = \\frac{TP+TN}{FP+FN+TP+TN}$.                                \n",
    "   - 精度：预测为正且分类正确的样本占预测值为正的比例，即：$PRE = \\frac{TP}{TP+FP}$.                     \n",
    "   - 召回率：预测为正且分类正确的样本占类别为正的比例，即：$REC =  \\frac{TP}{TP+FN}$.                     \n",
    "   - F1值：综合衡量精度和召回率，即：$F1 = 2\\frac{PRE\\times REC}{PRE + REC}$.                                     \n",
    "   - ROC曲线：以假阳率为横轴，真阳率为纵轴画出来的曲线，曲线下方面积越大越好。                                                          \n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics                           \n",
    "![jupyter](./1.21.png)                          \n",
    "在本次小案例中，我们使用ROC曲线作为最终评价指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 选择具体的模型并进行训练                              \n",
    "   - **逻辑回归logistic regression：**                      \n",
    "   说到分类问题与回归问题的区别，在于回归问题与分类问题需要预测的因变量不一样。在回归问题中，因变量是连续性变量，我们需要预测$E(Y|X)$是一个连续的实数，但是在分类问题中，我们往往是通过已知X的信息预测Y的类别，往往是一个离散集合中的某个元素。如：是否患癌症，图片是猫还是狗等。一个很自然的想法是能否用线性回归去处理分类问题，答案是可以但不好！先来看看线性回归处理分类问题会出现什么弊端，我们仔细来看这个线性回归的例子，${default = \\beta_0 + \\beta_1 Balance + \\beta_2 Income}$，只要输入Balance 和 Income 以及default的数据就能用最小二乘法估计出${\\beta_0,\\beta_1}$,设定预测的default>0.5就是违约反之不违约，感觉很完美的样子，但事实真的是这样吗？假设我们需要用某个人的债务(Balance)和收入(Income)去预测是否会信用卡违约(default)：       \n",
    "      - 我们假设有一个穷人Lisa,他的Balance和Income都很小，那么有可能会导致default的值为负数，那么这个负数代表什么意义呢？显然是没有任何意义的。                \n",
    "      ![jupyter](./1.23.png)                            \n",
    "      - 当我们的分类变量是多类的时候，以0.5为界限划分分类就不可用了，那么我们应该怎么找到一个界限衡量多分类呢？                              \n",
    "   基于以上问题，现在大家是否还觉得线性回归模型作为一个分类模型是否足够优秀呢？其实，为了解决以上的问题（1）我们来想想能不能将线性回归的结果default转化为区间[0:1]上，让default转变成一个违约的概率呢？下面我们来解决这个问题吧。                              \n",
    "   在推导逻辑回归之前，我们先来认识下一组函数，这组函数具有神奇的作用，可以将是实数轴上的数转换为[0:1]区间上的概率。\n",
    "  首先，我们假设我们的线性回归模型为 ${Y=\\beta_0+\\beta_1 X}$，那么这个函数是如何将线性回归的结果转化为概率呢？这个函数就是logistic 函数，具体的形式为   ${p(X) = \\dfrac{e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}}}$，他的函数图像如下图：（左边是线性回归，右边是逻辑函数）                             \n",
    "  ![jupyter](./1.24.png)                                   \n",
    "  因此，我们假设逻辑回归模型为：$p(y = 1|x) = \\frac{1}{1+e^{-w^Tx}}$ .                              \n",
    "  下面我们来具体推导下逻辑回归模型：                          \n",
    "  假设数据Data$\\{(x_i,y_i) \\},\\;\\;i = 1,2,...,N,\\;\\;x_i \\in R^p,y_i \\in \\{0,1 \\}$，设$p_1 = p(y=1|x) = \\sigma(w^T) = \\frac{1}{1+e^{-w^Tx}}$。因为y只可能取0或者1，因此假设数据服从0-1分布，也叫伯努力分布，即：当y=1时，$p(y|x)=p_1$，当y=0时，$p(y|x)=1-p_1$，可以写成$p(y|x) = p_1^y(1-p_1)^{1-y}$，可以带入y=0和y=1进去验证，结果和前面的结论一模一样。                    \n",
    "  我们使用极大似然估计MLE，即：                         \n",
    "  $$\n",
    "  \\hat{w} = argmax_w\\;\\;log\\;P(Y|X) = argmax_x\\;\\;log\\;\\prod_{i=1}^N P(y_i|x_i) = argmax_w \\sum\\limits_{i=1}^{N} log\\;P(y_i|x_i)\\\\\n",
    "  \\;\\;\\; = argmax_w \\sum\\limits_{i=1}^{N}(y_ilog\\;p_1 + (1-y_i)log(1-p_1)) \\\\ \n",
    "  记：L(w) = \\sum\\limits_{i=1}^{N}(y_ilog\\;p_1 + (1-y_i)log(1-p_1))\\\\\n",
    " \\;\\;\\; \\frac{\\partial L}{\\partial w_k} = \\sum\\limits_{i=1}^{N} y_i\\frac{1}{p_1}\\frac{\\partial p_1}{\\partial z}\\frac{\\partial z}{\\partial w_k} + (1-y_i)\\frac{1}{1-p_1}(-\\frac{\\partial p_1}{\\partial z}\\frac{\\partial z}{\\partial w_k})\\\\\n",
    "  \\;\\;\\;=\\sum\\limits_{i=1}^{N}y_i\\frac{1}{\\sigma(z)}(\\sigma(z_i)-\\sigma(z_i)^2)x_i + (1-y_i)\\frac{1}{1-\\sigma(z_i)}[-(\\sigma(z_i)-\\sigma(z_i)^2)x_i]\\\\\n",
    "  \\;\\;\\; =\\sum\\limits_{i=1}^{N}[(y_i-y_i\\sigma(z_i))x_i + (1-y_i)(-\\sigma(z_i))x_i]\\\\\n",
    "  \\;\\;\\; = \\sum\\limits_{i=1}^{N}y_ix_i-\\sigma(z_i)x_i = \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i\n",
    "  $$                 \n",
    "  因此，$\\frac{\\partial L}{\\partial w_k} = \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i$，由于这里涉及的函数不像线性回归一样能简单求出解析解，因此我们使用迭代的优化算法：梯度下降法，即：                       \n",
    "  $w_k^{(t+1)}\\leftarrow w_k^{(t)} - \\eta \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i^{(k)},\\;\\;\\;其中，x_i^{(k)}为第i个样本第k个特征$                                 \n",
    "  如果想了解关于梯度下降法等无约束算法的具体细节，可以参照笔者写的另外两篇知乎博客：                          \n",
    "  最优化理论之无约束优化基本结构及其python应用：https://zhuanlan.zhihu.com/p/163405865                                                   \n",
    "  最优化理论之负梯度方法与Newton型方法：https://zhuanlan.zhihu.com/p/165914126                                              \n",
    "  对于问题(2),我们值得注意的是，逻辑回归在实际中不太用于多分类问题，因为实际效果不是很好，所以我们可以借助其他模型来解决这个问题，那让我们来解决这个遗留下来的问题吧。                                \n",
    " \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 基于概率的分类模型：                               \n",
    "   (1) 线性判别分析：                                              \n",
    "   线性判别分析是一个比较久远的算法，我将会从两个方向去描述这个算法，因为我觉得每位读者都有自己喜欢的那个理解的方向，分别是基于贝叶斯公式和降维分类的思想。                        \n",
    "      - 基于贝叶斯公式对线性判别分析的理解：                       \n",
    "   在讨论如何解决多分类问题之前，我们先来说说贝叶斯的那些事吧。在概率统计的领域里有一条神奇的公式叫贝叶斯定理，具体的形式是：${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$ ，我们 先不要被公式的符号吓到，我们先来看看符号具体代表什么意思。我们假设观测有${K}$类，${\\pi_k}$为随机选择的观测来自第${k}$类的 __先验概率__，也就是样本里面第${k}$类的样本个数除以总样本的个数：${\\pi_k = \\dfrac{n_k}{n}}$。再来 ${f_k(x) =P(X=x|Y=k)}$，表示第${k}$类观测的X的密度函数，说的直白一点就是在${Y=k}$的样本里${X=x}$的样本个数，即${f_k(x) = P(X=x|Y=k) = \\dfrac{n_{(X=x,Y=k)}}{n_{(Y=k)}}}$，最后，${\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}=P(X=x)=\\dfrac{n_{(X=x)}}{n}$，也就是样本中${X=x}$的概率。\n",
    "      在讨论贝叶斯定理后，我们回到分类问题，这个定理跟我们的分类问题有什么关联呢？没错，这个公式${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$给出了给定样本条件下，${Y=k}$这个类别下的概率，这给分类问题提供了一条思路，那就是计算这个${P(Y=k|X=x)}$，而且我们的逻辑回归就是这么干的，但是在${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$这个公式中，分母${{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)} = P(X=x)}$当样本给定的时候是一个与分类${k}$无关的常数,所以我们的问题可以简化为只需要计算分子${{\\pi}_kf_k(x)}$,进而比较哪个类别的概率最大就知道属于哪个类别了，因此我们的分类思路就出来啦，这个思路不同于逻辑回归，逻辑回归需要计算具体的${P(Y=k|X=x)}$概率值，而我们现在的思路是通过贝叶斯定理计算贝叶斯定理的分子，比较分子最大的那个类别为最终类别。                 \n",
    "      在我们推导复杂算法之前，我们先推导下简单的当自变量个数只有一个的模型，即${p=1}$的简单模型。我们记${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$ 的分子为${g_k(x) = {\\pi}_kf_k(x)}$。在这里，我们做个模型假设：假设${f_k(x) }$服从正态分布，即${f_k(x) \\sim N(\\mu,\\sigma_k^2)}$，而且每个${\\sigma_k^2 = \\sigma^2}$，同方差假设。因此${f_k(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\dfrac{1}{2\\sigma^2}(x-\\mu_k)^2}}$，最终我们的${g_k(x) = \\pi_k\\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\dfrac{1}{2\\sigma^2}(x-\\mu_k)^2}}$,终于算出来啦。这个式子不是很好计算，我们对${g_k(x)}$取个对数，令${\\delta_k(x) = ln(g_k(x))=ln\\pi_k+\\dfrac{\\mu}{\\sigma^2}x-\\dfrac{\\mu^2}{2\\sigma^2}}$，到这里我们的模型建立模型，我们只需要把位置的${\\mu_k}$与${\\sigma^2}$估计出来就好了。${\\hat{\\mu}_k =\\dfrac{1}{n_k}\\sum\\limits_{i:y_i=k}x_i}$，也就是当${y=k}$这一类中${x}$的平均值；${\\hat{\\sigma}^2 =\\dfrac{1}{n-K}\\sum\\limits_{k=1}^K\\sum\\limits_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 }$，说白了就是计算每一类的方差，再求平均值。总结下上面的公式就是：                                    \n",
    "${\\begin{cases}\\delta_k(x) = ln(g_k(x))=ln\\pi_k+\\dfrac{\\mu}{\\sigma^2}x-\\dfrac{\\mu^2}{2\\sigma^2}\\\\{\\hat{\\mu}_k =\\dfrac{1}{n_k}\\sum\\limits_{i:y_i=k}x_i}\\\\{\\hat{\\sigma}^2 =\\dfrac{1}{n-K}\\sum\\limits_{k=1}^K\\sum\\limits_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2}\\end{cases}}$                              \n",
    "      至此，我们的模型就建立完成了，我们只需要代入数据求出${\\delta_k(x)}$，哪个${k}$对应的${\\delta_k(x)}$大，就是哪一类。                                   \n",
    "   （下图虚线是线性判别分析的决策边界，正态曲线哪边高样本就是哪一类）                  \n",
    "      ![jupyter](./1.25.png)                            \n",
    "      我们推到出了一个自变量的简单模型，就要泛化为多个自变量的线性判别分析了，即${p>1}$。其实原理一样的，只是将一元正态分布扩展为多元正态分布：\n",
    "      ${f_k(x)=\\dfrac{1}{(2\\pi)^{\\tfrac{p}{2}}|\\Sigma|^\\tfrac{1}{2}}e^{[-\\tfrac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)]}}$                           \n",
    "      ${\\hat{\\mu_k}=(\\mu_{k1},\\mu_{k2},......,\\mu_{kp})   ,   \\hat{\\Sigma}=\\dfrac{1}{p-1}\\sum\\limits_{j=1}^p(x_j-\\overline{x})(x_j-\\overline{x})^T}$                               \n",
    "      ${\\delta_k(x) = ln(\\pi_kf_k(x))=ln(\\pi_k)-(\\dfrac{p}{2}ln(2\\pi)+\\dfrac{1}{2}ln(|\\Sigma|))-\\dfrac{1}{2}(x-\\mu_k)^T\\Sigma^-1(x-\\mu_k)=x^T\\hat{\\Sigma}\\hat{\\mu}_k-\\dfrac{1}                                                       {2}\\hat{\\mu}_k^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k+ln\\hat{\\pi}_k}$                            \n",
    "      - 降维分类的思想理解线性判别分析：                   \n",
    "      基于数据进行分类时，一个很自然的想法是：将高维的数据降维至一维，然后使用某个阈值将各个类别分开。下面用图的形式展示：                   \n",
    "      ![jupyter](./1.26.png)                        \n",
    "      图中，数据的维度是二维的，我们的想法是把数据降维至一维，然后用阈值就能分类。这个似乎是一个很好的想法，我们总是希望降维后的数据同一个类别自身内部方差小，不同类别之间的方差要尽可能大。这也是合理的，因为同一个类别的数据应该更加相似，因此方差小；不同类别的数据之间应该很不相似，这样才能更容易对数据进行分类，我们简称为：**类内方差小，类间方差大**，在计算机语言叫“松耦合，高内聚”。在做具体的推导之前，我们对数据的形式和一些基本统计量做一些描述：                            \n",
    "      特征$X = (x_1,x_2,...,x_N)^T$，因变量$Y = (y_1,y_2,...,y_N)^T,\\;\\;其中，y_i \\in \\{+1,-1 \\}$，类别c1的特征$X_{c_1} = \\{x_i|y_i=+1 \\}$，同理，类别c2的特征$X_{c_2} = \\{x_i|y_i=-1 \\}$，属于c1类别的数据个数为$N_1$，属于类别c2的数据个数为$N_2$，其中，$N_1+N_2 = N$。                         \n",
    "      特征X投影在w方向至一维：$z_i = w^Tx_i,\\;\\;||w|| = 1$                            \n",
    "      全样本投影的均值$\\bar{z} = \\frac{1}{N}\\sum\\limits_{i=1}^{N}z_i = \\frac{1}{N}\\sum\\limits_{i=1}^{N}w^Tx_i$                    \n",
    "      全样本投影的协方差$S_z = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(z_i-\\bar{z})(z_i-\\bar{z})^T = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(w^Tx_i-\\bar{z})(w^Tx_i-\\bar{z})^T$                   \n",
    "      c1样本投影的均值$\\bar{z_1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}z_i = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}w^Tx_i$                    \n",
    "      c1样本投影的协方差$S_{z_1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(z_i-\\bar{z_1})(z_i-\\bar{z_1})^T = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(w^Tx_i-\\bar{z_1})(w^Tx_i-\\bar{z_1})^T$                       \n",
    "      c2样本投影的均值 $\\bar{z_2} = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}z_i = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}w^Tx_i$                     \n",
    "      c2样本投影的协方差$S_{z_2} = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(z_i-\\bar{z_2})(z_i-\\bar{z_2})^T = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(w^Tx_i-\\bar{z_2})(w^Tx_i-\\bar{z_2})^T$                      \n",
    "      类间差距：$(\\bar{z}_1-\\bar{z}_2)^2$                      \n",
    "      类内方差：$S_1 + S_2$                          \n",
    "      由于线性判别分析的目标是同一类别内方差小，不同类别之间距离大，因此损失函数定义为：   \n",
    "                            \n",
    "   $$\n",
    "      J(w) = \\frac{(\\bar{z}_1-\\bar{z}_2)^2}{s_1+s_2} = \\frac{w^T(\\bar{x}_{c_1}-\\bar{x}_{c_2})(\\bar{x}_{c_1}-\\bar{x}_{c_2})^Tw}{w^T(s_{c_1}+s_{c_2})w}\\\\\n",
    "      \\;\\;\\; \\hat{w} = argmax_w\\;J(w)\n",
    "   $$                             \n",
    "   记：$S_b = (\\bar{x}_{c_1}-\\bar{x}_{c_2})(\\bar{x}_{c_1}-\\bar{x}_{c_2})^T,\\;S_w = (s_{c_1}+s_{c_2})$，因此$J(w) = \\frac{w^TS_bw}{w^TS_ww}$                   \n",
    "   让J(w)对w求导等于0，求出：$w = S_w^{-1}(\\bar{x}_{c_1}-\\bar{x}_{c_2})$                       \n",
    "   (2) 朴素贝叶斯：                                        \n",
    "   在线性判别分析中，我们假设每种分类类别下的特征遵循同一个协方差矩阵，每两个特征之间是存在协方差的，因此在线性判别分析中各种特征是不是独立的。但是，朴素贝叶斯算法对线性判别分析作进一步的模型简化，它将线性判别分析中的协方差矩阵中的协方差全部变成0，只保留各自特征的方差，也就是朴素贝叶斯假设各个特征之间是不相关的。在之前所看到的偏差-方差理论中，我们知道模型的简化可以带来方差的减少但是增加偏差，因此朴素贝叶斯也不例外，它比线性判别分析模型的方差小，偏差大。虽然简化了模型，实际中使用朴素贝叶斯的案例非常多，甚至多于线性判别分析，例如鼎鼎大名的新闻分类，垃圾邮件分类等。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  逻辑回归\n",
    "'''\n",
    "penalty       {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’正则化方式\n",
    "dual      bool, default=False   是否使用对偶形式，当n_samples> n_features时，默认dual = False。   \n",
    "C        float, default=1.0      \n",
    "solver       {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’     \n",
    "l1_ratio         float, default=None           \n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_iris = LogisticRegression()\n",
    "log_iris.fit(X,y)\n",
    "log_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性判别分析\n",
    "'''\n",
    "参数：\n",
    "solver:{'svd'，'lsqr'，'eigen'}，默认='svd'\n",
    "solver的使用，可能的值：\n",
    "'svd'：奇异值分解（默认）。不计算协方差矩阵，因此建议将此求解器用于具有大量特征的数据。\n",
    "\n",
    "'lsqr'：最小二乘解，可以与收缩结合使用。\n",
    "\n",
    "'eigen'：特征值分解，可以与收缩结合使用。\n",
    "'''\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda_iris = LinearDiscriminantAnalysis()\n",
    "lda_iris.fit(X,y)\n",
    "lda_iris.score(X,y)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 朴素贝叶斯             \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_iris = GaussianNB()\n",
    "NB_iris.fit(X, y)\n",
    "NB_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 决策树 ：                     \n",
    "   与前面内容所讲的决策树回归大致是一样的，只是在回归问题中，选择分割点的标准是均方误差，但是在分类问题中，由于因变量是类别变量而不是连续变量，因此用均方误差显然不合适。那问题是用什么作为选择分割点的标准呢？我们先来分析具体的问题：                         \n",
    "   在回归树中，对一个给定的观测值，因变量的预测值取它所属的终端结点内训练集的平均因变量。与之相对应，对于分类树来说，给定一个观测值，因变量的预测值为它所属的终端结点内训练集的**最常出现的类**。分类树的构造过程与回归树也很类似，与回归树一样，分类树也是采用递归二叉分裂。但是在分类树中，均方误差无法作为确定分裂节点的准则，一个很自然的替代指标是分类错误率。分类错误率就是：此区域内的训练集中非常见类所占的类别，即：                                   \n",
    "   $$\n",
    "   E = 1-max_k(\\hat{p}_{mk})\n",
    "   $$                       \n",
    "   上式中的$\\hat{p}_{mk}$代表第m个区域的训练集中第k类所占的比例。但是在大量的事实证明：分类错误率在构建决策树时不够敏感，一般在实际中用如下两个指标代替：             \n",
    "   (1) 基尼系数：                   \n",
    "   $$\n",
    "   G = \\sum\\limits_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})\n",
    "   $$             \n",
    "   在基尼系数的定义中，我们发现这个指标衡量的是K个类别的总方差。不难发现，如果所有的$\\hat{p}_{mk}$的取值都接近0或者1，基尼系数会很小。因此基尼系数被视为衡量结点纯度的指标----如果他的取值小，那就意味着某个节点包含的观测值几乎来自同一个类别。                         \n",
    "   由基尼系数作为指标得到的分类树叫做：CART。                        \n",
    "   (2) 交叉熵：                       \n",
    "   可以替代基尼系数的指标是交叉熵，定义如下：                           \n",
    "   $$\n",
    "   D = -\\sum\\limits_{k=1}^{K} \\hat{p}_{mk}log\\;\\hat{p}_{mk}\n",
    "   $$                     \n",
    "   显然，如果所有的$\\hat{p}_{mk}$都接近于0或者1，那么交叉熵就会接近0。因此，和基尼系数一样，如果第m个结点的纯度越高，则交叉熵越小。事实证明，基尼系数和交叉熵在数值上时很接近的。                   \n",
    "   \n",
    "   ![jupyter](./1.27.png)                                            \n",
    "   决策树分类算法的完整步骤：                          \n",
    "      a.  选择最优切分特征j以及该特征上的最优点s：                \n",
    "      遍历特征j以及固定j后遍历切分点s，选择使得基尼系数或者交叉熵最小的(j,s)                                                   \n",
    "       b. 按照(j,s)分裂特征空间，每个区域内的类别为该区域内样本比例最多的类别。                           \n",
    "       c. 继续调用步骤1，2直到满足停止条件，就是每个区域的样本数小于等于5。        \n",
    "       d. 将特征空间划分为J个不同的区域，生成分类树。                 \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用决策树算法对iris分类：\n",
    "'''\n",
    "criterion:{“gini”, “entropy”}, default=”gini”\n",
    "max_depth:树的最大深度。\n",
    "min_samples_split:拆分内部节点所需的最少样本数\n",
    "min_samples_leaf :在叶节点处需要的最小样本数。\n",
    "\n",
    "'''\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_iris = DecisionTreeClassifier(min_samples_leaf=5)\n",
    "tree_iris.fit(X,y)\n",
    "tree_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 支持向量机SVM：                                   \n",
    "   支持向量机SVM是20世纪90年代在计算机界发展起来的一种分类算法，在许多问题中都被证明有较好的效果，被认为是适应性最广的算法之一。                         \n",
    "   ![jupyter](./1.28.png)                               \n",
    "   支持向量机的基本原理非常简单，如图所视，白色和蓝色的点各为一类，我们的目标是找到一个分割平面将两个类别分开。通常来说，如果数据本身是线性可分的，那么事实上存在无数个这样的超平面。这是因为给定一个分割平面稍微上移下移或旋转这个超平面，只要不接触这些观测点，仍然可以将数据分开。一个很自然的想法就是找到**最大间隔超平面**，即找到一个分割平面距离最近的观测点最远。下面我们来严格推导：                   \n",
    "   我们根据距离超平米那最近的点，只要同时缩放w和b可以得到：$w^Tx_1 + b = 1$与$w^Tx_2+b = -1$，因此：                      \n",
    "   $$\n",
    "  \\begin{array}{l}\n",
    "   w^{T} x_{1}+b=1 \\\\\n",
    "    w^{T} x_{2}+b=-1 \\\\\n",
    "    \\left(w^{T} x_{1}+b\\right)-\\left(w^{T} x_{2}+b\\right)=2 \\\\\n",
    "   w^{T}\\left(x_{1}-x_{2}\\right)=2 \\\\\n",
    "   \\qquad \\begin{array}{l}\n",
    "   w^{T}\\left(x_{1}-x_{2}\\right)=\\|w\\|_{2}\\left\\|x_{1}-x_{2}\\right\\|_{2} \\cos \\theta=2 \\\\\n",
    "   \\left\\|x_{1}-x_{2}\\right\\|_{2} \\cos \\theta=\\frac{2}{\\|w\\|_{2}}\n",
    "   \\end{array} \\\\\n",
    "    \\qquad \\begin{array}{l}\n",
    "   d_{1}=d_{2}=\\frac{\\left\\|x_{1}-x_{2}\\right\\|_{2} \\cos \\theta}{2}=\\frac{\\frac{2}{\\|w\\|_{2}}}{2}=\\frac{1}{\\|w\\|_{2}} \\\\\n",
    "   d_{1}+d_{2}=\\frac{2}{\\|w\\|_{2}}\n",
    "   \\end{array}\n",
    "   \\end{array}\n",
    "   $$                        \n",
    "   由此可知道SVM模型的具体形式：                           \n",
    "   $$\n",
    "  \\begin{aligned}\n",
    "\\min _{w, b} & \\frac{1}{2}\\|w\\|^{2} \\\\\n",
    "\\text { s.t. } & y^{(i)}\\left(w^{T} x^{(i)}+b\\right) \\geq 1, \\quad i=1, \\ldots, n\n",
    "\\end{aligned}\n",
    "   $$                     \n",
    "   可以将约束条件写为: $g_{i}(w)=-y^{(i)}\\left(w^{T} x^{(i)}+b\\right)+1 \\leq 0  $                                                     \n",
    "   可以将优化问题拉格朗日化\n",
    "   $$\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left[y^{(i)}\\left(w^{T} x^{(i)}+b\\right)-1\\right]\n",
    "   $$                        \n",
    "   因此：                   \n",
    "   $$\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{n} \\alpha_{i}\\left[y^{(i)}\\left(w^{T} x^{(i)}+b\\right)-1\\right]\n",
    "   $$                   \n",
    "   欲构造 dual 问题, 首先求拉格朗日化的问题中  $\\mathrm{w} $ 和  $\\mathrm{b} $ 的值, 对 $ \\mathrm{w}$  求梯度, 令梯度为  0,  可求得 w:              \n",
    "   对 b 求梯度, 令梯度为 0, 可得：              \n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial b} \\mathcal{L}(w, b, \\alpha)=\\sum_{i=1}^{n} \\alpha_{i} y^{(i)}=0\n",
    "   $$               \n",
    "\n",
    "   将  $\\mathrm{w}$  带入拉格朗日化的原问题可得                        \n",
    "   $$\n",
    "   \\begin{array}{l}\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} y^{(i)} y^{(j)} \\alpha_{i} \\alpha_{j}\\left(x^{(i)}\\right)^{T} x^{(j)}-b \\sum_{i=1}^{n} \\alpha_{i} y^{(i)} \\\\\n",
    "   \\mathcal{L}(w, b, \\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} y^{(i)} y^{(j)} \\alpha_{i} \\alpha_{j}\\left(x^{(i)}\\right)^{T} x^{(j)}\n",
    "   \\end{array}\n",
    "   $$                           \n",
    "   因此：                           \n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "    &\\text { 对拉格朗日化的原问题求最小值, 得到了 } \\mathrm{w} \\text { , 现在可以构造 dual 问題 }\\\\\n",
    "    &\\begin{aligned}\n",
    "    \\max _{\\alpha} & W(\\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} y^{(i)} y^{(j)} \\alpha_{i} \\alpha_{j}\\left\\langle x^{(i)}, x^{(j)}\\right\\rangle \\\\\n",
    "    \\text { s.t. } & \\alpha_{i} \\geq 0, \\quad i=1, \\ldots, n \\\\\n",
    "    & \\sum_{i=1}^{n} \\alpha_{i} y^{(i)}=0\n",
    "    \\end{aligned}\\\\\n",
    "    &\\text { 可以推导出 b的值为: } b^{*}=-\\frac{\\max _{i: y^{(i)}=-1} w^{* T} x^{(i)}+\\min _{i: y^{(i)}=1} w^{* T} x^{(i)}}{2}\\\\\n",
    "    &\\begin{array}{r}\n",
    "    \\text { SVM的决策子如下,值的符号为类别. } \\\\\n",
    "    \\qquad w^{T} x+b=\\left(\\sum_{i=1}^{n} \\alpha_{i} y^{(i)} x^{(i)}\\right)^{T} x+b=\\sum_{i=1}^{n} \\alpha_{i} y^{(i)}\\left\\langle x^{(i)}, x\\right\\rangle+b\n",
    "    \\end{array}\n",
    "    \\end{aligned}\n",
    "    $$                              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 非线性支持向量机：                     \n",
    "   在刚刚的讨论中，我们都是着重讨论了线性支持向量机是如何工作的，但是在现实生活中，我们很难碰到线性可分的数据集，如：             \n",
    "   ![jupyter](./1.29.png)                         \n",
    "   那我们应该如何处理非线性问题呢？答案就是将数据投影至更加高的维度！                    \n",
    "   ![jupyter](./1.30.png)                     \n",
    "   上图中，在一维数据做不到线性可分，我们将数据投影至二维平面就可以成功线性可分。那么，我们来详细探讨下这其中的奥妙：                        \n",
    "   \n",
    "   $$\n",
    "   \\begin{array}{l}\n",
    "   \\Phi: \\mathcal{X} \\mapsto \\hat{\\mathcal{X}}=\\Phi(\\mathbf{x}) \\\\\n",
    "   \\Phi\\left(\\left[x_{i 1}, x_{i 2}\\right]\\right)=\\left[x_{i 1}, x_{i 2}, x_{i 1} x_{i 2}, x_{i 1}^{2}, x_{i 2}^{2}\\right]\n",
    "   \\end{array}\n",
    "   $$                      \n",
    "   如果我们使用上面公式的形式将低维数据拓展至高维数据，则必须面临一个很大的问题，那就是：维度爆炸导致的计算量太大的问题。假如是一个2维特征的数据，我们可以将其映射到5维来做特征的内积，如果原始空间是三维，可以映射到到19维空间，似乎还可以处理。但是如果我们的低维特征是100个维度，1000个维度呢？那么我们要将其映射到超级高的维度来计算特征的内积。这时候映射成的高维维度是爆炸性增长的，这个计算量实在是太大了，而且如果遇到无穷维的情况，就根本无从计算了。能不能呢个避免这个问题呢？核函数隆重登场：                       \n",
    "   回顾线性可分SVM的优化目标函数：                  \n",
    "   $$\n",
    "   \\underbrace{ min }_{\\alpha}  \\frac{1}{2}\\sum\\limits_{i=1,j=1}^{m}\\alpha_i\\alpha_jy_iy_jx_i \\bullet x_j - \\sum\\limits_{i=1}^{m}\\alpha_i\\\\\n",
    "   s.t. \\; \\sum\\limits_{i=1}^{m}\\alpha_iy_i = 0\\\\\n",
    "   0 \\leq \\alpha_i \\leq C\n",
    "   $$                  \n",
    "   注意到上式低维特征仅仅以内积$x_i \\bullet x_j$ 的形式出现，如果我们定义一个低维特征空间到高维特征空间的映射$\\phi$，将所有特征映射到一个更高的维度，让数据线性可分，我们就可以继续按前两篇的方法来优化目标函数，求出分离超平面和分类决策函数了。也就是说现在的SVM的优化目标函数变成：                         \n",
    "   $$\n",
    "   \\begin{array}{c}\n",
    "   \\underbrace{\\min }_{\\alpha} \\frac{1}{2} \\sum_{i=1, j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\phi\\left(x_{i}\\right) \\bullet \\phi\\left(x_{j}\\right)-\\sum_{i=1}^{m} \\alpha_{i} \\\\\n",
    "   \\text { s. } t . \\sum_{i=1}^{m} \\alpha_{i} y_{i}=0 \\\\\n",
    "   0 \\leq \\alpha_{i} \\leq C\n",
    "   \\end{array}\n",
    "   $$                    \n",
    "   可以看到，和线性可分SVM的优化目标函数的区别仅仅是将内积$x_i \\bullet x_j$替换为$\\phi(x_i) \\bullet \\phi(x_j)$。我们要将其映射到超级高的维度来计算特征的内积。这时候映射成的高维维度是爆炸性增长的，这个计算量实在是太大了，而且如果遇到无穷维的情况，就根本无从计算了。下面引入核函数：               \n",
    "   假设$\\phi$是一个从低维的输入空间$\\chi$（欧式空间的子集或者离散集合）到高维的希尔伯特空间的$\\mathcal{H}$映射。那么如果存在函数$K(x,z)$，对于任意$x, z \\in \\chi$，都有：                    \n",
    "   $$\n",
    "   K(x, z) = \\phi(x) \\bullet \\phi(z)\n",
    "   $$                       \n",
    "   那么我们就称$K(x, z)$为核函数。                   \n",
    "   仔细发现，$K(x, z)$的计算是在低维特征空间来计算的，它避免了在刚才我们提到了在高维维度空间计算内积的恐怖计算量。也就是说，我们可以好好享受在高维特征空间线性可分的利益，却避免了高维特征空间恐怖的内积计算量。下面介绍几种常用的核函数：                     \n",
    "   (1)  多项式核函数：                   \n",
    "   多项式核函数（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：                         \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\left(\\left\\langle\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right\\rangle+c\\right)^{d}\n",
    "   $$             \n",
    "   C用来控制低阶项的强度，C=0,d=1代表无核函数。                       \n",
    "   (2) 高斯核函数：                                                   \n",
    "   高斯核函数（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是非线性分类SVM最主流的核函数。libsvm默认的核函数就是它。表达式为：                             \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|_{2}^{2}}{2 \\sigma^{2}}\\right)\n",
    "   $$                              \n",
    "   使用高斯核函数之前需要将特征标准化，因此这里衡量的是样本之间的相似度。                    \n",
    "   (3) Sigmoid核函数：                    \n",
    "   Sigmoid核函数（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：               \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\tanh \\left(\\alpha \\mathbf{x}_{i}^{\\top} \\mathbf{x}_{j}+c\\right)\n",
    "   $$                        \n",
    "   此时的SVM相当于没有隐藏层的简单神经网络。                     \n",
    "   (4) 余弦相似度核：                  \n",
    "   常用于衡量两段文字的余弦相似度，表达式为：                    \n",
    "   $$\n",
    "   K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\frac{\\mathbf{x}_{i}^{\\top} \\mathbf{x}_{j}}{\\left\\|\\mathbf{x}_{i}\\right\\|\\left\\|\\mathbf{x}_{j}\\right\\|}\n",
    "   $$\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "'''\n",
    "C:正则化参数。正则化的强度与C成反比。必须严格为正。惩罚是平方的l2惩罚。\n",
    "kernel:{'linear'，'poly'，'rbf'，'sigmoid'，'precomputed'}，默认='rbf'\n",
    "degree:多项式和的阶数\n",
    "gamma:“ rbf”，“ poly”和“ Sigmoid”的内核系数。\n",
    "shrinking:是否软间隔分类，默认true\n",
    "\n",
    "'''\n",
    "svc_iris = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_iris.fit(X, y)\n",
    "svc_iris.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) 评估模型的性能并调参:                        \n",
    "更详细的可以查看笔者的知乎：https://zhuanlan.zhihu.com/p/140040705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网格搜索经历时间：4.300 S\n",
      "0.9800000000000001\n",
      "{'svc__C': 1.0, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索进行超参数调优：\n",
    "# 方式1：网格搜索GridSearchCV()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))\n",
    "param_range = [0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0]\n",
    "param_grid = [{'svc__C':param_range,'svc__kernel':['linear']},{'svc__C':param_range,'svc__gamma':param_range,'svc__kernel':['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1)\n",
    "gs = gs.fit(X,y)\n",
    "end_time = time.time()\n",
    "print(\"网格搜索经历时间：%.3f S\" % float(end_time-start_time))\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机网格搜索经历时间：0.942 S\n",
      "0.9733333333333334\n",
      "{'svc__kernel': 'linear', 'svc__C': 100.0}\n"
     ]
    }
   ],
   "source": [
    "# 方式2：随机网格搜索RandomizedSearchCV()\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))\n",
    "param_range = [0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0]\n",
    "param_grid = [{'svc__C':param_range,'svc__kernel':['linear']},{'svc__C':param_range,'svc__gamma':param_range,'svc__kernel':['rbf']}]\n",
    "# param_grid = [{'svc__C':param_range,'svc__kernel':['linear','rbf'],'svc__gamma':param_range}]\n",
    "gs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_grid,scoring='accuracy',cv=10,n_jobs=-1)\n",
    "gs = gs.fit(X,y)\n",
    "end_time = time.time()\n",
    "print(\"随机网格搜索经历时间：%.3f S\" % float(end_time-start_time))\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当类别为两类时，可以绘制混淆矩阵与ROC曲线**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 混淆矩阵：\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\",header=None)\n",
    "'''\n",
    "乳腺癌数据集：569个恶性和良性肿瘤细胞的样本，M为恶性，B为良性\n",
    "'''\n",
    "# 做基本的数据预处理\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df.iloc[:,2:].values\n",
    "y = df.iloc[:,1].values\n",
    "le = LabelEncoder()    #将M-B等字符串编码成计算机能识别的0-1\n",
    "y = le.fit_transform(y)\n",
    "le.transform(['M','B'])\n",
    "# 数据切分8：2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)\n",
    "from sklearn.svm import SVC\n",
    "pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe_svc.fit(X_train,y_train)\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test,y_pred=y_pred)\n",
    "fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues,alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j,y=i,s=confmat[i,j],va='center',ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制ROC曲线：\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "scorer = make_scorer(f1_score,pos_label=0)\n",
    "gs = GridSearchCV(estimator=pipe_svc,param_grid=param_grid,scoring=scorer,cv=10)\n",
    "y_pred = gs.fit(X_train,y_train).decision_function(X_test)\n",
    "#y_pred = gs.predict(X_test)\n",
    "fpr,tpr,threshold = roc_curve(y_test, y_pred) ###计算真阳率和假阳率\n",
    "roc_auc = auc(fpr,tpr) ###计算auc的值\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) ###假阳率为横坐标，真阳率为纵坐标做曲线\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.结语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章中，我们重点讨论了各种回归和分类算法的具体推导与简单应用，并且给出了如何使用sklearn这个强大的python工具库进行简单的机器学习模型的建模代码。本章的重点是各个基础算法的掌握，包括回归和分类(重点是分类)算法以及怎么用网格搜索以及其他搜索方式进行调参。简单模型在进行复杂项目的时候往往显得力不从心，那么在下一章中，我们将开始本次开源项目的主题----集成学习，我们着重讨论如何将本章所学的基础模型进行集成，变成功能更加强大的集成模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了巩固本章的理解，在这里给个小任务，大家结合sklearn的fetch_lfw_people数据集，进行一次实战。fetch_lfw_people数据集是一个图像数据集，详细内容可以参照：                   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html                   \n",
    "案例的内容是对图像进行识别并分类。               \n",
    "参考资料：                \n",
    "https://blog.csdn.net/cwlseu/article/details/52356665                      \n",
    "https://blog.csdn.net/jasonzhoujx/article/details/81905923"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
